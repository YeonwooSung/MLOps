{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7597947",
   "metadata": {},
   "source": [
    "# Zero-Shot Text Classification with Triton Inference Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54995b9",
   "metadata": {},
   "source": [
    "The recent release of the GPT-3 model by Open AI is one of the largest NLP model in human history, with whooping 175 billion parameters. This gigantic model has achieved promising results under zero-shot, few-shot, and one-shot settings and in some cases even surpassed state-of-the-art models using the aforementioned techniques. All of this got me interested in to dig deeper into the process of zero-shot learning in NLP. Before the success of transformer models, most of the zero-shot learning research was concentrated towards Computer Vision only, but now, there has been a lot of interesting work going on in the NLP domain as well due to the increase in quality of sentence embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10909a31",
   "metadata": {},
   "source": [
    "## What is Zero-Shot-Learning (ZSL)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f822e22",
   "metadata": {},
   "source": [
    "In short, ZSL is the ability to detect classes that the model has never seen during training. In this blog post, I am using the Latent embedding approach where we find the latent embeddings of the given input sequence and hypothesis (label against which we want to classify the premise) by embedding both the premise and hypothesis into the same space of model and then finding the distance between these two embeddings in the same space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99aa29",
   "metadata": {},
   "source": [
    "# Client-Side Script to Interact with Triton Inference Server for Zero-Shot-Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3049093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sachin/anaconda3/envs/transformers/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from functools import partial\n",
    "import os\n",
    "import tritongrpcclient\n",
    "import tritongrpcclient.model_config_pb2 as mc\n",
    "import tritonhttpclient\n",
    "from tritonclientutils import triton_to_np_dtype\n",
    "from tritonclientutils import InferenceServerException\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51977c9",
   "metadata": {},
   "source": [
    "We fetch the tokenizer for sentence bert model from the transformer library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "481dbf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sachin/anaconda3/envs/transformers/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('deepset/sentence_bert')\n",
    "VERBOSE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a09d0d",
   "metadata": {},
   "source": [
    "## Let's test some input sentences and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35348a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sachin/anaconda3/envs/transformers/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "sentence1 = 'Who are you voting for 2021?'\n",
    "sentence2 = 'Jupiter’s Biggest Moons Started as Tiny Grains of Hail'\n",
    "sentence3 = 'Hi Matt, your payment is one week past due. Please use the link below to make your payment'\n",
    "labels = ['business', 'space and science', 'politics']\n",
    "input_name = ['input__0', 'input__1']\n",
    "output_name = 'output__0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b4a1c",
   "metadata": {},
   "source": [
    "run_inference function recieves sentence as an input, preprocess it (i.e. perform tokenization), hit the server with a preporcessed inputs and get back the embeddings from the triton server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22ab849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sachin/anaconda3/envs/transformers/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def run_inference(sentence, model_name='deepset', url='127.0.0.1:8000', model_version='1'):\n",
    "    triton_client = tritonhttpclient.InferenceServerClient(\n",
    "        url=url, verbose=VERBOSE)\n",
    "    model_metadata = triton_client.get_model_metadata(\n",
    "        model_name=model_name, model_version=model_version)\n",
    "    model_config = triton_client.get_model_config(\n",
    "        model_name=model_name, model_version=model_version)\n",
    "    # I have restricted the input sequence length to 256\n",
    "    inputs = tokenizer.batch_encode_plus([sentence] + labels,\n",
    "                                     return_tensors='pt', max_length=256,\n",
    "                                     truncation=True, padding='max_length')\n",
    "    \n",
    "    input_ids = inputs['input_ids']\n",
    "    input_ids = np.array(input_ids, dtype=np.int32)\n",
    "    mask = inputs['attention_mask']\n",
    "    mask = np.array(mask, dtype=np.int32)\n",
    "    mask = mask.reshape(4, 256) \n",
    "    input_ids = input_ids.reshape(4, 256)\n",
    "    input0 = tritonhttpclient.InferInput(input_name[0], (4,  256), 'INT32')\n",
    "    input0.set_data_from_numpy(input_ids, binary_data=False)\n",
    "    input1 = tritonhttpclient.InferInput(input_name[1], (4, 256), 'INT32')\n",
    "    input1.set_data_from_numpy(mask, binary_data=False)\n",
    "    output = tritonhttpclient.InferRequestedOutput(output_name,  binary_data=False)\n",
    "    response = triton_client.infer(model_name, model_version=model_version, inputs=[input0, input1], outputs=[output])\n",
    "    embeddings = response.as_numpy('output__0')\n",
    "    embeddings = torch.from_numpy(embeddings)\n",
    "    sentence_rep = embeddings[:1].mean(dim=1)\n",
    "    label_reps = embeddings[1:].mean(dim=1)\n",
    "    similarities = F.cosine_similarity(sentence_rep, label_reps)\n",
    "    closest = similarities.argsort(descending=True)\n",
    "    for ind in closest:\n",
    "        print(f'label: {labels[ind]} \\t similarity: {similarities[ind]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b699e89",
   "metadata": {},
   "source": [
    "Here we're using the cosine similarity to retrieve the closest label to the input sentence based on its embeddings. We're calculating this metric based on the equation:\n",
    "$$\n",
    " \\frac{\n",
    "  \\sum\\limits_{i=1}^{n}{a_i b_i}\n",
    "  }{\n",
    "      \\sqrt{\\sum\\limits_{j=1}^{n}{a_j^2}}\n",
    "      \\sqrt{\\sum\\limits_{k=1}^{n}{b_k^2}}\n",
    "  }\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Once we calculate the cosine similarities, we can then SORT the labels according to highest similarity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcfe804a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sachin/anaconda3/envs/transformers/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Who are you voting for 2021?\n",
      "\n",
      "\n",
      "label: politics \t similarity: 0.23479251563549042\n",
      "label: space and science \t similarity: 0.13357104361057281\n",
      "label: business \t similarity: 0.03533152863383293\n",
      "\n",
      "\n",
      "Input sentence: Jupiter’s Biggest Moons Started as Tiny Grains of Hail\n",
      "\n",
      "\n",
      "label: space and science \t similarity: 0.3903110921382904\n",
      "label: business \t similarity: 0.184669628739357\n",
      "label: politics \t similarity: 0.1614534705877304\n",
      "\n",
      "\n",
      "Input sentence: Hi Matt, your payment is one week past due. Please use the link below to make your payment\n",
      "\n",
      "\n",
      "label: business \t similarity: 0.221212238073349\n",
      "label: politics \t similarity: 0.18082530796527863\n",
      "label: space and science \t similarity: 0.05963622406125069\n"
     ]
    }
   ],
   "source": [
    "print(\"Input sentence:\", sentence1)\n",
    "print('\\n')\n",
    "run_inference(sentence1)\n",
    "print('\\n')\n",
    "print(\"Input sentence:\", sentence2)\n",
    "print('\\n')\n",
    "run_inference(sentence2)\n",
    "print('\\n')\n",
    "print(\"Input sentence:\", sentence3)\n",
    "print('\\n')\n",
    "run_inference(sentence3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4981fa",
   "metadata": {},
   "source": [
    "## Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7317dc9",
   "metadata": {},
   "source": [
    "In additon to the sentence bert model, transformers library provides much bigger and more advanced models like RoBerta model which can dramatically improves the result for zero shot text classification. I have written a blog post which will walkthrough you from the same process but with more advanced transformer model (RoBerta) shows much better results for zero shot learning approach. I am attaching the blogpost for further reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020a44c",
   "metadata": {},
   "source": [
    "https://medium.com/nvidia-ai/how-to-deploy-almost-any-hugging-face-model-on-nvidia-triton-inference-server-with-an-8ee7ec0e6fc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c5ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
