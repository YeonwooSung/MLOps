{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Trace/Convert almost any PyTorch Geometric model into Triton acceptable models !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "from torch_geometric.data import NeighborSampler\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import os\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "# importing matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to dataset\n",
    "root = \"/home/sachin/Desktop/arangoml/datasets\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PygNodePropPredDataset('ogbn-products', root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting train val test split idx based on sales ranking\n",
    "split_idx = dataset.get_idx_split()\n",
    "evaluator = Evaluator(name='ogbn-products')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = split_idx['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighborhood Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = NeighborSampler(data.edge_index, node_idx=test_idx,\n",
    "                              sizes=[15, 10, 5], batch_size=1,\n",
    "                              shuffle=False, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting random test node and its adjacency matrix\n",
    "dummy_n_ids = []\n",
    "dummy_adjs = []\n",
    "for idx, (batch_size, n_id, adjs) in enumerate(test_loader):\n",
    "    if idx == 550:\n",
    "        dummy_n_ids.append(n_id)\n",
    "        dummy_adjs.append(adjs)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 236488, 2383556, 1616861, 1667901, 1785374,   37632,  757171,  258762])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ids of the node involved in computation\n",
    "dummy_n_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[EdgeIndex(edge_index=tensor([[1, 2, 0, 2, 0, 1, 3, 2, 4, 5, 6, 7],\n",
       "          [0, 0, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]]), e_id=tensor([25936595, 92402649, 25936594, 92402650, 92402648, 92402651, 92402647,\n",
       "          92402646, 94509855,  4828386, 53135952, 27156330]), size=(8, 4)),\n",
       "  EdgeIndex(edge_index=tensor([[1, 2, 0, 2, 0, 1, 3],\n",
       "          [0, 0, 1, 1, 2, 2, 2]]), e_id=tensor([25936595, 92402649, 25936594, 92402650, 92402648, 92402651, 92402647]), size=(4, 3)),\n",
       "  EdgeIndex(edge_index=tensor([[1, 2],\n",
       "          [0, 0]]), e_id=tensor([25936595, 92402649]), size=(3, 1))]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ajacency list or edge index\n",
    "dummy_adjs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len of edge_index is equal to number of number of hops from which we want to extract neighborhood information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dummy_adjs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating adjs for performing a trace on the GraphSage model\n",
    "# will contain only edge_idx and size attributes\n",
    "edge_list_0 = []\n",
    "edge_list_1 = []\n",
    "edge_list_2 = []\n",
    "edge_adjs = []\n",
    "for idx, e_idx in enumerate(dummy_adjs[0]):\n",
    "    if idx == 0:\n",
    "        edge_list_0.append(e_idx[0])\n",
    "        #edge_list_0.append(e_idx[1])\n",
    "        edge_list_0.append(torch.tensor(np.asarray(e_idx[2])))\n",
    "    elif idx == 1:\n",
    "        edge_list_1.append(e_idx[0])\n",
    "        #edge_list_1.append(e_idx[1])\n",
    "        edge_list_1.append(torch.tensor(np.asarray(e_idx[2])))\n",
    "    else:\n",
    "        edge_list_2.append(e_idx[0])\n",
    "        #edge_list_2.append(e_idx[1])\n",
    "        edge_list_2.append(torch.tensor(np.asarray(e_idx[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving to cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "edge_index_0 = edge_list_0[0]\n",
    "edge_index_0 = edge_index_0.to(device)\n",
    "edge_size_0 = edge_list_0[1]\n",
    "edge_size_0 = edge_size_0.to(device)\n",
    "\n",
    "edge_index_1 = edge_list_1[0]\n",
    "edge_index_1 = edge_index_1.to(device)\n",
    "edge_size_1 = edge_list_1[1]\n",
    "edge_size_1 = edge_size_1.to(device)\n",
    "\n",
    "edge_index_2 = edge_list_2[0]\n",
    "edge_index_2 = edge_index_2.to(device)\n",
    "edge_size_2 = edge_list_2[1]\n",
    "edge_size_2 = edge_size_2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading node feature matrix of the graph\n",
    "x = data.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of nodes involved in the computation graph\n",
    "x[dummy_n_ids[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 100])\n"
     ]
    }
   ],
   "source": [
    "# lets create node dummy input for the trace\n",
    "dummy_x = x[dummy_n_ids[0]]\n",
    "print(dummy_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 100])\n"
     ]
    }
   ],
   "source": [
    "# padding nodes\n",
    "max_nodes = 1000\n",
    "total_nodes = dummy_x.size(0)\n",
    "nodes_padded = max_nodes - total_nodes\n",
    "dummy_x_pad = F.pad(input=dummy_x, pad=(0, 0, 0, nodes_padded), mode='constant', value=0)\n",
    "dummy_x_pad = dummy_x_pad.to(device)\n",
    "print(dummy_x_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3376, -0.3109,  0.2868,  ..., -0.7435,  0.1572, -0.1681],\n",
       "        [ 0.1358,  0.0866, -0.5094,  ...,  0.8339,  0.3380,  0.6472],\n",
       "        [-1.4801,  0.6196, -0.1442,  ..., 12.1236, -2.6969, -2.7455],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rest of the rows in dummy_x are filled with 0\n",
    "dummy_x_pad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph sage\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "    \n",
    "    def forward(self, x, edge_index_0, edge_size_0, edge_index_1, edge_size_1, edge_index_2, edge_size_2):\n",
    "        # `train_loader` computes the k-hop neighborhood of a batch of nodes,\n",
    "        # and returns, for each layer, a bipartite graph object, holding the\n",
    "        # bipartite edges `edge_index`, the index `e_id` of the original edges,\n",
    "        # and the size/shape `size` of the bipartite graph.\n",
    "        # Target nodes are also included in the source nodes so that one can\n",
    "        # easily apply skip-connections or add self-loops.\n",
    "        max_target_nodes = 500\n",
    "        for i in range(3):\n",
    "            xs = []\n",
    "            \n",
    "            if i == 0:\n",
    "                edge_index = edge_index_0\n",
    "                size = edge_size_0\n",
    "            elif i == 1:\n",
    "                edge_index = edge_index_1\n",
    "                size = edge_size_1\n",
    "            elif i ==2:\n",
    "                edge_index = edge_index_2\n",
    "                size = edge_size_2\n",
    "                \n",
    "            x_target = x[:size[1]]  # Target nodes are always placed first.\n",
    "            tar_nodes_padded = max_target_nodes - size[1]\n",
    "            x_target = F.pad(input=x_target, pad=(0, 0, 0, tar_nodes_padded), mode='constant', value=0)\n",
    "\n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "            \n",
    "            if i != self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                #x = F.dropout(x, p=0.5, training=self.training)\n",
    "            xs.append(x)\n",
    "            # layer 1 embeddings\n",
    "            if i == 0: \n",
    "                x_all = torch.cat(xs, dim=0)\n",
    "                layer_1_embeddings = x_all\n",
    "            # layer 2 embeddings\n",
    "            elif i == 1:\n",
    "                x_all = torch.cat(xs, dim=0)\n",
    "                layer_2_embeddings = x_all\n",
    "            # layer 3 embeddings\n",
    "            elif i == 2:\n",
    "                x_all = torch.cat(xs, dim=0)\n",
    "                layer_3_embeddings = x_all    \n",
    "        #return x.log_softmax(dim=-1)\n",
    "        return layer_1_embeddings, layer_2_embeddings, layer_3_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model and chechkpoint\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SAGE(dataset.num_features, 256, dataset.num_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading checkpont\n",
    "model_checkpoint = '/home/sachin/Desktop/arangoml/obgn_wts/weight_checkpoint.pth.tar'\n",
    "model_w = torch.load(model_checkpoint)\n",
    "model_w = model_w[\"state_dict\"]\n",
    "model.load_state_dict(model_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing PyTorch Geometric GraphSage Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion of the model is done using its JIT traced version. According to PyTorch’s documentation: ‘Torchscript’ is a way to create serializable and optimizable models from PyTorch code”.\n",
    "\n",
    "It allows the developer to export their model to be re-used in other programs, such as efficiency-oriented C++ programs. Exporting a model requires: Dummy inputs and Standard length to execute the model’s forward pass.\n",
    "\n",
    "During the model’s forward pass with dummy inputs, PyTorch keeps the track of different operations on each tensor and records these operations to create the “trace” of the model.\n",
    "\n",
    "Since the created trace is relative to the dummy input dimensions, therefore the model inputs in the future will be constrained by the dimension of the dummy input, and will not work for other sequences length or batch size.\n",
    "\n",
    "It is therefore recommended to trace the model with the largest dummy input dimension that you can think can be fed to the model in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorch_to_TorchScript(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PyTorch_to_TorchScript, self).__init__()\n",
    "        self.model = model\n",
    "    def forward(self, data, edge_index_0, edge_size_0, edge_index_1, edge_size_1, edge_index_2, edge_size_2):\n",
    "        return self.model(data, edge_index_0, edge_size_0, edge_index_1, edge_size_1, edge_index_2, edge_size_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after trace it will save the model in cwd\n",
    "pt_model = PyTorch_to_TorchScript().eval()\n",
    "#pt_model = pt_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace\n",
    "traced_script_module = torch.jit.trace(pt_model, (dummy_x_pad, edge_index_0, edge_size_0, edge_index_1, edge_size_1, edge_index_2, edge_size_2), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the traced model in cwd\n",
    "traced_script_module.save(\"./model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading traced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_model = torch.jit.load(\"./model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_out = tr_model(dummy_x_pad, edge_index_0, edge_size_0, edge_index_1, edge_size_1, edge_index_2, edge_size_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([500, 256]), torch.Size([500, 256]), torch.Size([500, 47]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer-1, layer-2, layer-3 embeddings\n",
    "tr_out[0].shape, tr_out[1].shape, tr_out[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 47])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_out[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-8.6776e+00, -9.1641e+00, -1.4251e+01, -4.8633e+00,  2.7073e+01,\n",
       "        -9.1761e+00,  4.3235e-02,  1.4600e+01, -2.1659e+00, -8.8615e+00,\n",
       "        -1.1793e+01, -1.3701e+01, -4.8658e+00,  5.9286e+00, -6.9220e+00,\n",
       "        -9.5293e+00, -1.7088e+01, -1.1222e+01,  8.1884e-01, -1.5866e+01,\n",
       "         5.5329e-01, -1.4108e+01, -1.2657e+01, -1.9597e+01,  8.1923e+00,\n",
       "        -1.5794e+01, -2.2992e+01, -2.2723e+01, -4.6216e+01, -5.1885e+01,\n",
       "        -2.5836e+01, -1.6740e+01, -1.7584e+01, -1.2375e+01, -3.3004e+01,\n",
       "        -1.5356e+01, -3.4252e+01, -2.4260e+01, -1.7691e+01, -2.4234e+01,\n",
       "        -3.3041e+01, -1.1956e+01, -2.2771e+01, -2.2491e+01, -2.1991e+01,\n",
       "        -2.2300e+01, -2.2732e+01], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_out[2][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the Model Configuration File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration file, config.pbtxt contains the detail of permissible input/outputs types and shapes, favorable batch sizes, versioning, platform since the server doesn't know details about these configurations, therefore, we write them into a separate configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration for the above GraphSage Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "name: \"graph_embeddings\"\n",
    "\n",
    "platform: \"pytorch_libtorch\"\n",
    "\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [1000, 100]\n",
    "  } ,\n",
    "  \n",
    "{\n",
    "    name: \"input__1\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [2, -1]\n",
    "  },\n",
    "  \n",
    "{\n",
    "    name: \"input__2\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [2]\n",
    "  },\n",
    "  \n",
    "{\n",
    "    name: \"input__3\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [2, -1]\n",
    "  },\n",
    "  \n",
    "{\n",
    "    name: \"input__4\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [2]\n",
    "  },\n",
    "  \n",
    "{\n",
    "    name: \"input__5\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [2, -1]\n",
    "  },\n",
    "  \n",
    "{\n",
    "    name: \"input__6\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [2]\n",
    "  }\n",
    "  \n",
    "]\n",
    "\n",
    "output [\n",
    "{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [500, 256]\n",
    "  },\n",
    "  \n",
    "{\n",
    "    name: \"output__1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [500, 256]\n",
    "  },\n",
    "  \n",
    "{\n",
    "    name: \"output__2\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [500, 47]\n",
    "  }\n",
    "  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyg] *",
   "language": "python",
   "name": "conda-env-pyg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
