# Multimodal RAG example

This example demonstrates how to implement a simple multimodal RAG solution using the `Amazon Titan Multimodal Embeddings G1` model for multimodal embeddings and the `Anthropic Claude v2` model for text generation.

![Multimodal RAG](./images/multimodal-rag-Page-1.drawio.png)

1. We download a subset of data from the [Amazon Berkley Objects](https://amazon-berkeley-objects.s3.amazonaws.com/index.html) dataset. The data includes Amazon products with metadata and catalog images. The metadata includes multiple tags that provide short text description of the product in the image. The data is filtered to only keep images that are associated for tags with description in a given language (`enUS` in our example), to limit the size of the data.

1. We convert all downloaded images into Base64 encoding.

1. An image and its associated text are converted into embeddings in a single `invoke_model` call to the `amazon.titan-embed-image-v1` model. We embed all images in our dataset in this way.

1. These embeddings are then ingested into in-memory [FAISS](https://github.com/facebookresearch/faiss) database to store and search for embeddings vectors. In a real-world scenario, you will likely want to use a persistent data store such as the [vector engine for Amazon OpenSearch Service Serverless](https://aws.amazon.com/opensearch-service/serverless-vector-engine/) or the pgvector extension for PostgreSQL.

1. Now for retrieval, we consider the following scenario: a customer is looking for a product and has a text description of the product and, optionally, an image of the product, we do an embeddings based similarity search by converting the text description and the image into embeddings using the `Amazon Titan Multimodal Embeddings G1` model and retrieve the most relevant results from the vector database. 

1. We then further refine these search results by creating a text prompt using the description for the retrieved objects and asking the LLM (`Anthropic Claude v2`) to do the following:
    * Reason through the responses based on the customer's description of what they were looking for and then either accept or reject each of the search results
    * Explain the reasoning why each result was accepted or rejected. 
    * The text response generated by the model along with the accepted set of results (images and text) are returned to the customer.

## Contents

The example consists of three files:

- [`0_data_prep.ipynb`](./0_data_prep.ipynb) - This notebook contains the data download and data preparation code. It downloads the images and metadata from the [Amazon Berkley Objects](https://amazon-berkeley-objects.s3.amazonaws.com/index.html) dataset, scales these images (if needed) to fit into the 2048x2048 pixel limit as required the `Amazon Titan Multimodal Embeddings G1` model and finally converts these images into Base64 encoding,

- [`download_images.py`](./download_images.py) - This script downloads the images from `amazon-berkeley-objects` bucket. It uses the Python `asyncio` to download multiple files concurrently. It is called from as part of code cells in the [`0_data_prep.ipynb`](./0_data_prep.ipynb) notebook.

- [`1_multimodal_rag.ipynb`](./1_multimodal_rag.ipynb) - This notebook ingests the Base64 encoded image data along with the accompanying text into the vector database. It implements the RAG functionality by using the user query (text) and an associated image. Just for the purpose of illustration, the input image is generated using `Stability AI's Stable Diffusion XL` model, this can be replaced with an actual image the user may have.

## Setup (Optional)

The notebooks install all required Python packages upfront. In case you want to run these notebooks in a custom conda environment then you can create one using the following commands:

```
conda create --name multimodal_rag_py39 -y python=3.9 ipykernel
source activate multimodal_rag_py39
pip install -r requirements.txt -U
```

Use the `multimodal_rag_py39` conda environment for all notebooks in this folder.

## Running

Run the following two notebooks in the order listed below:

1. [`0_data_prep.ipynb`](./0_data_prep.ipynb)
1. [`1_multimodal_rag.ipynb`](./1_multimodal_rag.ipynb) 



