{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal RAG for PDF files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example references [sudarshan koirala's work](https://github.com/sudarshan-koirala/youtube-stuffs/blob/main/langchain/LangChain_Multi_modal_RAG.ipynb) to build a multimodal-rag for pdf that contains tables,images and text paragraphs.  \n",
    "The vector database used here is Amazon Opensearch Serverless (aoss).  \n",
    "Refer to the [public documentation](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-vector-search.html) to set this up.  \n",
    "Below is the data ingestion pipeline.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data ingestion](./diagrams/multimodal-rag.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the inference pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![inference](./diagrams/multimodal-rag-inference.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install unstructured[all-docs] transformers opensearch-py boto3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load sample PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = './data'\n",
    "target_files = [os.path.join(data_dir,file_name) for file_name in os.listdir(data_dir)]\n",
    "image_output_dir = 'data-output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize bedrock runtime client and prompt used to summarize tables and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime',region_name='us-west-2')\n",
    "\n",
    "summary_prompt = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
    "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define methods to invoke bedrock FMs.  \n",
    "invoke_model uses titan embeddings to convert text to vector embeddings for search  \n",
    "invoke_llm_model uses claude LLM to summarise the context and produce the final output returned to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_model(input):\n",
    "    response = bedrock_runtime_client.invoke_model(\n",
    "        body=json.dumps({\n",
    "            'inputText': input\n",
    "        }),\n",
    "        modelId=\"amazon.titan-embed-text-v1\",\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\",\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    return response_body.get(\"embedding\")\n",
    "\n",
    "def invoke_llm_model(input):\n",
    "    response = bedrock_runtime_client.invoke_model(\n",
    "        body=json.dumps({\n",
    "            \"prompt\": \"\\n\\nHuman: {input}\\n\\nAssistant:\".format(input=input),\n",
    "            \"max_tokens_to_sample\": 300,\n",
    "            \"temperature\": 0.5,\n",
    "            \"top_k\": 250,\n",
    "            \"top_p\": 1,\n",
    "            \"stop_sequences\": [\n",
    "                \"\\n\\nHuman:\"\n",
    "            ],\n",
    "            # \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "        }),\n",
    "        modelId=\"anthropic.claude-v2:1\",\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\",\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    return response_body.get(\"completion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Blip2 model is used to do image captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "caption_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\", load_in_8bit=True, device_map={\"\": 0}, torch_dtype=torch.float16\n",
    ") \n",
    "prompt_caption = \"Question: What is in the image? Be specific about graphs, such as bar plots. Answer:\"\n",
    "\n",
    "def generate_image_captions(image_path,prompt):\n",
    "    image = Image.open(open(image_path,'rb'))\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.float16)\n",
    "    generated_ids = caption_model.generate(**inputs,max_new_tokens=50)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    return generated_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unstructured python package is used to segregate and retrieve the tables, images and text paragraphs from a PDF file.  \n",
    "The output directory of image files is specified in the image_output_dir_path argument in partition_pdf.  \n",
    "Tables are summarized to text using the Claude bedrock endpoint. Both the raw table elements and summarized text are stored.  \n",
    "Images are summarized using the image caption model.  \n",
    "Text paragraphs are stored as they are. They can be chunked before storing if they are too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "extracted_elements_list = []\n",
    "for target_file in target_files:\n",
    "    image_output_dir_path = os.path.join(image_output_dir,target_file.split('/')[1].split('.')[0])\n",
    "    table_and_text_elements = partition_pdf(\n",
    "        filename=target_file,\n",
    "        extract_images_in_pdf=True,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\", #Uses title elements to identify sections within the document for chunking\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        image_output_dir_path=image_output_dir_path,\n",
    "    )\n",
    "    tables = []\n",
    "    texts = []\n",
    "    for element in table_and_text_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tables.append({'raw':str(element),'summary':invoke_llm_model(summary_prompt.format(element=str(element)))})\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))\n",
    "    image_captions = []\n",
    "    for image_file in os.listdir(image_output_dir_path):\n",
    "        image_captions.append(generate_image_captions(os.path.join(image_output_dir_path,image_file),prompt_caption))\n",
    "    \n",
    "    extracted_elements_list.append({\n",
    "        'source': target_file,\n",
    "        'tables': tables,\n",
    "        'texts': texts,\n",
    "        'images': image_captions\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The opensearchpy python package is used to interact with the aoss database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "host = 'your collection id.region.aoss.amazonaws.com'\n",
    "region = 'us-west-2'\n",
    "service = 'aoss'\n",
    "index = 'your collection index'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, service)\n",
    "\n",
    "ospy_client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    pool_maxsize = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to prepare documents for insertion into the aoss database.  \n",
    "src_doc refers to the original document (e.g. wildfires.pdf)  \n",
    "raw_element refers to tables,images or text paragraphs extracted from the unstructured python package  \n",
    "raw_element_type is one of table,image or text  \n",
    "processed_element refers to post processed elements from raw_element; this can be table summary, text chunks, image captions  \n",
    "processed_element_embedding refers to vector embeddings generated from processed_element using the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_document(embedding,raw_element,processed_element,doc_type,src_doc):\n",
    "    document = { \n",
    "        # \"_id\": str(hash(raw_element)),\n",
    "        \"processed_element_embedding\": embedding,\n",
    "        \"processed_element\": processed_element,\n",
    "        \"raw_element_type\": doc_type,\n",
    "        \"raw_element\": raw_element,\n",
    "        \"src_doc\": src_doc\n",
    "    }\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate embeddings for text, table summary and image captions and define documents for each of the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for extracted_element in extracted_elements_list:\n",
    "    texts = extracted_element['texts']\n",
    "    tables = extracted_element['tables']\n",
    "    image_captions = extracted_element['images']\n",
    "    src_doc = extracted_element['source']\n",
    "    for text in texts:\n",
    "        embedding = invoke_model(text)\n",
    "        document = prep_document(embedding,text,text,'text',src_doc)\n",
    "        documents.append(document)\n",
    "    for table in tables:\n",
    "        table_raw = table['raw']\n",
    "        table_summary = table['summary']\n",
    "        embeddings = invoke_model(table_summary)\n",
    "        document = prep_document(embedding,table_raw,table_summary,'table',src_doc)\n",
    "        documents.append(document)\n",
    "    for image_caption in image_captions:\n",
    "        embedding = invoke_model(image_caption)\n",
    "        document = prep_document(embedding,image_caption,image_caption,'image',src_doc)\n",
    "        documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert each of the documents into aoss. Bulk indexing is not done at this point in time due to a [bug](https://repost.aws/zh-Hant/questions/QUxXol2_SQRb-7iYoouyjl8A/questions/QUxXol2_SQRb-7iYoouyjl8A/aws-opensearch-serverless-bulk-api-document-id-is-not-supported-in-create-index-operation-request?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    response = ospy_client.index(\n",
    "        index = index,\n",
    "        body = doc,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask a question.    \n",
    "Generate embeddings for the question.  \n",
    "Perform a search against aoss using the k nearest neighbours algorithm to retrieve the most relevant documents.  \n",
    "Here, the processed_element is returned as context but it can be the raw_element as well. E.g. table instead of table summary.  \n",
    "Pass these documents as context to the Claude LLM and obtain the result.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'your query'\n",
    "embedding = invoke_model(question)\n",
    "k = 4 # number of neighbours, size and k are the same to return k results in total. If size is not specified, k results will be returned per shard.\n",
    "query = {\n",
    "    \"size\": k,\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"processed_element_embedding\": {\n",
    "                \"vector\": embedding, \n",
    "                \"k\": k}\n",
    "            },\n",
    "    }\n",
    "}\n",
    "\n",
    "response = ospy_client.search(\n",
    "    body = query,\n",
    "    index = index\n",
    ")\n",
    "\n",
    "hits = response['hits']['hits']\n",
    "prompt_template = \"\"\"\n",
    "    The following is a friendly conversation between a human and an AI. \n",
    "    The AI is talkative and provides lots of specific details from its context.\n",
    "    If the AI does not know the answer to a question, it truthfully says it \n",
    "    does not know.\n",
    "    {context}\n",
    "    Instruction: Based on the above documents, provide a detailed answer for, {question} Answer \"don't know\" \n",
    "    if not present in the document. \n",
    "    Solution:\"\"\"\n",
    "context = []\n",
    "for hit in hits:\n",
    "    context.append(hit['_source']['processed_element'])\n",
    "\n",
    "\n",
    "llm_prompt = prompt_template.format(context='\\n'.join(context),question=question)\n",
    "output = invoke_llm_model(llm_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
