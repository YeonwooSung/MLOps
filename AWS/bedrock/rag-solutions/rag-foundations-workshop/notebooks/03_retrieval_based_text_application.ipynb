{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a5ab2f-d044-4956-b75b-7408d9c3e323",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation with Amazon Bedrock - Solving Contextual Limitations with RAG\n",
    "\n",
    "> *PLEASE NOTE: This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "Previously we saw that Amazon Bedrock could provide an answer to a technical question, however we had to manually provide it with the relevant data and provide the contex ourselves. While that approach works with short documents or single-ton applications, it fails to scale to enterprise level question answering where there could be large enterprise documents which cannot all be fit into the prompt sent to the model.\n",
    "\n",
    "We can improve upon this process by implementing an architecure called Retreival Augmented Generation (RAG). RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context. \n",
    "\n",
    "In this notebook we explain how to approach the pattern of Question Answering to find and leverage the documents to provide answers to the user questions.\n",
    "\n",
    "## Solution\n",
    "To the above challenges, this notebook uses the following strategy\n",
    "\n",
    "### Prepare documents for search\n",
    "![](./images/embeddings_lang.png)\n",
    "\n",
    "Before being able to answer the questions, the documents must be processed and a stored in a document store index\n",
    "- Load the documents\n",
    "- Process and split them into smaller chunks\n",
    "- Create a numerical vector representation of each chunk using Amazon Bedrock Titan Embeddings model\n",
    "- Create an index using the chunks and the corresponding embeddings\n",
    "\n",
    "### Respond to user question\n",
    "![Question](./images/chatbot_lang.png)\n",
    "\n",
    "When the documents index is prepared, you are ready to ask the questions and relevant documents will be fetched based on the question being asked. Following steps will be executed.\n",
    "- Create an embedding of the input question\n",
    "- Compare the question embedding with the embeddings in the index\n",
    "- Fetch the (top N) relevant document chunks\n",
    "- Add those chunks as part of the context in the prompt\n",
    "- Send the prompt to the model under Amazon Bedrock\n",
    "- Get the contextual answer based on the documents retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27610c0f-7de6-4440-8f76-decf30e3c5ca",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup the `boto3` client connection to Amazon Bedrock\n",
    "\n",
    "Just like previous notebooks, we will create a client side connection to Amazon Bedrock with the `boto3` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b2a05-78a9-40ca-9b5e-121030f9ede1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "region = os.environ.get(\"AWS_REGION\")\n",
    "boto3_bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6832f0-92fa-44d0-a736-505f83890c7b",
   "metadata": {},
   "source": [
    "---\n",
    "## Semantic Similarity with Amazon Titan Embeddings\n",
    "\n",
    "Semantic search refers to searching for information based on the meaning and concepts of words and phrases, rather than just matching keywords. Embedding models like Amazon Titan Embeddings allow semantic search by representing words and sentences as dense vectors that encode their semantic meaning.\n",
    "\n",
    "Semantic matching is extremely helpful for RAG because it returns results that are conceptually related to the user's query, even if they don't contain the exact keywords. This leads to more relevant and useful search results which can be injected into our LLM's prompts.\n",
    "\n",
    "First, let's take a look below to illustrate the capabilities of semantic search with Amazon Titan.\n",
    "\n",
    "The `embed_text_input` function below is an example function which will return an embedding output based on text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec61c7e-5d7b-4404-98ac-28d9246e8489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def embed_text_input(bedrock_client, prompt_data, modelId=\"amazon.titan-embed-text-v1\"):\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "    body = json.dumps({\"inputText\": prompt_data})\n",
    "    response = bedrock_client.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "    return np.array(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2b26ef-b83c-4c41-908f-61f7007038b4",
   "metadata": {},
   "source": [
    "To give an example of how this works, lets take a look at matching a user input to two \"documents\". We use a dot product calculation to rank the similarity between the input and each document, but there are many ways to do this in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd435b1-4cc4-4fc2-bc14-ea3912424d18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = 'Things to do on vacation'\n",
    "document_1 = 'swimming, site seeing, sky diving'\n",
    "document_2 = 'cleaning, note taking, studying'\n",
    "\n",
    "user_input_vector = embed_text_input(boto3_bedrock, user_input)\n",
    "document_1_vector = embed_text_input(boto3_bedrock, document_1)\n",
    "document_2_vector = embed_text_input(boto3_bedrock, document_2)\n",
    "\n",
    "doc_1_match_score = np.dot(user_input_vector, document_1_vector)\n",
    "doc_2_match_score = np.dot(user_input_vector, document_2_vector)\n",
    "\n",
    "print(f'\"{user_input}\" matches \"{document_1}\" with a score of {doc_1_match_score:.1f}')\n",
    "print(f'\"{user_input}\" matches \"{document_2}\" with a score of {doc_2_match_score:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad6f495-c290-4f9d-a5a5-5920d534e027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_input = 'Things to do that are productive'\n",
    "document_1 = 'swimming, site seeing, sky diving'\n",
    "document_2 = 'cleaning, note taking, studying'\n",
    "\n",
    "user_input_vector = embed_text_input(boto3_bedrock, user_input)\n",
    "document_1_vector = embed_text_input(boto3_bedrock, document_1)\n",
    "document_2_vector = embed_text_input(boto3_bedrock, document_2)\n",
    "\n",
    "doc_1_match_score = np.dot(user_input_vector, document_1_vector)\n",
    "doc_2_match_score = np.dot(user_input_vector, document_2_vector)\n",
    "\n",
    "print(f'\"{user_input}\" matches \"{document_1}\" with a score of {doc_1_match_score:.1f}')\n",
    "print(f'\"{user_input}\" matches \"{document_2}\" with a score of {doc_2_match_score:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2e609-e784-4c8f-802b-841c14aade56",
   "metadata": {},
   "source": [
    "The example above shows how the semantic meaning behind the user input and provided documents can be effectively ranked by Amazon Titan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0574847-34b2-464e-b2de-b0c7bfdce58a",
   "metadata": {},
   "source": [
    "---\n",
    "## Simplifying Search with LangChain and FAISS\n",
    "\n",
    "Two helpful tools that help set up these semantic similarity vector search engines are LangChain and FAISS. We will use LangChain to help prepare text documents, create an easy to use abstration to the Amazon Bedrock embedding model. We will use FAISS to create a searchable data structure for documents in vector formats.\n",
    "\n",
    "First, let's import the required LangChain libraries for the system. Notice that LangChain has a FAISS wrapper class which we will be using as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ad922-108b-4530-b40c-af79f2562ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a2acbb-ced8-4839-97de-46b8becf889f",
   "metadata": {},
   "source": [
    "### Prepare Text with LangChain\n",
    "\n",
    "In order to load our document into FAISS, we first need to split the document into smaller chunks.\n",
    "\n",
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt. Also the embeddings model has a limited length of input tokens, so for the sake of this use-case we are creating chunks of roughly 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9840eaef-1161-47d6-8f53-c11136c387de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the sagemaker FAQ list\n",
    "with open('../data/sagemaker/sagemaker_faqs.csv') as f:\n",
    "    doc = f.read()\n",
    "\n",
    "# create a loader\n",
    "docs = []\n",
    "loader = TextLoader('')\n",
    "docs.append(Document(page_content=doc))\n",
    "\n",
    "# split documents into chunks\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator='\\n',\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    "    \n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b3691-a569-4eb1-939e-9858cc6ef5c4",
   "metadata": {},
   "source": [
    "Below is an example of one of the document chunks. Notice how the semantic text could easily be searched to answer a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e8c2f7-5d2f-41a6-ba98-b9cc8e4115fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7501e4-ec5e-4ee6-bd6e-1c1a25ecd294",
   "metadata": {},
   "source": [
    "### Create an Embedding Store with FAISS\n",
    "\n",
    "Once the documents are prepared, LangChain's `BedrockEmbeddings` and `FAISS` classes make it very easy to create an in memory vector store as shown below.\n",
    "\n",
    "```python\n",
    "# create instantiation to embedding model\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"amazon.titan-embed-text-v1\"\n",
    ")\n",
    "\n",
    "# create vector store\n",
    "vs = FAISS.from_documents(split_docs, embedding_model)\n",
    "```\n",
    "\n",
    "For times sake in this lab, we have already run the code above and provided the FAISS index as a persistent file in the `faiss-index/langchain` directory. We load the vector store (along with a connection to the Titan embedding model) into memory with the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2f0b3a-bc10-4732-b931-f735e5e9a462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model = BedrockEmbeddings(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"amazon.titan-embed-text-v1\"\n",
    ")\n",
    "vs = FAISS.load_local('../faiss-index/langchain/', embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfcef82-dcc8-4b7b-a523-5d5e11450de4",
   "metadata": {},
   "source": [
    "### Search the FAISS Vector Store\n",
    "\n",
    "We can now use the `similarity_search` function to match a question to the best 3 chunks of text from our document which was loaded into FAISS. Notice how the search result is correctly matched to the input question :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c40af3-4956-4d66-8eb5-6a1efce56834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_results = vs.similarity_search(\n",
    "    'How are SageMaker JumpStart foundation models priced?', k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c160ba98-7fa2-4934-b526-12f5fbf82719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46adbf60-3efb-4589-9772-d8b8fd6068e3",
   "metadata": {},
   "source": [
    "---\n",
    "## Combine Search Results with Text Generation\n",
    "\n",
    "In the final section of this notebook, we can now combine our vector search capability with our LLM in order to dynamically provide context to answer questions effectively with RAG. \n",
    "\n",
    "First, we will start by using a utility from LangChain called prompt templates. The `PromptTemplate` class allows us to easily inject context and a human input into the Claude prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f869b78e-055f-4adc-bfec-66039b29d5a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "RAG_PROMPT_TEMPLATE = '''Here is some important context which can help inform the questions the Human asks.\n",
    "Make sure to not make anything up to answer the question if it is not provided in the context.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Human: {human_input}\n",
    "\n",
    "Assistant:\n",
    "'''\n",
    "PROMPT = PromptTemplate.from_template(RAG_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a88ccb-2cd7-4d42-92cb-986a1b32622d",
   "metadata": {},
   "source": [
    "Just like before, we will again use the `similarity_search` function to provide relevant context from our documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745986d4-37f0-4dcb-8173-776f65238db0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_input = 'How are SageMaker JumpStart foundation models priced?'\n",
    "search_results = vs.similarity_search(human_input, k=3)\n",
    "context_string = '\\n\\n'.join([f'Document {ind+1}: ' + i.page_content for ind, i in enumerate(search_results)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944e0370-3813-42bf-8c54-530ac8620ce7",
   "metadata": {},
   "source": [
    "Now we will augment the LangChain prompt template with the human input and the context from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6290ad0f-e5bb-4fb9-88a0-f3bf86e00bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = PROMPT.format(human_input=human_input, context=context_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19678b9-8141-4112-af1d-4876954a09fc",
   "metadata": {},
   "source": [
    "Finally, we will use the LangChain `Bedrock` class to call the Claude model with our augmented prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa08bd76-e269-4593-b6bf-75b2c3b0a611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Bedrock\n",
    "\n",
    "llm = Bedrock(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"anthropic.claude-instant-v1\",\n",
    "    model_kwargs={\n",
    "        \"max_tokens_to_sample\": 500,\n",
    "        \"temperature\": 0.9,\n",
    "    },\n",
    ")\n",
    "output = llm(prompt_data).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056971d2-7a44-4511-a6af-38f5de365848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Markdown(f'{output}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25428296",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Scaling Vector Databases\n",
    "\n",
    "In this lab, we have only used a local, in-memory vector database with FAISS. This is due to the fact that is this is a workshop and not a production setting. If you are looking for a way to easily scale this FAISS solution on AWS, check out [this example](https://github.com/aws-samples/sagemaker-vector-store-microservice) which utilize Amazon SageMaker to deploy a vector search microservice with FAISS.\n",
    "\n",
    "However, once you get to production and have billions (or more) vectors which need to be used in a RAG architecture, you will need to employ a larger scale solution which is purpose built and tuned for distributed vector search. AWS offers multiple ways to accomplish this this. Here are a few of the notable options available today.\n",
    "\n",
    "### Amazon Open Search\n",
    "\n",
    "The vector engine for Amazon OpenSearch Serverless introduces a simple, scalable, and high-performing vector storage and search capability that helps developers build machine learning (ML)–augmented search experiences and generative artificial intelligence (AI) applications without having to manage the vector database infrastructure. Get contextually relevant responses across billions of vectors in milliseconds by querying vector embeddings, which can be combined with text-based keywords in a single hybrid request.\n",
    "\n",
    "Check out these links for more information...\n",
    "* [Vector Engine for Amazon OpenSearch Serverless](https://aws.amazon.com/opensearch-service/serverless-vector-engine/)\n",
    "* [Amazon OpenSearch Service’s vector database capabilities explained](https://aws.amazon.com/blogs/big-data/amazon-opensearch-services-vector-database-capabilities-explained/)\n",
    "\n",
    "### Amazon Aurora with `pgvector`\n",
    "\n",
    "Amazon Aurora PostgreSQL-Compatible Edition now supports the pgvector extension to store embeddings from machine learning (ML) models in your database and to perform efficient similarity searches. pgvector can store and search embeddings from Amazon Bedrock which helps power vector search for RAG. pgvector on Aurora PostgreSQL is a great option for a vector database for teams who are looking for the power of semantic search in combination with tried and trusted Amazon Relational Database Services (RDS).\n",
    "\n",
    "Check out these links for more information...\n",
    "* [Feature announcement](https://aws.amazon.com/about-aws/whats-new/2023/07/amazon-aurora-postgresql-pgvector-vector-storage-similarity-search/)\n",
    "* [Leverage pgvector and Amazon Aurora PostgreSQL for Natural Language Processing, Chatbots and Sentiment Analysis](https://aws.amazon.com/blogs/database/leverage-pgvector-and-amazon-aurora-postgresql-for-natural-language-processing-chatbots-and-sentiment-analysis/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48a0e8-147d-4525-a6b2-68a09af1b2c4",
   "metadata": {},
   "source": [
    "---\n",
    "## Next steps\n",
    "\n",
    "Now you have been able to enhance your Amazon Bedrock LLM with RAG in order to better answer user questions with up-to-date context. In the next section, we will learn how to combine this solution with a chat based paradigm in order to create a more interactive application which utilizes RAG."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
