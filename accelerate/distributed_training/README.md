# accelerate distributed training recipes

## SLURM

To manage the resources of the nodes in the cluster for distributed training, we could use SLURM (Simple Linux Utility for Resource Management).

### SLURM Setup

1. Install MUNGE on all nodes (MUNGE is an authentication service for creating and validating credentials)

```bash
export MUNGEUSER=991
groupadd -g $MUNGEUSER munge
useradd  -m -c "MUNGE Uid 'N' Gid Emporium" -d /var/lib/munge -u $MUNGEUSER -g munge  -s /sbin/nologin munge
```
2. Install SLURM on all nodes

```bash
apt install slurm-wlm slurm-wlm-doc 
```

3. Setup Master Node's MUNGE

```bash
dd if=/dev/urandom of=/etc/munge/munge.key bs=1c count=4M
ls -l /etc/munge/munge.key
chmod a-r /etc/munge/munge.key
chmod u-w /etc/munge/munge.key $ chmod u+r /etc/munge/munge.key
chown munge:munge /etc/munge/munge.key
munge –n
munge –n | unmunge
scp /etc/munge/munge.key node1:/etc/munge/
systemctl enable munge
/etc/init.d/munge restart
```

4. Setup Slave Nodes' MUNGE

```bash
ls -l /etc/munge/munge.key
chmod a-r /etc/munge/munge.key
chmod u-w /etc/munge/munge.key 
chmod u+r /etc/munge/munge.key
chown munge:munge /etc/munge/munge.key
systemctl enable munge
/etc/init.d/munge restart
```

5. Check if Master Node can communicate with Slave Nodes

```bash
munge –n | ssh node1 unmunge 
```

6. Edit the SLURM configuration file `/etc/slurm-llnl/slurm.conf` on the Master Node

```bash
# slurm.conf file generated by configurator easy.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ControlMachine=node1
ControlAddr=IP주소
#
#MailProg=/bin/mail
MpiDefault=none
#MpiParams=ports=#-#
ProctrackType=proctrack/pgid
ReturnToService=1
#SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPidFile=/var/run/slurm-llnl/slurmctld.pid
#SlurmctldPort=6817
# SlurmdPidFile=/var/run/slurmd.pid
#SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser=slurm
#SlurmdUser=root
StateSaveLocation=/var/spool/slurmctld
SwitchType=switch/none
TaskPlugin=task/none
#
#
# TIMERS
#KillWait=30
#MinJobAge=300
#SlurmctldTimeout=120
#SlurmdTimeout=300
#
#
# SCHEDULING
FastSchedule=1
SchedulerType=sched/backfill
#SchedulerPort=7321
#SelectType=select/linear
SelectType=select/cons_res
SelectTypeParameters=CR_Core
#SelectTypeParameters=CR_Core_Memory
#
#
# LOGGING AND ACCOUNTING
AccountingStorageType=accounting_storage/none
ClusterName=cluster
#JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/none
#SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurmctld.log
#SlurmdDebug=3
SlurmdLogFile=/var/log/slurmd.log
#
#
# COMPUTE NODES
GresTypes=gpu
NodeName=node1 CPUs=32 Boards=1 SocketsPerBoard=2 CoresPerSocket=16 ThreadsPerCore=1 Gres=gpu:$gpuname$:8 State=UNKNOWN
NodeName=node2 CPUs=32 Boards=1 SocketsPerBoard=2 CoresPerSocket=16 ThreadsPerCore=1 Gres=gpu:$gpuname$:8 State=UNKNOWN
NodeName=node3 CPUs=32 Boards=1 SocketsPerBoard=2 CoresPerSocket=16 ThreadsPerCore=1 Gres=gpu:$gpuname$:8 State=UNKNOWN
PartitionName=batch Nodes=node1,node2,node3 Default=YES MaxTime=INFINITE State=UP OverSubscribe=FORCE
```

7. Set up additional GPU configuration in the SLURM configuration file

```bash
$ vim /etc/slurm-llnl/cgroup.conf

###cgroup.conf###
CgroupAutomount=yes
CgroupReleaseAgentDir="/etc/slurm/cgroup"

ConstrainCores=yes
ConstrainDevices=yes
ConstrainRAMSpace=yes
#################
```

8. Start the SLURM service

```bash
mkdir /var/spool/slurmctld
chown slurm: /var/spool/slurmctld
chmod 755 /var/spool/slurmctld
touch /var/log/slurmctld.log
chown slurm: /var/log/slurmctld.log
touch /var/log/slurm_jobacct.log /var/log/slurm_jobcomp.log
chown slurm: /var/log/slurm_jobacct.log /var/log/slurm_jobcomp.log
systemctl enable slurmctld
systemctl start slurmctld
```

9. Start the SLURM service on the Slave Nodes

```bash
mkdir /var/spool/slurmd
chown slurm: /var/spool/slurmd
chmod 755 /var/spool/slurmd
touch /var/log/slurmd.log
chown slurm: /var/log/slurmd.log
systemctl enable slurmd
/etc/init.d/slurmd restart
scontrol show nodes
sinfo –a
```

Now, the SLURM cluster is ready to use.
Use `srun` or other SLURM commands to utilize distributed nodes for HPC.

## References

Resources in this directory are copied from [huggingface/alignment-handbook](https://github.com/huggingface/alignment-handbook).
Credits to @huggingface.
