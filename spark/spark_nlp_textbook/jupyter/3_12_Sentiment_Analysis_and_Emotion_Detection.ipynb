{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "4otPNObURiQ_",
    "outputId": "98ded3f2-e709-410e-fb61-de16af4042ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
      "ERROR: could not open HSTS store at '/home/alex/.wget-hsts'. HSTS will be disabled.\n",
      "--2020-07-28 19:53:35--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz’\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  10.3MB/s    in 10s     \n",
      "\n",
      "2020-07-28 19:53:45 (7.84 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('aclImdb_v1.tar.gz'):\n",
    "    ! wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    ! tar xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t2oj6mZpSkV_"
   },
   "source": [
    "# Sentiment Analysis and Emotion Detection\n",
    "\n",
    "Sentiment  analysis is a set of techniques used for quantifying some sentiment based on text content. There are many community sites and e-commerce sites that allow users to comment and rate products and services. However, this is not the only place where people discuss products and services—there is also social media. We can leverage the data from the sites with comments and ratings to learn the relationship between the language used and positive or negative sentiment. These approaches can be extended to predicting the emotions of the author of a piece of text. Sentiment analysis is one of the most popular uses of NLP.\n",
    "\n",
    "For this application, we are trying to build a program that we can use to quantify movie reviews. Although many, but not all, movie reviewers use some quantifiable metrics—for example, thumbs up/down, stars, or letter grades, these are not normalized. Two reviewers who use a 10-point scale may have different distributions. One reviewer may give most movies a 4–6 range, where another gives a 6–8 range. We could normalize them, but what about the other reviewers who use different metrics or no metrics at all? It might be better if we build a model that looks at the reviews and produces a score. This way, we know that the scores from a given reviewer are based on the text of the review, instead of on an ad-hoc score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_z4gLvQJSk03"
   },
   "source": [
    "## Problem Statement and Constraints\n",
    "1. What is the problem we are trying to solve?\n",
    "\n",
    "We want to build an application that takes the text of a movie review and produces a score. We will use this to aggregate reviews, so this application will run as a batch process. We will surface this to the user using a display that shows how positively or negatively the movie was received. We will not worry about the other aspects of the presentation. We will assume that the display will be embedded in other content.\n",
    "\n",
    "2. What constraints are there?\n",
    "\n",
    "Here are our constraints:\n",
    "\n",
    "* We are assuming that we are working with English-language reviews.\n",
    "* We do not have much constraint on the speed of the program, since this is a batch offline process. We want to return the aggregate score for 95% of movies in less than 1 minute.\n",
    "* We want to make sure that our model is performing well on this task, so we will use a well-known data set. We will use the Large Movie Review Dataset based on IMDb user reviews.\n",
    "* We will assume that the input to this program is a JavaScript Object Notation (JSON) file of reviews. The output will be a score.\n",
    "* The model must have an F1 score of at least 0.7 on new data.\n",
    "\n",
    "It may seem unreasonable to set a desired metric threshold before we have even looked at the data, but this situation is common. Negotiating with stakeholders is important. If an arbitrary threshold has been set but experimentation reveals it is unrealistic, the data scientist should be able to explain to the stakeholders why the problem is more difficult than expected.\n",
    "\n",
    "As you work on the project, this list may change. The earlier you catch missed constraints, the better. If you discover a constraint just before deployment, it can be very expensive to fix. This is why we want to iterate with stakeholders during development.\n",
    "\n",
    "Now that we have listed our constraints, let's discuss how we can build our application.\n",
    "\n",
    "3. How do we solve the problem with the constraints?\n",
    "\n",
    "The first constraint, that the reviews are in English, actually makes our task easier. The second constraint, concerning how long it takes to calculate the aggregate score, controls how complex of a model we can build, but it is a light constraint. We have the IMDb data set. When we build our program, we will load from JSON. However, our modeling code does not need to follow such constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kXDRSUcCSlEf"
   },
   "source": [
    "## Plan the Project\n",
    "\n",
    "To plan the project, let's define what our acceptance criteria are. The product owner would normally define these by incorporating stakeholder requests. In this chapter, you are both product owner and developer.\n",
    "\n",
    "We want a script that does the following:\n",
    "\n",
    "* Takes a file with reviews in JSON objects, one per line\n",
    "* Returns distribution information based on the output from the model\n",
    "  * Mean\n",
    "  * Standard deviation\n",
    "  * Quartiles\n",
    "  * Min\n",
    "  * Max\n",
    "  \n",
    "We will use Spark NLP to process the data and a Spark MLlib model to predict the sentiment.\n",
    "\n",
    "Now that we have these high-level acceptance criteria, let's look at the data. First, we will load the data into DataFrames and add the label columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LnhT051jTDZw"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpIGJexeTEex"
   },
   "outputs": [],
   "source": [
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEJ0lRlTTFf5"
   },
   "outputs": [],
   "source": [
    "pos_train = spark.sparkContext.wholeTextFiles(\n",
    "    'aclImdb/train/pos/')\n",
    "neg_train = spark.sparkContext.wholeTextFiles(\n",
    "    'aclImdb/train/neg/')\n",
    "pos_test = spark.sparkContext.wholeTextFiles(\n",
    "    'aclImdb/test/pos/')\n",
    "neg_test = spark.sparkContext.wholeTextFiles(\n",
    "    'aclImdb/test/neg/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ck7B50PTHFX"
   },
   "outputs": [],
   "source": [
    "pos_train = spark.createDataFrame(pos_train, ['path', 'text'])\n",
    "pos_train = pos_train.repartition(100)\n",
    "pos_train = pos_train.withColumn('label', lit(1)).persist()\n",
    "neg_train = spark.createDataFrame(neg_train, ['path', 'text'])\n",
    "neg_train = neg_train.repartition(100)\n",
    "neg_train = neg_train.withColumn('label', lit(0)).persist()\n",
    "pos_test = spark.createDataFrame(pos_test, ['path', 'text'])\n",
    "pos_test = pos_test.repartition(100)\n",
    "pos_test = pos_test.withColumn('label', lit(1)).persist()\n",
    "neg_test = spark.createDataFrame(neg_test, ['path', 'text'])\n",
    "neg_test = neg_test.repartition(100)\n",
    "neg_test = neg_test.withColumn('label', lit(0)).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ycLBL7tNSlRa"
   },
   "source": [
    "Let's look at an example of a positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "bCuEaXcXTJ2I",
    "outputId": "80d0de68-c559-4385-e844-114358a4cc91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is one of may all-time favourite films. Parker Posey's character is over-the-top entertaining, and the librarian motif won't be lost on anyone who has ever worked in the books and stacks world.<br /><br />If you're a library student, RENT THIS. Then buy the poster and hang it on your wall. The soundtrack is highly recommendable too. I've shown this film to more library friends than any other -- they all fall in love with it.\n"
     ]
    }
   ],
   "source": [
    "print(pos_train.first()['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CBGeLwCQSls5"
   },
   "source": [
    "This seems like a clearly positive review. We can identify a few words that seem like a good signal, like \"best.\"\n",
    "\n",
    "Now, let's look an example of a negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "8_TgS_dOTTiH",
    "outputId": "4a5a3937-702a-4842-9300-823b1c62607a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw this movie at the Edmonton International Film Festival, with the great Dr. Uwe Boll in attendance.<br /><br />The film is, simply put, very, very bad. And no, not in the usual Uwe Boll \"so bad it's actually entertaining\" way, but just plain bad. The plot concerns a man who leads a terrible life (because of a past criminal record, apparently), can't get a job, and with an awful 900 pound cheating wife. This man turns to his cult-leader uncle in a plan to steal a truck load of toys that contain the bird flu virus. Al Qaeda also has designs on stealing the toys, and what follows is just under two hours of completely incomprehensible sex and violence.<br /><br />The acting is awful (except for Dave Foley, who really tries, despite it all), the jokes never rise above children being shot in the chest in slow motion, and people taking a poo. It's supposed to be satire, but I'm not sure of what.<br /><br />Think \"Airplane!\", but done by the creators of South Park, and without any jokes.\n"
     ]
    }
   ],
   "source": [
    "print(neg_train.first()['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5RsgTCcuSl7k"
   },
   "source": [
    "This is a clear example of a negative review. We see many words here that seem like solid indicators of negative sentiment, like \"awful\" and \"cheesy.\"\n",
    "\n",
    "Notice that there are some HTML artifacts that we will want to remove.\n",
    "\n",
    "Now, let's look at the corpus as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "Wz8iHZCsTWEX",
    "outputId": "5b9a2dce-4711-469b-fcaf-8ec447f49491"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_train size 12500\n",
      "neg_train size 12500\n",
      "pos_test size 12500\n",
      "neg_test size 12500\n"
     ]
    }
   ],
   "source": [
    "print('pos_train size', pos_train.count())\n",
    "print('neg_train size', neg_train.count())\n",
    "print('pos_test size', pos_test.count())\n",
    "print('neg_test size', neg_test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TGlR5SLGSmJA"
   },
   "source": [
    "So we have 50,000 documents. Having such an even distribution between positive and negative is artificial in this case. Let's look at the length of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "K0qYj3ZATYvg",
    "outputId": "84e0d26a-b91a-483f-c5b7-7f5796ab628d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1347.160240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1046.747365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>695.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>982.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1651.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13704.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text_len\n",
       "count  12500.000000\n",
       "mean    1347.160240\n",
       "std     1046.747365\n",
       "min       70.000000\n",
       "25%      695.000000\n",
       "50%      982.000000\n",
       "75%     1651.000000\n",
       "max    13704.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_train.selectExpr('length(text) AS text_len')\\\n",
    "    .toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6E_iuHUdSmfq"
   },
   "source": [
    "There appears to be a lot of variation in character lengths. This may be a useful feature. Text length may seem very low level, but it can often be useful information about a text. We may find that longer comments may be more likely to be negative due to rants. In this situation, it would be more useful if we had reviewer IDs, so we could get a sense of what is normal for a reviewer; alas, that is not in the data. \n",
    "\n",
    "Now that we have taken a brief look at the data, let's begin to design our solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jrX-FY5SSmol"
   },
   "source": [
    "## Design the Solution\n",
    "\n",
    "First, let's separate our project into two phases.\n",
    "\n",
    "1. Training and measuring the model\n",
    "\n",
    "The quality of modeling code is often overlooked. This is an important piece of a project. You will want to be able to hand off your experiment, so the code should be reusable. You also need the model to be reproducible, not just for academic reasons but also in case the model needs to be rebuilt for business purposes. You may also want to return to the project at some point to improve the model.\n",
    "\n",
    "One common way of making a modeling project reusable is to build a notebook, or a collection of notebooks. We won't cover that in this chapter, since the modeling project is straightforward.\n",
    "\n",
    "2. Building the script\n",
    "\n",
    "The script will take one argument, the path to the reviews in JSON format—one JSON-formatted review per line. It will output a JSON-formatted report on the distribution of reviews.\n",
    "\n",
    "The following are the acceptance criteria for the script:\n",
    "\n",
    "* It should have a helpful usage output\n",
    "* It should run in less than 1 minute (for 95% of movies)\n",
    "* It should output a file in the following format\n",
    "\n",
    "```\n",
    "{\n",
    "    \"count\": ###,\n",
    "    \"mean\": 0.###,\n",
    "    \"std\": 0.###,\n",
    "    \"median\": 0.###,\n",
    "    \"min\": 0.###,\n",
    "    \"max\": 0.###,\n",
    "}\n",
    "```\n",
    "\n",
    "The scores, which we will take the mean of, should be floating-point numbers between 0 and 1. Many classifiers output predicted probabilities, but this does an assumption on the output of this script.\n",
    "\n",
    "Now that we have a plan, let's implement it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_XdrXsKeSmxC"
   },
   "source": [
    "## Implement the Solution\n",
    "\n",
    "Recall the steps to a modeling project we discussed in the classification & regression chapter. Let's go through them here.\n",
    "\n",
    "1. Get data.\n",
    "2. Look at the data.\n",
    "3. Process data.\n",
    "\n",
    "We already have the data, and we have looked at it. Let's do some basic processing and store it so we can more quickly iterate on our model.\n",
    "\n",
    "First, let's combine positives and negatives into two data sets, train, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S3rTKuCMfZy8"
   },
   "outputs": [],
   "source": [
    "train = pos_train.unionAll(neg_train)\n",
    "test = pos_test.unionAll(neg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HwlzDJX-Sm1f"
   },
   "source": [
    "Now, let's use Spark NLP to process the data. We will save both the lemmatized and normalized tokens, as well as GloVe embeddings. This way, we can experiment with different features.\n",
    "\n",
    "Let's create our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "t5wcmS1nfcCe",
    "outputId": "0d4c0e61-9439-4e5c-962d-4a0cc0fb268d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n",
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "assembler = DocumentAssembler()\\\n",
    "    .setInputCol('text')\\\n",
    "    .setOutputCol('document')\n",
    "sentence = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentences\")\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols(['sentences'])\\\n",
    "    .setOutputCol('tokens')\n",
    "lemmatizer = LemmatizerModel.pretrained()\\\n",
    "    .setInputCols(['tokens'])\\\n",
    "    .setOutputCol('lemmas')\n",
    "normalizer = Normalizer()\\\n",
    "    .setCleanupPatterns([\n",
    "        '[^a-zA-Z.-]+', \n",
    "        '^[^a-zA-Z]+', \n",
    "        '[^a-zA-Z]+$',\n",
    "    ])\\\n",
    "    .setInputCols(['lemmas'])\\\n",
    "    .setOutputCol('normalized')\\\n",
    "    .setLowercase(True)\n",
    "glove = WordEmbeddingsModel.pretrained(name='glove_100d') \\\n",
    "    .setInputCols(['document', 'normalized']) \\\n",
    "    .setOutputCol('embeddings') \\\n",
    "\n",
    "nlp_pipeline = Pipeline().setStages([\n",
    "    assembler, sentence, tokenizer, \n",
    "    lemmatizer, normalizer, glove\n",
    "]).fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4y3k3MhLSnd7"
   },
   "source": [
    "Let's select just the values we are interested in—namely, the original data plus the normalized tokens and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CeXutAWufe-_"
   },
   "outputs": [],
   "source": [
    "train = nlp_pipeline.transform(train) \\\n",
    "    .selectExpr(\n",
    "        'path', 'text', 'label', \n",
    "        'normalized.result AS normalized', \n",
    "        'embeddings.embeddings'\n",
    "    )\n",
    "\n",
    "test = nlp_pipeline.transform(test) \\\n",
    "    .selectExpr(\n",
    "        'path', 'text', 'label', \n",
    "        'normalized.result AS normalized', \n",
    "        'embeddings.embeddings'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bpmdhHpLfgNu"
   },
   "outputs": [],
   "source": [
    "nlp_pipeline.write().overwrite().save('nlp_pipeline.3.12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wwOIKP1jSnaA"
   },
   "source": [
    "Recall the simplest version of doc2vec that we covered in the word embeddings chapter, in which we average the word vectors in a document to create a document vector. We will use this technique here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q43yIyNxfkTe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "\n",
    "def avg_wordvecs_fun(wordvecs):\n",
    "    return DenseVector(np.mean(wordvecs, axis=0))\n",
    "\n",
    "avg_wordvecs = spark.udf.register(\n",
    "    'avg_wordvecs', \n",
    "    avg_wordvecs_fun, \n",
    "    returnType=VectorUDT())\n",
    "\n",
    "train = train.withColumn('avg_wordvec', avg_wordvecs('embeddings'))\n",
    "test = test.withColumn('avg_wordvec', avg_wordvecs('embeddings'))\n",
    "train = train.drop('embeddings')\n",
    "test = test.drop('embeddings')\n",
    "train = train.persist()\n",
    "test = test.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h3x4Wiy9SmBy"
   },
   "source": [
    "4. Featurize\n",
    "\n",
    "Let's see how well our model does with just TF.IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qxYqihzIf4u9"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19ZdZo--f5xf"
   },
   "outputs": [],
   "source": [
    "tf = CountVectorizer()\\\n",
    "    .setInputCol('normalized')\\\n",
    "    .setOutputCol('tf')\n",
    "idf = IDF()\\\n",
    "    .setInputCol('tf')\\\n",
    "    .setOutputCol('tfidf')\n",
    "\n",
    "featurizer = Pipeline().setStages([tf, idf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zEFt8NiCSlzC"
   },
   "source": [
    "5. Model\n",
    "\n",
    "Now that we have our features, we can build our first model. Let's start with logistic regression, which is often a good baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQz5J1awf8lt"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZopKLgr4f9i0"
   },
   "outputs": [],
   "source": [
    "vec_assembler = VectorAssembler()\\\n",
    "    .setInputCols(['avg_wordvec'])\\\n",
    "    .setOutputCol('features')\n",
    "logreg = LogisticRegression()\\\n",
    "    .setFeaturesCol('features')\\\n",
    "    .setLabelCol('label')\n",
    "\n",
    "model_pipeline = Pipeline()\\\n",
    "    .setStages([featurizer, vec_assembler, logreg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9sRYbDchf-eH"
   },
   "outputs": [],
   "source": [
    "model = model_pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "keLYpYa0Slkv"
   },
   "source": [
    "Now let's save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0JlADVQgBK-"
   },
   "outputs": [],
   "source": [
    "model.write().overwrite().save('model.3.12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9H-OCClrSlKk"
   },
   "source": [
    "Now that we have fit a model, let's get our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BOw2RCekgD7N"
   },
   "outputs": [],
   "source": [
    "train_preds = model.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2OfwpFoHgFCu"
   },
   "outputs": [],
   "source": [
    "test_preds = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1JZLKjEwSk81"
   },
   "source": [
    "6. Evaluate\n",
    "\n",
    "Let's calculate our F1 score on train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7TSg7ibEgHvk"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kx2w6FSugJA8"
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator()\\\n",
    "    .setMetricName('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "41P9dvaDgKB_",
    "outputId": "54e6e5de-b4a0-4c84-f9e9-3391496645f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8024794425179784"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "l3-dEE37gLPk",
    "outputId": "f83263bf-cef4-40f9-d497-31e5cdb19de5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8013948747166948"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oua4SlRNSkr9"
   },
   "source": [
    "This is above the minimal acceptance criteria, so we are ready to ship this model.\n",
    "\n",
    "7. Review\n",
    "\n",
    "We can, of course, identify ways to improve the model. But it is important to get a first version out. After we have deployed an initial version, we can begin to look at ways to improve the model.\n",
    "\n",
    "8. Deploy\n",
    "\n",
    "For this application, deployment is merely making the script available. Realistically, offline \"deployments\" often involve creating a workflow that can be run on demand or periodically. For this application, having the script in a place that can be run for new reviews is all that is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3pZF49zPgP9s",
    "outputId": "a467687a-d318-454e-df4a-d2adc53f951f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing movie_review_analysis.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile movie_review_analysis.py\n",
    "\n",
    "\"\"\"\n",
    "This script takes file containing reviews of the same. \n",
    "It will output the results of the analysis to std.out.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import argparse as ap\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('beginning...')\n",
    "    parser = ap.ArgumentParser(description='Movie Review Analysis')\n",
    "    parser.add_argument('-file', metavar='DATA', type=str,\n",
    "                        required=True, \n",
    "                        help='The file containing the reviews '\\\n",
    "                             'in JSON format, one JSON review '\\\n",
    "                             'per line')\n",
    "    \n",
    "    options = vars(parser.parse_args())\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"Movie Analysis\") \\\n",
    "        .config(\"spark.driver.memory\", \"12g\") \\\n",
    "        .config(\"spark.executor.memory\", \"12g\") \\\n",
    "        .config(\"spark.jars.packages\", \n",
    "                \"JohnSnowLabs:spark-nlp:2.2.2\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    nlp_pipeline = PipelineModel.load('nlp_pipeline.3.12')\n",
    "    model = PipelineModel.load('model.3.12')\n",
    "    \n",
    "    data = spark.read.json(options['file'])\n",
    "    \n",
    "    nlp_procd = nlp_pipeline.transform(data)\n",
    "    preds = model.transform(nlp_procd)\n",
    "    \n",
    "    results = preds.selectExpr(\n",
    "        'count(*)',\n",
    "        'mean(rawPrediction[1])',\n",
    "        'std(rawPrediction[1])',\n",
    "        'median(rawPrediction[1])',\n",
    "        'min(rawPrediction[1])',\n",
    "        'max(rawPrediction[1])',\n",
    "    ).first().asDict()\n",
    "    \n",
    "    print(json.dump(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_jcCBrONSkHv"
   },
   "source": [
    "This script can be used for taking a set of reviews and aggregating into a single score plus some additional statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KtSujnrqgVIm"
   },
   "source": [
    "## Test and Measure the Solution\n",
    "\n",
    "Now that we have a first implementation of the application, let's talk about metrics. In a more realistic scenario, you would define your metrics in the planning stage. However, it is easier to explain some of these topics once we have something concrete to refer to.\n",
    "\n",
    "### Business Metrics\n",
    "\n",
    "Normally, an NLP project ties into a new or existing product or service. In this case, let's say that the output of this script will be used in a film blog. Likely, you will already be tracking views. When first introducing this feature to the blog, you may want to do some A/B testing. Ideally, you would do the testing by randomly showing or not showing the score on blog entries. If that isn't technically feasible, you could show the scores in some entries and not in others during the initial deployment.\n",
    "\n",
    "Aggregated scores, like those produced by this tool, can be added to blog entries but may not necessarily affect views much. This feature may make your review more attractive for mentions by other outlets. This can be an additional metric. You can possibly capture this by logging where visitors are coming from.\n",
    "\n",
    "You might want to consider including the aggregate in the messages and notifications you send out. For example, if you notify people of new entries via email, you can include the aggregate in the subject. Also, consider adding the aggregate in the title of the entry.\n",
    "\n",
    "Once you have decided on your business metrics, you can start working on the technical metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hH3TG6x2gVWi"
   },
   "source": [
    "### Model-Centric Metrics\n",
    "\n",
    "For sentiment analysis, you will generally be using classification metrics, like we are here. Sometimes, sentiment labels have grades—for example, very bad, bad, neutral, good, very good. In these situations you can potentially build a regression model instead of a classifier.\n",
    "\n",
    "There are traditional metrics used with classifiers like precision and recall. In order to make sense, you need to decide which label is the \"positive.\" With precision and recall, \"positive\" is in the sense of \"testing positive.\" This can make discussing these metrics a little confusing. Let's say that for this application, the good sentiment has the positive label. Assuming this, precision is the proportion of reviews that are predicted to be good that are actually good. Recall is the proportion of actually good reviews that are predicted to be good.  Another common classification metric used with precision and recall is f-score. This is the harmonic mean of precision and recall. This is a convenient way of summarizing these metrics. We can calculate precision, recall, and the f-score with MulticlassClassificationEvaluator in Spark.\n",
    "\n",
    "  Another way of measuring classifier models is with a metric called log-loss. This is also called cross-entropy. The idea behind log-loss is measuring how different the observed distribution of labels is from the predicted distribution. This has the benefit of not relying on a mapping of the meaning of good and bad labels to positive and negative. On the downside, this is less interpretable than precision and recall.\n",
    "\n",
    "When deciding on your model metrics, you should decide on which ones will be useful for experimentation and what singular metric is best for reporting out to share with stakeholders. The metric you select for stakeholders should be one that can be easily explained to an audience that may not be familiar with data-science concepts.\n",
    "\n",
    "An important part of every machine learning project is deployment. You want to make sure that you have metrics for this stage as well.\n",
    "\n",
    "### Infrastructure Metrics\n",
    "\n",
    "The infrastructure metrics you choose depend on how your application is deployed. In this case, because the application is a script you likely want to measure the time it takes to run the script. We could put this in the script, but if we are deploying this in some sort of a workflow system, it will likely measure this.\n",
    "\n",
    "We will talk about more common infrastructure metrics when we get to other applications. Now that we have talked about the metrics for monitoring the technology behind our application, let's talk about metrics we can use to make sure that we are properly supporting the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-a86M2rngVmR"
   },
   "source": [
    "### Process Metrics\n",
    "\n",
    "There are many software development metrics out there, and most of them depend on how you track work—for example, number of tickets per unit of time or average time from opening a ticket to closing a ticket.\n",
    "\n",
    "In an application like this, you are not developing new features, so ticket-based metrics won't make sense. You can measure responsiveness to bugs with this. The process around this application is that of evaluating reviews for a movie. Measure how long it takes to gather the aggregate score for a new movie. As you automate the process of submitting a set of reviews for aggregation, this will improve.\n",
    "\n",
    "Another valuable metric for machine-learning–based applications is how long it takes to develop a new model. A simple model like this should not require more than a week, including gathering data, data validation, iterating on model training, documenting results, and deployment. If you find that making a new model takes prohibitively long, try and determine what part of the development process is slowing you down. The following are some common problems:\n",
    "\n",
    "* Data problems are discovered when iterating on the model\n",
    "  * Do improve the data validation so these problems are caught earlier\n",
    "  * Don't just remove the problems in an ad-hoc way without knowing the scope of the data problem—this can lead to invalid models\n",
    "* Every time a new model is needed there is too much data-cleaning work required\n",
    "  * Do improve the ETL pipeline to handle some of this cleaning in an automated manner, or, if possible, find a better data source\n",
    "  * Don't pad the time necessary to build the model and accept that each new developer cleans the data—this can lead to inconsistent models over time\n",
    "* The score of a new model is very different than the previous model\n",
    "  * Do review the evaluation code used in the current and previous models; the difference may be valid, or the measurement code could have a bug\n",
    "  * Don't ignore these changes—this can lead to deploying a worse model that was improperly measured\n",
    "\n",
    "Now that we have ways to measure the technology and processes of our application, let's talk about monitoring. This is essential in data-science–based applications because we make assumptions about the data when doing modeling. These assumptions may not hold in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q4qISw5KgV0L"
   },
   "source": [
    "### Offline Versus Online Model Measurement\n",
    "\n",
    "The difficulty in monitoring a model is that we generally do not have labels in production. So we can't measure our model with things like precision or root mean square error (RMSE). What we should do is measure that the distributions of features and predictions in production are similar to what we saw during experimentation. We can measure this in offline applications—in other words, applications that are run at request like the application in this chapter and online applications like a model that is available as a web service.\n",
    "\n",
    "For an application like this, we have only offline measurement. We should track the aggregates over time. Naïvely, we can assume that the mean average score for movies should be stable. This may very well not be the case, but if there is a trend, we should review the data to make sure that reviews are indeed changing overall and it's not that our model may have been overfit.\n",
    "\n",
    "When we look at applications that are deployed as real-time applications, we will discuss online metrics. In spirit, they are similar—they monitor the features and the scores.\n",
    "\n",
    "There is one more step we need to discuss for this application—that is the review. Data-science–based applications are more complicated to review than most other software because you must review the actual software just as thoroughly as any other application, but you must also review the methodology. NLP applications are even more complicated. The theory behind linguistics and natural language data is not as cleanly modeled as other simpler kinds of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_2FjP4vgWD0"
   },
   "source": [
    "## Review\n",
    "\n",
    "The review process is another vital part of writing any application. It is easy for a developer to have blind spots in their own projects. This is why we must bring in others to review our work. This process can be difficult for technical and human reasons. The most important part of any review process is that it not be personal. Both the reviewer and the developer should approach the process with the goal of collaboration. If a problem is found, that is an opportunity. The developer has avoided a later problem, and the reviewer can learning something that may help them avoid problems in future work.\n",
    "\n",
    "Let's talk about the steps of the review.\n",
    "\n",
    "1. Architecture review: this is where other engineers, product owners, and stakeholders review how the application will be deployed. This should be done at the end of the planning stage of development\n",
    "\n",
    "2. Model review: this is done when the developer or data scientist has a model that they believe will meet the expectations of the stakeholder. The model should be reviewed with other data scientists or those familiar with machine learning concepts, and another review should be conducted with stakeholders. The technical review should cover the data, processing, modeling, and measurement aspects of the project. The nontechnical review should explain the assumptions and limitations of the model to verify that it will meet the expectations of the stakeholders.\n",
    "\n",
    "3. Code reviews: this is necessary for any software application. The code should be reviewed by someone who has some knowledge of the project. If the reviewer has no context on the application, it will be difficult or impossible for them to catch logical bugs in the code.\n",
    "\n",
    "In our situation, this application is very simple. We are not developing anything but a script, so there is no actual architecture to review. However, the plan to deploy this as a script must be reviewed by the stakeholders. The model review would also be straightforward, since we have a clean data set and simple model. This will generally not be the case. Similarly, our script is very simple. A code review might suggest that we develop a small test data set to make sure that a model will run the data we expect.\n",
    "\n",
    "These reviews take place during the development of the model. Once we are ready to deploy, we should have some more reviews to make sure that we have prepared for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N94huYQqgWRJ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RXpIzdEggWgN"
   },
   "source": [
    "## Initial Deployment\n",
    "\n",
    "Work with your product owner and DevOps (if you have DevOps) to discuss how your project will be deployed. In this situation, our project is merely a script, so there is not an actual deployment.\n",
    "\n",
    "### Fallback Plans\n",
    "\n",
    "When deploying your application, you should also have a fallback plan. If there is a major problem, can you bring down your application until it is fixed, or must there be something there no matter what? If it is the latter, consider having a \"dummy\" stand-in that you can deploy. You should work out the specifics with the stakeholders. Ideally, this should be discussed early in the project because this can help guide development and testing.\n",
    "\n",
    "In our situation, this script is not mission critical. If the script doesn't run, it will cause a delay only in the use of the aggregate score. Perhaps a backup script that uses a much simpler model could be devised if there absolutely must be a score added.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Finally, once you are ready to deploy, you should decide what will be the next steps. In our situation, we would likely want to talk about how the model can be improved. The model's performance is not terrible, but it is well below state-of-the-art. Perhaps we can consider building a more complex model.\n",
    "\n",
    "Additionally, we may eventually want to put this model behind a service. This will allow reviews to be scored immediately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvipfjLcgWYy"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Now we have built our first application. This is a simple application, but it has allowed us to learn many things about how we will deploy more complex applications. In the next chapter we will again be looking at an offline application, but this will not be based on just the output of a model. We will be building an ontology that we can query.\n",
    "\n",
    "Many Spark-based applications are offline tools like this. If we want to serve a live model behind a service, we would need to look elsewhere. There are several options for this, which we will discuss in #productionizing_nlp_applications.\n",
    "\n",
    "Sentiment analysis is a fascinating task, and it uses the tools and techniques we have already covered. There are more complex examples, but by using good development processes we can always grow a simple application."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM0aQOzJjdP5j8h5tXmGrq+",
   "include_colab_link": true,
   "name": "3.12_Sentiment_Analysis_and_Emotion_Detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
