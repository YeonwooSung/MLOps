{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "rez6nNB0nM9P",
    "outputId": "17c0dedd-1c3f-46ba-c2a2-27fa8a134e0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alex/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  import nltk\n",
    "  \n",
    "  nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0nwEWxhvmBPB"
   },
   "source": [
    "# Chatbot\n",
    "\n",
    " When we discussed language models, we showed how we can generate text. Building a chatbot is similar, except that we are modeling an exchange. This can make our requirements more complex or, actually, more simple depending on how we want to approach the problem.\n",
    "\n",
    "In this chapter we will discuss some of the ways this can be modeled, and then we will build a program that will use a generative model to take and then generate responses.  First, let's talk about what discourse is.\n",
    "\n",
    "Morphology and syntax tell us how morphemes are combined into words, and words into phrases and sentences. The combination of sentences into larger language acts is not as easily modeled. There is an idea of an inappropriate combination of sentences. Let's look at some examples:\n",
    "\n",
    "> I went to the doctor, yesterday. It is just a sprained ankle.\n",
    "> I went to the doctor, yesterday. Mosquitoes have 47 teeth.\n",
    "\n",
    "In the first example, the second sentence is obviously related to the first. From these two sentences, combined with common knowledge, we can infer that the speaker went to the doctor for an ankle problem that turned out to be a sprain. The second example makes no sense. From a linguistics point of view, sentences are generated from concepts and then encoded into words and phrases. The concepts that are expressed by sentences are connected, so a sequence of sentences should be connected by similar concepts. This will be true whether there is only one speaker or more in a conversation.\n",
    "\n",
    "The pragmatics of a discourse is important to understanding how to model it. If we are modeling a customer-service exchange, the range of responses can be limited.  These limited types of responses are often called intents. When building a customer-service chatbot, this greatly reduces the potential complexity. If we are modeling general conversation, this can become much more difficult. Language models learn what is likely to occur in a sequence, but they cannot learn to generate concepts. So our choice is to either build something that models the probable sequences or find a way to cheat.\n",
    "\n",
    "We can cheat by building canned responses to unrecognized intents. For example, if the user makes a statement that our simple model is not expecting, we can have it respond with, \"Sorry, I don't understand.\" If we are logging the conversations, we can use exchanges that use the canned responses to expand the intents we cover.\n",
    "\n",
    "In the example we are covering, we will be building a program that purely models the full text of the discourse. Essentially, it is a language model. The difference will be in how we use it.\n",
    "\n",
    "This chapter is different than previous ones in that it doesn't make use of Spark. Spark is great for processing large amounts of data in batches. It's not great in interactive applications. Also, recurrent neural networks can take a long time to train with large amounts of data. So, in this chapter we are working a small piece of data. If you have the right hardware, you change the NLTK processing to use Spark NLP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oa1FM_qcmBMS"
   },
   "source": [
    "## Problem Statement and Constraints\n",
    " We will build a story-building tool. The idea is to help someone write an original story similar to one of the Grimm fairy tales. This model will be much more complex, in the sense of containing many more parameters, than the previous language model was. The program will be a script that asks for an input sentence and generates a new sentence. The user then takes that sentence, modifies and corrects it, and enters it.\n",
    "\n",
    "1. What is the problem we are trying to solve?\n",
    "\n",
    "We want a system that will recommend the next sentence in a story. We also must recognize the limitations of text generation techniques. We will need to have the user in the loop. So we need a model that can generate related text and a system that lets us review the output.\n",
    "\n",
    "2. What constraints are there?\n",
    "\n",
    "First, we need a model that has two notions of context—the previous sentence and the current sentence. We don't need to worry about performance as much, since this will be interacting with a person. This might seem counterintuitive because most interactive systems require quite low latency. However, if you consider what this program is producing, it is not unreasonable to wait one to three seconds for a response.\n",
    "\n",
    "3. How do we solve the problem with the constraints?\n",
    "\n",
    "We will be building a neural network for generating text, specifically an RNN, as discussed in Chapters 4 and 8. We could learn the word embeddings in this model, but we can instead use a prebuilt embedding. This will help us train a model more quickly.\n",
    "\n",
    "## Plan the Project\n",
    " Most of the work on this project will be developing a model. Once we have a model, we will build a simple script that we can use to write our own Grimm-style fairy tale. Once we've developed this script, this model could potentially be used to power a Twitter bot or Slackbot.\n",
    "\n",
    "In a real production setting for text generation, we would want to monitor the quality of generated text. This would allow us to improve the generated text over time by developing more targeted training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_8lW1PNnmBHM"
   },
   "source": [
    "## Design the Solution\n",
    " If\n",
    "  you recall our language model, we used three layers.\n",
    "\n",
    "1. nput\n",
    "2. Embedding\n",
    "3. LSTM\n",
    "4. Dense output\n",
    "\n",
    "We input windows of characters of a fixed size and predicted the following character. Now we need to find a way to take into account larger portions of text. There are a couple of options.\n",
    "\n",
    "Many RNN architectures include a layer for learning an embedding for the words. This would merely require us to learn more parameters, so we will use a pretrained GloVe model instead. Also, we will be building our model on the token level, and not on the character level as before.\n",
    "\n",
    "We could make the window size much larger than the average sentence. This has the benefit of keeping the same model architecture. The downside is that our LSTM layer will have to maintain information over quite long distances. We can use one of the architectures used for machine translations.\n",
    "\n",
    "Let's consider the concatenating approach.\n",
    "\n",
    "1. Context input\n",
    "2. Context LSTM\n",
    "3. Current input\n",
    "4. Current LSTM\n",
    "5. Concatenate 2 and 4\n",
    "6. Dense output\n",
    "\n",
    "The current inputs will be windows over sentences, so for each window of a given sentence we will use the same context vector. This approach has the benefit of being able to be extended to multiple sentences. The downside is that the model has to learn to balance the information from far away and from nearby.\n",
    "\n",
    "Let's consider the stateful approach.\n",
    "\n",
    "Context input\n",
    "\n",
    "1. Context LSTM\n",
    "2. Current input\n",
    "3. Current LSTM, initialized with state of 2\n",
    "4. Dense output\n",
    "\n",
    "This helps make training easier by reducing the influence of the previous sentence. This is a double-edged sword, however, because the context gives us less information. We will be using this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4o4FFYrzmBJv"
   },
   "source": [
    "## Implement the Solution\n",
    " Let's start out by doing our imports. This chapter will rely on Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8Nicd-QQmuc8",
    "outputId": "09f45570-1154-484a-c58c-bcbdd52f44b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, CuDNNLSTM\n",
    "from keras.layers.merge import Concatenate\n",
    "import keras.utils as ku\n",
    "import keras.preprocessing as kp\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8UcILRsgmvzV"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XDQ1uK_emBEw"
   },
   "source": [
    "Let's also define some special tokens for the beginning and ending of sentences, as well as for unknown tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uOQJwNJ6m8kk"
   },
   "outputs": [],
   "source": [
    "START = '>'\n",
    "END = '###'\n",
    "UNK = '???'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cu1QzKnVmBCH"
   },
   "source": [
    "Now, we can load the data. We will need to replace some of the special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oieLLncym_EK"
   },
   "outputs": [],
   "source": [
    "with open('../grimms_fairytales.txt', encoding='UTF-8') as fp:\n",
    "    text = fp.read()\n",
    "    \n",
    "text = text\\\n",
    "    .replace('\\t', ' ')\\\n",
    "    .replace('“', '\"')\\\n",
    "    .replace('”', '\"')\\\n",
    "    .replace('“', '\"')\\\n",
    "    .replace('‘', \"'\")\\\n",
    "    .replace('’', \"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ErcLRkkwmA_z"
   },
   "source": [
    "Now, we can process our text into tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iIbxKL7JnBgW"
   },
   "outputs": [],
   "source": [
    "sentences = nltk.tokenize.sent_tokenize(text)\n",
    "sentences = [s.strip()for s in sentences]\n",
    "sentences = [[t.lower() for t in nltk.tokenize.wordpunct_tokenize(s)] for s in sentences]\n",
    "word_counts = Counter([t for s in sentences for t in s])\n",
    "word_counts = pd.Series(word_counts)\n",
    "vocab = [START, END, UNK] + list(sorted(word_counts.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgWM0I9HmA9P"
   },
   "source": [
    "We need to define some hyperparameters for our model.\n",
    "\n",
    "* dim is the size of the token embeddings\n",
    "* `w` is the size of the windows we'll use\n",
    "* `max_len` is the sentence length that we use\n",
    "* `units` is the size of the state vectors we'll use for our LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R7hGXBrvnIHu"
   },
   "outputs": [],
   "source": [
    "dim = 50\n",
    "w = 10\n",
    "max_len = int(np.quantile([len(s) for s in sentences], 0.95))\n",
    "units = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "smqB3kjWmA6w"
   },
   "source": [
    "Now, let's load the GloVe embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wy0OXlHlve00"
   },
   "source": [
    "You can download the GloVe model we arr using at `https://drive.google.com/file/d/1H-_WHRHlt6VmpJxPJjzAyO77UjuSg9mD/view?usp=sharing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_k3RZ8Fnatj"
   },
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open('glove.6B.50d.txt', encoding='utf-8') as fp:\n",
    "    for line in fp:\n",
    "        token, embedding = line.split(maxsplit=1)\n",
    "        if token in vocab:\n",
    "            embedding = np.fromstring(embedding, 'f', sep=' ')\n",
    "            glove[token] = embedding\n",
    "            \n",
    "vocab = list(sorted(glove.keys()))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OX-pPfGAmA4B"
   },
   "source": [
    "We will also need to have a lookup for the one-hot–encoded output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NfIRuQ9wvmnR"
   },
   "outputs": [],
   "source": [
    "i2t = dict(enumerate(vocab))\n",
    "t2i = {t: i for i, t in i2t.items()}\n",
    "\n",
    "token_oh = ku.to_categorical(np.arange(vocab_size))\n",
    "token_oh = {t: token_oh[i,:] for t, i in t2i.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_NKcDT_vmA1f"
   },
   "source": [
    "Now, we can define some utility functions.\n",
    "\n",
    "We will need to pad the end of the sentences; otherwise, we will not learn from the last words in the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a3Cj4xg7vozA"
   },
   "outputs": [],
   "source": [
    "def pad_sentence(sentence, length):\n",
    "    sentence = sentence[:length]\n",
    "    if len(sentence)  < length:\n",
    "        sentence += [END] * (length - len(sentence))\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VKfcb45QmAy4"
   },
   "source": [
    "We also need to convert sentences to matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1LLTPJM6vq4V"
   },
   "outputs": [],
   "source": [
    "def sent2mat(sentence, embedding):\n",
    "    mat = [embedding.get(t, embedding[UNK]) for t in sentence]\n",
    "    return np.array(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8Rupd9cmAwm"
   },
   "source": [
    "We need a function for converting sequences to a sequence of sliding windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7aCxXaAAvtJF"
   },
   "outputs": [],
   "source": [
    "def slide_seq(seq, w):\n",
    "    window = []\n",
    "    target = []\n",
    "    for i in range(len(seq)-w-1):\n",
    "        window.append(seq[i:i+w])\n",
    "        target.append(seq[i+w])\n",
    "    return window, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ScyFEVL3mAt_"
   },
   "source": [
    "Now we can build our input matrices. We will have two input matrices. One is from the context, and one is from the current sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Gfn-gGavvTe"
   },
   "outputs": [],
   "source": [
    "Xc = []\n",
    "Xi = []\n",
    "Y = []\n",
    "\n",
    "for i in range(len(sentences)-1):\n",
    "    \n",
    "    context_sentence = pad_sentence(sentences[i], max_len)\n",
    "    xc = sent2mat(context_sentence, glove)\n",
    "    \n",
    "    input_sentence = [START]*(w-1) + sentences[i+1] + [END]*(w-1)\n",
    "    for window, target in zip(*slide_seq(input_sentence, w)):\n",
    "        xi = sent2mat(window, glove)\n",
    "        y = token_oh.get(target, token_oh[UNK])\n",
    "    \n",
    "        Xc.append(np.copy(xc))\n",
    "        Xi.append(xi)\n",
    "        Y.append(y)\n",
    "    \n",
    "Xc = np.array(Xc)\n",
    "Xi = np.array(Xi)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "iQ1LMGlNvw89",
    "outputId": "6ea45a50-9efd-4c7b-a342-e5458f055f66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context sentence:  (42, 50)\n",
      "input sentence:  (10, 50)\n",
      "target sentence:  (4407,)\n"
     ]
    }
   ],
   "source": [
    "print('context sentence: ', xc.shape)\n",
    "print('input sentence: ', xi.shape)\n",
    "print('target sentence: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MljaQse-mArn"
   },
   "source": [
    "Let's build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "tF1E75J0vzml",
    "outputId": "79456fca-ff48-4de6-c5ee-6329d6fcb76d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/envs/spark-nlp-in-action/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "input_c = Input(shape=(max_len,dim,), dtype='float32')\n",
    "lstm_c, h, c = LSTM(units, return_state=True)(input_c)\n",
    "\n",
    "input_i = Input(shape=(w,dim,), dtype='float32')\n",
    "lstm_i = LSTM(units)(input_i, initial_state=[h, c])\n",
    "\n",
    "out = Dense(vocab_size, activation='softmax')(lstm_i)\n",
    "model = Model(input=[input_c, input_i], output=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "0w-WeltAv1Ot",
    "outputId": "4205d26d-ac26-4958-d0d5-6c7473601f49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 42, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10, 50)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 200), (None, 200800      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 200)          200800      input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4407)         885807      lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,287,407\n",
      "Trainable params: 1,287,407\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGTV-9GHv3H1"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy', optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "60sYobrVmApG"
   },
   "source": [
    "Now we can train our model. Depending on your hardware, this can potentially take four minutes per epoch on CPU. This is our most complex model yet with almost 1.3 million parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "1q2p_7fcwIj9",
    "outputId": "548bb046-e732-400f-f1a2-a21c555ebb8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "145061/145061 [==============================] - 381s 3ms/step - loss: 3.7793 - accuracy: 0.3901\n",
      "Epoch 2/10\n",
      "145061/145061 [==============================] - 316s 2ms/step - loss: 3.1681 - accuracy: 0.4325\n",
      "Epoch 3/10\n",
      "145061/145061 [==============================] - 289s 2ms/step - loss: 2.8946 - accuracy: 0.4517\n",
      "Epoch 4/10\n",
      "145061/145061 [==============================] - 288s 2ms/step - loss: 2.6826 - accuracy: 0.4671\n",
      "Epoch 5/10\n",
      "145061/145061 [==============================] - 288s 2ms/step - loss: 2.5016 - accuracy: 0.4816\n",
      "Epoch 6/10\n",
      "145061/145061 [==============================] - 287s 2ms/step - loss: 2.3462 - accuracy: 0.4963\n",
      "Epoch 7/10\n",
      "145061/145061 [==============================] - 289s 2ms/step - loss: 2.2066 - accuracy: 0.5140\n",
      "Epoch 8/10\n",
      "145061/145061 [==============================] - 288s 2ms/step - loss: 2.0827 - accuracy: 0.5315\n",
      "Epoch 9/10\n",
      "145061/145061 [==============================] - 290s 2ms/step - loss: 1.9729 - accuracy: 0.5499\n",
      "Epoch 10/10\n",
      "145061/145061 [==============================] - 289s 2ms/step - loss: 1.8736 - accuracy: 0.5653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8857047898>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xc, Xi], Y, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlmGv0PXmAmj"
   },
   "source": [
    "Once we have this model trained, we can try to generate some sentences. This function will need a context sentence and an input sentence—we can simply supply one word to begin. The function will append tokens to the input sentence until the END token is generated or we have hit the maximum allowed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zwKlAkALwMQP"
   },
   "outputs": [],
   "source": [
    "def generate_sentence(context_sentence, input_sentence, max_len=100):\n",
    "    context_sentence = [t.lower() for t in nltk.tokenize.wordpunct_tokenize(context_sentence)]\n",
    "    context_sentence = pad_sentence(context_sentence, max_len)\n",
    "    context_vector = sent2mat(context_sentence, glove)\n",
    "    input_sentence = [t.lower() for t in nltk.tokenize.wordpunct_tokenize(input_sentence)]\n",
    "    input_sentence = [START] * (w-1) + input_sentence\n",
    "    input_sentence = input_sentence[:w]\n",
    "    output_sentence = input_sentence\n",
    "\n",
    "    input_vector = sent2mat(input_sentence, glove)\n",
    "    predicted_vector = model.predict([[context_vector], [input_vector]])\n",
    "    predicted_token = i2t[np.argmax(predicted_vector)]\n",
    "    output_sentence.append(predicted_token)\n",
    "    i = 0\n",
    "    while predicted_token != END and i < max_len:\n",
    "        input_sentence = input_sentence[1:w] + [predicted_token]\n",
    "        input_vector = sent2mat(input_sentence, glove)\n",
    "        predicted_vector = model.predict([[context_vector], [input_vector]])\n",
    "        predicted_token = i2t[np.argmax(predicted_vector)]\n",
    "        output_sentence.append(predicted_token)\n",
    "        i += 1\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k4Qogm6DmAjw"
   },
   "source": [
    "Because we need to supply the first word of the new sentence, we can simply sample from the beginning tokens found in our corpus. Let's save the distribution of first words that we will need as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQuYP8oSwPoj"
   },
   "outputs": [],
   "source": [
    "first_words = Counter([s[0] for s in sentences])\n",
    "first_words = pd.Series(first_words)\n",
    "first_words = first_words / first_words.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "PoowOdxLMCcu",
    "outputId": "26e1c933-0ddd-4717-ac87-08f8d9479a2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the       0.127373\n",
       "in        0.015756\n",
       "close     0.000380\n",
       "when      0.055809\n",
       "and       0.067957\n",
       "            ...   \n",
       "high      0.000190\n",
       "late      0.000190\n",
       "maid      0.000380\n",
       "meat      0.000190\n",
       "lights    0.000190\n",
       "Length: 290, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VSj901ERwQ3H"
   },
   "outputs": [],
   "source": [
    "first_words.to_json('grimm-first-words.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k2VWqBm7wR5P"
   },
   "outputs": [],
   "source": [
    "with open('glove-dict.pkl', 'wb') as out:\n",
    "    pkl.dump(glove, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2M7dXFYwSuU"
   },
   "outputs": [],
   "source": [
    "with open('vocab.pkl', 'wb') as out:\n",
    "    pkl.dump(i2t, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9MZ5wb-amAhE"
   },
   "source": [
    "Let's see what is generated without human intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "jNwM-UvwwUvF",
    "outputId": "51705c04-b7fd-4f00-ca12-8374a35a4ca6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In old times, when wishing was having, there lived a King whose daughters were all beautiful, but the youngest was so beautiful that the sun itself, which has seen so much, was astonished whenever it shone in her face. ###\n",
      "\" what did you take her ??? ###\n",
      "the king ' s son was called to the king ' s daughter , and she was not to get up again , but the king ' s son was standing himself , and when he saw that the prince was gazing at ###\n",
      "the king ' s son was beside upon the table , and the whole palace which was in the centre of the hall , and when he saw the beautiful princess was sitting at the table , and when she saw the beautiful ###\n",
      "\" what did you take her ??? ###\n",
      "cannon - plant . ###\n",
      "but the king ' s son was about to carry , and the king ' s son was called to be celebrated , and the king ' s son was called to be celebrated , and the king ' s son was called ###\n",
      "then the king ' s son was about to be married , and she was not to go on the night . ###\n",
      "\" what did you take her ??? ###\n",
      "he was going to seek him , and he could not get rid of them . ###\n",
      "then the king ordered the tailor to be brought to the gardener , and when he was afraid , and said , \" i have been a white dove , and on the same time , there was a small porch , and ###\n"
     ]
    }
   ],
   "source": [
    "context_sentence = '''\n",
    "In old times, when wishing was having, there lived a King whose\n",
    "daughters were all beautiful, but the youngest was so beautiful that\n",
    "the sun itself, which has seen so much, was astonished whenever it\n",
    "shone in her face.\n",
    "'''.strip().replace('\\n', ' ')\n",
    "\n",
    "input_sentence = np.random.choice(first_words.index, p=first_words)\n",
    "\n",
    "for _ in range(10):\n",
    "    print(context_sentence, END)\n",
    "    output_sentence = generate_sentence(context_sentence, input_sentence, max_len)\n",
    "    output_sentence = ' '.join(output_sentence[w-1:-1])\n",
    "    context_sentence = output_sentence\n",
    "    input_sentence = np.random.choice(first_words.index, p=first_words)\n",
    "print(output_sentence, END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x3qlMwSrmAeH"
   },
   "source": [
    "This model won't be passing the Turing test any time soon. This is why we need to have a human in the loop. Let's build our script. First, let's save our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VAdLezIPwbqO"
   },
   "outputs": [],
   "source": [
    "model.save('grimm-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MWQfpUbDmAbG"
   },
   "source": [
    "Our script will need to have access to some of our utility functions, as well as to the hyperparameters—for example, `dim`, w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nwImOlUZwfnN",
    "outputId": "51c42c38-55cb-4f45-d091-fcf6552c0b9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fairywriter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fairywriter.py\n",
    "\"\"\"\n",
    "This script helps you generate a fairytale.\n",
    "\"\"\"\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import load_model\n",
    "import keras.utils as ku\n",
    "import keras.preprocessing as kp\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "START = '>'\n",
    "END = '###'\n",
    "UNK = '???'\n",
    "\n",
    "\n",
    "FINISH_CMDS = ['finish', 'f']\n",
    "BACK_CMDS = ['back', 'b']\n",
    "QUIT_CMDS = ['quit', 'q']\n",
    "CMD_PROMPT = ' | '.join(','.join(c) for c in [FINISH_CMDS, BACK_CMDS, QUIT_CMDS])\n",
    "QUIT_PROMPT = '\"{}\" to quit'.format('\" or \"'.join(QUIT_CMDS))\n",
    "ENDING = ['THE END']\n",
    "\n",
    "\n",
    "def pad_sentence(sentence, length):\n",
    "    sentence = sentence[:length]\n",
    "    if len(sentence)  < length:\n",
    "        sentence += [END] * (length - len(sentence))\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def sent2mat(sentence, embedding):\n",
    "    mat = [embedding.get(t, embedding[UNK]) for t in sentence]\n",
    "    return np.array(mat)\n",
    "\n",
    "\n",
    "def generate_sentence(context_sentence, input_sentence, vocab, max_len=100, hparams=(42, 50, 10)):\n",
    "    max_len, dim, w = hparams\n",
    "    context_sentence = [t.lower() for t in nltk.tokenize.wordpunct_tokenize(context_sentence)]\n",
    "    context_sentence = pad_sentence(context_sentence, max_len)\n",
    "    context_vector = sent2mat(context_sentence, glove)\n",
    "    input_sentence = [t.lower() for t in nltk.tokenize.wordpunct_tokenize(input_sentence)]\n",
    "    input_sentence = [START] * (w-1) + input_sentence\n",
    "    input_sentence = input_sentence[:w]\n",
    "    output_sentence = input_sentence\n",
    "\n",
    "    input_vector = sent2mat(input_sentence, glove)\n",
    "    predicted_vector = model.predict([[context_vector], [input_vector]])\n",
    "    predicted_token = vocab[np.argmax(predicted_vector)]\n",
    "    output_sentence.append(predicted_token)\n",
    "    i = 0\n",
    "    while predicted_token != END and i < max_len:\n",
    "        input_sentence = input_sentence[1:w] + [predicted_token]\n",
    "        input_vector = sent2mat(input_sentence, glove)\n",
    "        predicted_vector = model.predict([[context_vector], [input_vector]])\n",
    "        predicted_token = vocab[np.argmax(predicted_vector)]\n",
    "        output_sentence.append(predicted_token)\n",
    "        i += 1\n",
    "    return output_sentence\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = load_model('grimm-model')\n",
    "    (_, max_len, dim), (_, w, _) = model.get_input_shape_at(0)\n",
    "    hparams = (max_len, dim, w)\n",
    "    first_words = pd.read_json('grimm-first-words.json', typ='series')\n",
    "    with open('glove-dict.pkl', 'rb') as fp:\n",
    "        glove = pkl.load(fp)\n",
    "    with open('vocab.pkl', 'rb') as fp:\n",
    "        vocab = pkl.load(fp)\n",
    "    \n",
    "    print(\"Let's write a story!\")\n",
    "    title = input('Give me a title ({}) '.format(QUIT_PROMPT))\n",
    "    story = [title]\n",
    "    context_sentence = title\n",
    "    input_sentence = np.random.choice(first_words.index, p=first_words)\n",
    "    if title.lower() in QUIT_CMDS:\n",
    "        exit()\n",
    "    \n",
    "    print(CMD_PROMPT)\n",
    "    while True:\n",
    "        input_sentence = np.random.choice(first_words.index, p=first_words)\n",
    "        generated = generate_sentence(context_sentence, input_sentence, vocab, hparams=hparams)\n",
    "        generated = ' '.join(generated)\n",
    "        ### the model creates a suggested sentence\n",
    "        print('Suggestion:', generated)\n",
    "        ### the user responds with the sentence they want add\n",
    "        ### the user can fix up the suggested sentence or write their own\n",
    "        ### this is the sentence that will be used to make the next suggestion\n",
    "        sentence = input('Sentence: ')\n",
    "        if sentence.lower() in QUIT_CMDS:\n",
    "            story = []\n",
    "            break\n",
    "        elif sentence.lower() in FINISH_CMDS:\n",
    "            story.append(np.random.choice(ENDING))\n",
    "            break\n",
    "        elif sentence.lower() in BACK_CMDS:\n",
    "            if len(story) == 1:\n",
    "                print('You are at the beginning')\n",
    "            story = story[:-1]\n",
    "            context_sentence = story[-1]\n",
    "            continue\n",
    "        else:\n",
    "            story.append(sentence)\n",
    "            context_sentence = sentence\n",
    "            \n",
    "    print('\\n'.join(story))\n",
    "    print('exiting...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X5Lr3H-8mAYB"
   },
   "source": [
    "Let's give our script a run. I'll use it to read the suggestion and take elements of it to add the next line. A more complex model might be able to produce sentences that can be edited and added, but this model isn't quite there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uU2PzV0jwkUP"
   },
   "outputs": [],
   "source": [
    "# Feel free to uncomment and try it yourself. Below is my example usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0-SpcDomAS4"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "Let's write a story!\n",
    "Give me a title (\"quit\" or \"q\" to quit) The Wolf Goes Home\n",
    "finish,f | back,b | quit,q\n",
    "Suggestion: > > > > > > > > > and when they had walked for the time , and \n",
    "the king ' s son seated himself on the leg , and said , \" i will go to \n",
    "church , and you shall be have lost my life ??? ###\n",
    "Sentence: There was once a prince who got lost in the woods on the way \n",
    "to a church.\n",
    "Suggestion: > > > > > > > > > she was called hans , and as the king ' s \n",
    "daughter , who was so beautiful than the children , who was called clever \n",
    "elsie . ###\n",
    "Sentence: The prince was called Hans, and he was more handsome than the \n",
    "boys.\n",
    "Suggestion: > > > > > > > > > no one will do not know what to say , but i \n",
    "have been compelled to you ??? ###\n",
    "Sentence: The Wolf came along and asked, \"does no one know where are?\"\n",
    "Suggestion: > > > > > > > > > there was once a man who had a daughter who \n",
    "had three daughters , and he had a child and went , the king ' s daughter \n",
    ", and said , \" you are growing and thou now , i will go and fetch\n",
    "Sentence: The Wolf had three daughters, and he said to the prince, \"I \n",
    "will help you return home if you take one of my daughters as your \n",
    "betrothed.\"\n",
    "Suggestion: > > > > > > > > > but the king ' s daughter was humble , and \n",
    "said , \" you are not afraid ??? ###\n",
    "Sentence: The prince asked, \"are you not afraid that she will be killed \n",
    "as soon as we return home?\" \n",
    "Suggestion: > > > > > > > > > i will go and fetch the golden horse ??? \n",
    "###\n",
    "Sentence: The Wolf said, \"I will go and fetch a golden horse as dowry.\"\n",
    "Suggestion: > > > > > > > > > one day , the king ' s daughter , who was \n",
    "a witch , and lived in a great forest , and the clouds of earth , and in \n",
    "the evening , came to the glass mountain , and the king ' s son\n",
    "Sentence: The Wolf went to find the forest witch that she might conjure \n",
    "a golden horse.\n",
    "Suggestion: > > > > > > > > > when the king ' s daughter , however , was \n",
    "sitting on a chair , and sang and reproached , and said , \" you are not \n",
    "to be my wife , and i will take you to take care of your ??? ###\n",
    "Sentence: The witch reproached the wolf saying, \"you come and ask me such \n",
    "a favor with no gift yourself?\"\n",
    "Suggestion: > > > > > > > > > then the king said , \" i will go with you \n",
    "??? ###\n",
    "Sentence: So the wolf said, \"if you grant me this favor, I will be your \n",
    "servant.\"\n",
    "Suggestion: > > > > > > > > > he was now to go with a long time , and \n",
    "the other will be polluted , and we will leave you ??? ###\n",
    "Sentence: f\n",
    "The Wolf Goes Home\n",
    "There was once a prince who got lost in the woods on the way to a church.\n",
    "The prince was called Hans, and he was more handsome than the boys.\n",
    "The Wolf came along and asked, \"does no one know where are?\"\n",
    "The Wolf had three daughters, and he said to the prince, \"I will help \n",
    "you return home if you take one of my daughters as your betrothed.\"\n",
    "The prince asked, \"are you not afraid that she will be killed as soon as \n",
    "we return home?\" \n",
    "The Wolf said, \"I will go and fetch a golden horse as dowry.\"\n",
    "The Wolf went to find the forest witch that she might conjure a golden \n",
    "horse.\n",
    "The witch reproached the wolf saying, \"you come and ask me such a favor \n",
    "with no gift yourself?\"\n",
    "So the wolf said, \"if you grant me this favor, I will be your servant.\"\n",
    "THE END\n",
    "exiting..\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KFCNQF3tw7yQ"
   },
   "source": [
    "You can do additional epochs to get better suggestions, but beware of overfitting. If you overfit this model, then it will generate worse results if you provide it with contexts and inputs that it doesn't recognize.\n",
    "\n",
    "Now that we have a model that we can interact with, the next step would be to integrate it with a chatbot system. Most systems require some server that will serve the model. The specifics will depend on your chatbot platform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1UwMZ_Mzw-CM"
   },
   "source": [
    "## Test and Measure the Solution\n",
    " Measuring a chatbot depends more on the end purpose of the product than it does for most applications. Let's consider the different kinds of metrics we will use for measuring.\n",
    "\n",
    "### Business Metrics\n",
    " If you are building a chatbot to support customer service, then the business metrics will be centered around the customer experience. If you are building a chatbot for entertainment purposes, as is the case here, there are no obvious business metrics. However, if the entertaining chatbot is being used for marketing, you can use marketing metrics.\n",
    "\n",
    "### Model-Centric Metrics\n",
    " It's difficult to measure live interactions in the same way that the model measures in training. In training, we know the \"correct\" response, but due to the interactive nature of the model we don't have a definite correct answer. To measure a live model, you will need to manually label conversations.\n",
    "\n",
    "Now let's talk about the infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P7s7FdIyw-I2"
   },
   "source": [
    "## Review\n",
    " When reviewing a chatbot, you will need to do the normal reviews needed for any project. The additional requirement will be to get the chatbot in front of a proxy for the actual user. As with any application that requires user interaction, user testing is central.\n",
    "\n",
    "## Conclusion\n",
    "In this chapter, we learned how to build a model for an interactive application. There are many different kinds of chatbots. The example we see here is based on a language model, but we can also build a recommendation model. It all depends on what kind of interaction you are expecting. In our situation, we are entering and receiving full sentences. If your application has a constrained set of responses, then your task becomes easier. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNRw+UYfIyH3wMwnZoPNueg",
   "include_colab_link": true,
   "name": "3.15_Chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
