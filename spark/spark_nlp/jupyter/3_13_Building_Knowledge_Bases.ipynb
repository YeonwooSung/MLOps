{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "colab_type": "code",
    "id": "hDNVqNrSkVnV",
    "outputId": "ebfc22b7-0f1d-4eba-8dae-c91e8bd96492"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# from https://gist.github.com/korakot/328aaac51d78e589b4a176228e4bb06f\n",
    "if not os.path.exists('neo4j.tar.gz'):\n",
    "    # download 3.5.8 or neo4j-enterprise-4.0.0-alpha09mr02-unix\n",
    "    !curl https://neo4j.com/artifact.php?name=neo4j-community-3.5.8-unix.tar.gz -o neo4j.tar.gz\n",
    "    # decompress and rename\n",
    "    !tar -xf neo4j.tar.gz  # or --strip-components=1\n",
    "    !mv neo4j-community-3.5.8 nj\n",
    "    # disable password, and start server\n",
    "    !sed -i '/#dbms.security.auth_enabled/s/^#//g' nj/conf/neo4j.conf\n",
    "    !nj/bin/neo4j start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "q_L2yBEUi5lN",
    "outputId": "16929e98-6ea2-4991-985c-40c3352e3f4d"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('simplewiki-latest-pages-articles-multistream.xml.bz2'):\n",
    "    ! wget https://dumps.wikimedia.org/simplewiki/latest/simplewiki-latest-pages-articles-multistream.xml.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "up0NWV_sjPB5"
   },
   "source": [
    "# Building Knowledge Bases\n",
    "\n",
    "This application is about organizing information and making it easy to access by humans and computers alike.  This is known as a knowledge base. The popularity of knowledge bases in the field of NLP has waned in recent decades as the focus has moved away from \"expert systems\" to statistical machine learning approaches.\n",
    "\n",
    " An expert system is a system that attempts to use knowledge to make decisions. This knowledge is about entities, relationships between entities, and rules.  Generally, expert systems had inference engines that allowed the software to utilize the knowledge base to make a decision. These are sometimes described as collections of if-then rules. However, these systems were much more complicated than this. The knowledge bases and rule sets could be quite large for the technology of the time, so the inference engines needed to be able to efficiently evaluate many logical statements.\n",
    "\n",
    "Generally, an expert system has a number of actions it can take. There are rules for which action it should take. When the time to take an action comes, the system has a collection of statements and must use these to identify the best action. For example, let's say we have an expert system for controlling the temperature in a house. We need to be able to make decisions based on temperature and time. Whenever the system makes a decision to toggle the heater, or air conditioner, or to do nothing it must take the current temperature (or perhaps a collection of temperature measurements) and the current time, combined with the rule set to determine what action to take. This system has a small number of entities—the temperatures and the time. Imagine if a system had thousands of entities, with multiple kinds of relationships, and a growing ruleset. Resolving the statements available at decision time in a knowledge base this large can be expensive.\n",
    "\n",
    "In this chapter we will be building a knowledge base. We want a tool for building a knowledge base from a wiki and a tool for querying the knowledge base. This system should fit on a single machine now. We also want to be able to update the knowledge base with new kinds of entities and relationships. Such a system could be used by a domain expert in exploring a topic, or it could be integrated with an expert system. This means that it should have a human usable interface and a responsive API.\n",
    "\n",
    "Our fictional scenario is a company that is building a machine learning platform. This company primarily sells to other businesses. The sales engineers sometimes fall out of sync with the current state of the system. The engineers are good and update the wiki where appropriate, but the sales engineers are having a hard time keeping up to date. The sales engineers create help tickets for engineers to help them update sales demos. The engineers do not like this. So this application will be used to create a knowledge base that will make it easier for the sales engineers to check out what may have changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JiS4AuDmjPnA"
   },
   "source": [
    "## Problem Statement and Constraints\n",
    "\n",
    "1. What is the problem we are trying to solve?\n",
    "\n",
    " We want to take a wiki and produce a knowledge base. There should also be ways for humans and other software to query the knowledge base. We can assume that the knowledge base will fit on a single machine.\n",
    "\n",
    "2. What constraints are there?\n",
    "\n",
    "  * The knowledge-base builder should be easily updatable. It should be easy to configure new types of relationships.\n",
    "  * The storage solution should allow us to easily add new entities and relationships.\n",
    "  * Answering queries will require less than 50GB disk space and less than 16 GB memory.\n",
    "  * There should be a query for getting related entities. For example, at the end of a wiki article there are often links to related pages. The \"get related\" query should get these entities.\n",
    "  * The \"get related\" query should take less than 500ms.\n",
    "\n",
    "3. How do we solve the problem with the constraints?\n",
    "\n",
    "  * The knowledge-base builder can be a script that takes a wiki dump and processes the XML and the text. This is where we can use Spark NLP in a larger Spark pipeline.\n",
    "  * Our building script should monitor resources, to warn if we are nearing the prescribed limits.\n",
    "  * We will need a database to store our knowledge base. There are many options. We will use Neo4j, a graph database. Neo4j is also relatively well known. There are other solutions possible, but graph databases inherently structure data in the way that facilitates knowledge bases.\n",
    "  * Another benefit to Neo4j is that it comes with a GUI for humans to query and a REST API for programmatic queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35TP1RujjP1q"
   },
   "source": [
    "## Plan the Project\n",
    " \n",
    "Let's define the acceptance criteria. We want a script that does the following:\n",
    "\n",
    "* Takes a wiki dump, generally a compressed XML file\n",
    "* Extracts entities, such as article titles\n",
    "* Extracts relationships, such as links between articles\n",
    "* Stores the entities and relationships in Neo4J\n",
    "* Warns if we are producing too much data\n",
    "\n",
    "We want a service that does the following:\n",
    "\n",
    "* Allows a \"get related\" query for a given entity—results must be at least the articles linked in the entity's article\n",
    "* Performs a \"get related\" query in under 500ms\n",
    "* Has a human-usable frontend\n",
    "* Has a REST API\n",
    "* Requires less than 16GB memory to run\n",
    "\n",
    "This is somewhat similar to the application in sentiments. However, unlike in that chapter, the model is not a machine learning model—it is instead a data model. We have a script that will build a model, but now we also want a way to serve the model. An additional, and important, difference is that the knowledge base does not come with a simple score (e.g., an F1-score). This means that we will have to put more thought into metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vkHvigh-jQIL"
   },
   "source": [
    "## Design the Solution\n",
    "\n",
    "So we will need to start up Neo4J. Once you have it installed, you should be able to go to localhost:7474 for the UI.\n",
    "\n",
    "Since we are using an off-the-shelf solution, we will not be going very much into graph databases. Here are the important facts.\n",
    "\n",
    "Graph databases are built to store data as nodes and edges between nodes. The meaning of node in this case is usually some kind of entity, and the meaning of  an edge is some kind of relationship. There can be different types of nodes and different types of relationships. Outside of a database, graph data can be easily stored in CSVs. There will be CSVs for the nodes. This CSV will have an ID column, some sort of name, and properties—depending on the type. Edges are similar, except that the row for an edge will also have the IDs of the two nodes the edge connects. We will not be storing properties.\n",
    "\n",
    "Let's consider a simple scenario in which we want to store information about books. In this scenario we have three kinds of entities: authors, books, and genres. There are three kinds of relationships: an author writes a book, an author is a genre author, a book is in a genre. For Neo4j, this data could be stored in six CSVs. The entities are the nodes of the graph, and the relationships are the edges\n",
    "\n",
    "![Simple graph example](https://i.imgur.com/Ko4aOO6.png)  \n",
    "_Simple graph example_\n",
    "\n",
    "Since we don't have access to a company's internal wiki, we will be using an actual Wikipedia dump. But rather than getting the full English language dump, which would be enormous, we will use the Simple English wikidump.\n",
    "\n",
    "Simple English is a subset of the English language. It uses about 1,500 words, not counting proper nouns and some technical terms. This is useful for us because this will help us simplify the code we need to write. If this were a real company wiki, there would likely need to be a few iterations of data cleaning. Take a look at a dump of the [Simple English Wikipedia](https://dumps.wikimedia.org/simplewiki/).\n",
    "\n",
    "Here is our plan:\n",
    "\n",
    "1. Get the data\n",
    "2. Explore the data\n",
    "3. Parse the wiki for entities and relationships\n",
    "4. Save the entities and relationships in CSVs\n",
    "5. Load the CSVs into Neo4J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W2t0lKFnjQVY"
   },
   "source": [
    "## Implement the Solution\n",
    "\n",
    "First, let's load the data. Most wikidumps are available as bzip2 compressed XML files. Fortunately, Spark has the ability to deal with this kind of data. Let's load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avahdGJOl0By"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import sparknlp\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ye9FI7a0l1ZA"
   },
   "outputs": [],
   "source": [
    "packages = [\n",
    "    'JohnSnowLabs:spark-nlp:2.5.1',\n",
    "    'com.databricks:spark-xml_2.11:0.6.0'\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Knowledge Graph\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.jars.packages\", ','.join(packages)) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "no-P3K_yjQiN"
   },
   "source": [
    "To give Spark a hint for parsing the XML, we need to configure what the `rootTag` is—the name of the element that contains all of our \"rows.\" We also need to configure the rowTag—the name of the elements that represent our rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9g4DyQxNl5aa"
   },
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    "    .format('xml')\\\n",
    "    .option(\"rootTag\", \"mediawiki\")\\\n",
    "    .option(\"rowTag\", \"page\")\\\n",
    "    .load(\"simplewiki-latest-pages-articles-multistream.xml.bz2\")\\\n",
    "    .persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gqfgMlrvjRT_"
   },
   "source": [
    "Now, let's see what the schema looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "colab_type": "code",
    "id": "g55KDNX9l-kp",
    "outputId": "01874fe0-5a16-420c-cb0c-a3565634a0dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- ns: long (nullable = true)\n",
      " |-- redirect: struct (nullable = true)\n",
      " |    |-- _VALUE: string (nullable = true)\n",
      " |    |-- _title: string (nullable = true)\n",
      " |-- restrictions: string (nullable = true)\n",
      " |-- revision: struct (nullable = true)\n",
      " |    |-- comment: struct (nullable = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _deleted: string (nullable = true)\n",
      " |    |-- contributor: struct (nullable = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _deleted: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- ip: string (nullable = true)\n",
      " |    |    |-- username: string (nullable = true)\n",
      " |    |-- format: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- minor: string (nullable = true)\n",
      " |    |-- model: string (nullable = true)\n",
      " |    |-- parentid: long (nullable = true)\n",
      " |    |-- sha1: string (nullable = true)\n",
      " |    |-- text: struct (nullable = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _bytes: long (nullable = true)\n",
      " |    |    |-- _space: string (nullable = true)\n",
      " |    |-- timestamp: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Stn5G3PIjRfR"
   },
   "source": [
    "That is somewhat complicated, so we should try and simplify. Let's see how many documents we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_VEhGBGimBUB",
    "outputId": "f5ea6f99-b1fb-412b-d0db-ab08cc067bb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315316"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L8_iaXgkjRs5"
   },
   "source": [
    "Let's look at the page for \"Paper,\" so we can get a handle on how to simplify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yUrd9o0FmDgA",
    "outputId": "3744aa88-6ef3-4aea-a489-77fe95ac90f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 3319\n",
      "Title Paper\n",
      "\n",
      "redirect None\n",
      "\n",
      "text\n",
      "[[File:Cranes made by Origami paper.jpg|thumb|132x132px|Paper creations]]\n",
      "[[File:InternationalPaper6413.jpg|right|frame|[[International Paper]] is the world’s largest pulp and paper maker]]\n",
      "[[File:Gutenberg bible Old Testament Epistle of St Jerome.jpg|thumb|right|250px|Before about 1820, most printed books used [[wikt:rag|rag]] paper, usually made from waste cotton rags]]\n",
      "[[File:Manuscripts in the Yunnan Nationalities Museum - DSC03947.JPG|thumb|right|250px|Dai scripture on [[mulberry]]-bark paper, Yunnan, China]]\n",
      "[[File:Papyrus.jpg|thumb|right|250px|Writing on [[papyrus]]]]\n",
      "[[File:Ruins of the guard tower.jpg|thumb|right|220px|Ruins of a watchtower on the [[Great Wall of China]]: see \"first paper\"]]\n",
      "\n",
      "Modern '''paper''' is a thin [[material]] of (mostly) [[wood fibre]]s pressed together. People write on paper with a [[pencil]] or [[pen]], and [[book]]s are made of paper. Paper can absorb [[liquid]]s such as [[water]], so people can clean things with paper. There are many types of paper.\n",
      "\n",
      "The '''pulp and paper industry''' comprises companies that use wood as raw material and produce [[Pulp (paper)|pulp]], paper, board and other cellulose-based products.\n",
      "\n",
      "== Paper making ==\n",
      "Modern paper is normally made from [[wood]] [[wikt:pulp|pulp]].<ref>{{cite book|page=3|title=The Quantum Universe: everything that can happen does happen|publisher=Allen Lane|date=2011|first1=Brian|first2=Jeff|last1=Cox|last2=Forshaw|isbn=978-1-846-14432-5}}</ref> Wood is ground up and mixed with [[water]] and other [[chemical]]s to make a thin liquid called \"paper pulp\". Paper pulp can be [[Sodium hypochlorite|bleached]] to make paper more white, and [[dye]]s can be added to make colored paper. This pulp is pressed into [[wikt:sheet|sheets]] of paper. Printing is often done on paper before the paper is cut into sheets. Newsprint paper (newspaper) comes in a huge roll, and goes through the printing process as one continuous sheet. It is cut by a machine-driven [[guillotine]] blade later. Folding comes last, then packing for distribution. \n",
      "\n",
      "Sometimes paper is made heavier and more glossy (shiny) by adding [[clay]], and by 'milling' it. Milling is done by [[wikt:squeeze|squeezing]] the paper through a series of rollers. Sometimes paper is made from used or waste paper: this is [[recycling]].\n",
      "\n",
      "Not all paper is made from wood. Other kinds of fiber can be used. People still make paper from [[cotton]], [[linen]] and [[hemp]] for special purposes.\n",
      "\n",
      "== History of paper ==\n",
      "[[Writing]] started long before the invention of paper. People wrote on many kinds of material. They wrote on cloth, on the stone walls and on wood. In [[Mesopotamia]] the [[Sumer]]ians wrote on [[clay]] tablets, many of which have survived today. In Europe, people wrote on [[vellum]].\n",
      "\n",
      "=== First paper ===\n",
      "Many [[century|centuries]] ago&nbsp;–&nbsp;as early as the [[3rd millennium BC]] (that's over 2000 BC)&nbsp;–&nbsp;people in [[Egypt]] made a kind of paper from the [[papyrus]] plant.<ref name = Skeat>[http://www-user.uni-bremen.de/~wie/Egerton/BellSkeat2.html H. Idris Bell and T.C. Skeat, 1935. \"Papyrus and its uses\"] ([[British Museum]] pamphlet).</ref> This is where the word 'paper' comes from. The people of [[Greece]] and [[Rome]] learned to do this too. The Romans wrote on [[parchment]] (made from animal skin), on [[wax]]ed tablets and on wood (see [[Vindolanda]]).\n",
      "\n",
      "In [[China]] 105 AD, the [[eunuch]] [[Cai Lun]] told his Emperor he had made paper. They had previously used [[bamboo]] and [[silk]].<ref>Han dynasty 206BC–220 AD.</ref><ref name=Carter>Carter, Thomas Francis 1925. ''The invention of printing in China and its spread westward''. Columbia N.Y.</ref> The material used in this ancient paper included [[cotton]] [[wikt:rag|rags]], [[hemp]], various plant fibres and old [[Fishing net|fish nets]]. The oldest existing paper with writing on it was found in the ruins of a watchtower in the [[Great Wall of China]]. It dates to about 150 AD.<ref name=Carter/><sup>p5</sup><ref>Stein, M. Aurel 1921. ''Serindia''. London 1921.</ref><ref>Papermaking. 2007. In: ''Encyclopedia Britannica'', from ''Encyclopedia Britannica Online''.</ref> Even earlier paper (but with no writing on it) has been claimed: \"The oldest surviving piece of paper in the world is made of hemp fibers, discovered in 1957 in a tomb near Xian, China, and dates from between the years 140 and 87 BC\".<ref>Temple, Robert 1986. ''The genius of China: 3,000 Years of science, discovery, and invention''. Simon and Schuster, New York.</ref> Paper-making was regarded by the Chinese as so valuable that they kept it secret as long as they could.\n",
      "\n",
      "=== Spread of paper ===\n",
      "People in [[Japan]] learned how to make paper with fibres of the [[mulberry]] tree, around 610 AD. This is called Japanese paper or ''[[Washi]]''. The Chinese invention spread to [[India]], and then to the [[Middle East]], and then to [[Italy]].\n",
      "\n",
      "An opportunity occurred after The Battle of Talas in 751. Then an [[Arab]] army captured soldiers of the Chinese. There were some paper makers among the captured soldiers. From them, paper-making spread throughout the [[Islamic world]].  In 757, a paper mill was built at [[Samarkand]]. People learned to use [[linen]] as paper raw material and to use starch made from [[flour]] as an additive.\n",
      "\n",
      "The [[Italy|Italians]] used [[hemp]] and [[linen]] rags. In 1276 the first Italian paper mill was built at [[Fabriano]] and, until the 14 century, Italy was a paper supplier in Europe. In 1282 the first [[watermark]] was introduced in [[Bologna]].\n",
      "\n",
      "=== Machine-made paper ===\n",
      "Paper was hard to make. It was cheaper than the old writing materials, but still expensive. A mechanical paper maker was conceived in [[France]] 1798, but invented in [[England]]. At least one [[paper mill]] was using them by 1812.<ref>Hunter, Dard 1978. ''Paper-making: the history and technique of an ancient craft''. Courier Dover</ref><ref>Munsell, Joe 2009 [reprint of 1870 4th ed]. ''A chronology of paper and paper-making''. Albany.</ref> Now the process was cheaper but the [[raw material]] was still expensive.\n",
      "\n",
      "In 1840 Friedrich Gottlob Keller Invented a machine that could make pulp for paper out of [[wood]] fibres (instead of the expensive rag paper). Paper became cheap enough for everyone to buy. Around the same time, other [[invention]]s were made, like the [[pencil]], the [[fountain pen]], and a [[printing press]] that used [[steam power]]. With this new [[information technology]], people wrote more letters, made more books and [[newspaper]]s, and kept more records of what they did.\n",
      "\n",
      "Today, some of the largest paper-producing countries are [[China]], [[USA]], [[Canada]], [[Finland]], [[Sweden]] and [[Russia]]. Paper is produced in large factories called ''paper mills''. They produce hundreds of thousands of tons of paper each year.\n",
      "\n",
      "== Uses of paper ==\n",
      "[[File:Making Paper.gif|thumb|right|200px|Making Chinese paper]]\n",
      "\n",
      "Paper is used for [[write|writing]] and [[wikt:print|printing]]. Books, [[magazine]]s and [[newspaper]]s are printed on paper.\n",
      "\n",
      "Paper is often used for [[money]]. Paper used for money is made in special ways. It does not use wood fiber. It is mostly [[cotton]] with additives to make it hard for people to print their own money. A piece of paper money is called a [[banknote]], a bill or a note.\n",
      "\n",
      "Paper can be used for cleaning. Special forms of paper are used, such as [[paper towel]]s, [[facial tissue]]s or [[toilet paper]].\n",
      "\n",
      "Pretty paper can be used as [[decoration]]. It can be pasted onto the walls of a room; this is called [[wallpaper]]. Paper can be used to wrap gifts. This is called [[wrapping paper]] or [[gift wrap]].\n",
      "\n",
      "Some kinds of paper are strong and can be used in [[box]]es and other [[container|packaging]] [[material]]. Sometimes several layers of paper are held together with [[glue]], to make [[cardboard]].\n",
      "\n",
      "==Related pages==\n",
      "* [[Paper size]]\n",
      "* [[Cardboard]]\n",
      "\n",
      "== References ==\n",
      "{{Reflist}}\n",
      "\n",
      "[[Category:Basic English 850 words]]\n",
      "[[Category:Paper| ]]\n",
      "[[Category:Writing tools]]\n"
     ]
    }
   ],
   "source": [
    "row = df.filter('title = \"Paper\"').first()\n",
    "\n",
    "print('ID', row['id'])\n",
    "print('Title', row['title'])\n",
    "print()\n",
    "print('redirect', row['redirect'])\n",
    "print()\n",
    "print('text')\n",
    "print(row['revision']['text']['_VALUE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ls7i1EDjR5v"
   },
   "source": [
    "It looks like the text is stored in `revision.text._VALUE`. There seem to be a few special entries, namely categories and redirects. In most wikis, pages are organized into different categories. Pages are often in multiple categories. These categories have their own pages that link back to the articles. Redirects are pointers from an alternate name for an article to the actual entry.\n",
    "\n",
    "Let's look at some categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "h02Sdtu3mKL7",
    "outputId": "5bfd898b-4d89-4c2a-a902-c6fd06690943"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------\n",
      " title | Category:Computer science \n",
      "-RECORD 1--------------------------\n",
      " title | Category:Sports           \n",
      "-RECORD 2--------------------------\n",
      " title | Category:Athletics        \n",
      "-RECORD 3--------------------------\n",
      " title | Category:Body parts       \n",
      "-RECORD 4--------------------------\n",
      " title | Category:Tools            \n",
      "-RECORD 5--------------------------\n",
      " title | Category:Movies           \n",
      "-RECORD 6--------------------------\n",
      " title | Category:Grammar          \n",
      "-RECORD 7--------------------------\n",
      " title | Category:Mathematics      \n",
      "-RECORD 8--------------------------\n",
      " title | Category:Alphabet         \n",
      "-RECORD 9--------------------------\n",
      " title | Category:Countries        \n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('title RLIKE \"Category.*\"').select('title')\\\n",
    "    .show(10, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ckafrGajSdg"
   },
   "source": [
    "Now let's look at a redirect. It looks like the redirect target, where the redirect points, is stored under redirect._title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "2uUKnm2rmOfh",
    "outputId": "696e8057-fd5b-4ce6-80b3-a0fed7faaf50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------\n",
      " _title | Catharism   \n",
      " title  | Albigensian \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('redirect IS NOT NULL')\\\n",
    "    .select('redirect._title', 'title')\\\n",
    "    .show(1, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_LrZx0IjS1F"
   },
   "source": [
    "This essentially gives us a synonymy relationship. So, our entities will be titles of articles. Our relationships will be redirects, and links will be in the related section of the page. First let's get our entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "s0oThVkRmSP4",
    "outputId": "1fc17744-c722-47bf-f78a-f1f98cec8c34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315316\n"
     ]
    }
   ],
   "source": [
    "entities = df.select('title').collect()\n",
    "entities = [r['title'] for r in entities]\n",
    "entities = set(entities)\n",
    "print(len(entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZvE9DVdajTBS"
   },
   "source": [
    "We may want to introduce a same-category relationship, so we extract the categories, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ExZnKMemUh4"
   },
   "outputs": [],
   "source": [
    "categories = [e for e in entities if e.startswith('Category:')]\n",
    "entities = [e for e in entities if not e.startswith('Category:')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Il01qNJbjS6W"
   },
   "source": [
    "Now, let's get the redirects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XmF_c8tbmW6w",
    "outputId": "bae99438-a721-4d1f-803c-7c63a24fa418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70178\n"
     ]
    }
   ],
   "source": [
    "redirects = df.filter('redirect IS NOT NULL')\\\n",
    "    .select('redirect._title', 'title').collect()\n",
    "redirects = [(r['_title'], r['title']) for r in redirects]\n",
    "print(len(redirects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OTcsFoeqjSvr"
   },
   "source": [
    "Now we can get the articles from revision.text._VALUE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OXEt222cmZup"
   },
   "outputs": [],
   "source": [
    "data = df.filter('redirect IS NULL').selectExpr(\n",
    "    'revision.text._VALUE AS text',\n",
    "    'title'\n",
    ").filter('text IS NOT NULL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uUCb8H5RjSXp"
   },
   "source": [
    "To get the related links, we need to know what section we are in. So let's split the texts into sections. We can then use the `RegexMatcher` annotator to identify links. Viewing the data, it looks like sections look like == Paper making == as we saw in the previous example. Let's define a regex for this, adding in the possibility for extra whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_f6OpKxWmeAC"
   },
   "outputs": [],
   "source": [
    "section_ptn = re.compile(r'^ *==[^=]+ *== *$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ifJyjr6ljRzx"
   },
   "source": [
    "Now, we will define a function that will take a partition of the data and generate new rows for the sections. We will need to keep track of the article title, the section, and the text of the section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eL_sP2a9mgax"
   },
   "outputs": [],
   "source": [
    "def sectionize(rows):\n",
    "    for row in rows:\n",
    "        title = row['title']\n",
    "        text = row['text']\n",
    "        lines = text.split('\\n')\n",
    "        buffer = []\n",
    "        section = 'START'\n",
    "        for line in lines:\n",
    "            if section_ptn.match(line):\n",
    "                yield (title, section, '\\n'.join(buffer))\n",
    "                section = line.strip('=').strip().upper()\n",
    "                buffer = []\n",
    "                continue\n",
    "            buffer.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qb_qjIZ1jRlu"
   },
   "source": [
    "Now we will call `mapPartitions` to create a new `RDD` and convert that to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bV5VY5yamlQL"
   },
   "outputs": [],
   "source": [
    "sections = data.rdd.mapPartitions(sectionize)\n",
    "sections = spark.createDataFrame(sections, \\\n",
    "    ['title', 'section', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-z1W6hZojRZC"
   },
   "source": [
    "Let's look at the most common sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "colab_type": "code",
    "id": "xUkeweS0mnn5",
    "outputId": "1cdd9e31-7fb2-405d-b709-5346c0c2ac7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(section='START', count=132022),\n",
       " Row(section='REFERENCES', count=36472),\n",
       " Row(section='RELATED PAGES', count=9021),\n",
       " Row(section='HISTORY', count=6758),\n",
       " Row(section='CLUB CAREER STATISTICS', count=3910),\n",
       " Row(section='INTERNATIONAL CAREER STATISTICS', count=2491),\n",
       " Row(section='GEOGRAPHY', count=2489),\n",
       " Row(section='EARLY LIFE', count=2072),\n",
       " Row(section='CAREER', count=1996),\n",
       " Row(section='NOTES', count=1938)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections.select('section').groupBy('section')\\\n",
    "    .count().orderBy(col('count').desc()).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4aXeX7_XjRMD"
   },
   "source": [
    "Plainly, `START` is the most common because it captures the text between the start of the article and the first section, so almost all articles will have this. This is from Wikipedia, so `REFERENCES` is the next most common. It looks like `RELATED PAGES` occurs on only 8,603 articles. Now, we will use Spark-NLP to extract all the links from the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dqd-WauVmu6I",
    "outputId": "f88278dd-819a-4191-ce44-f978397b4ebe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wiki_regexes.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile wiki_regexes.csv\n",
    "\\[\\[[^\\]]+\\]\\]~link\n",
    "\\{\\{[^\\}]+\\}\\}~anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVlzskoVmwNC"
   },
   "outputs": [],
   "source": [
    "assembler = DocumentAssembler()\\\n",
    "    .setInputCol('text')\\\n",
    "    .setOutputCol('document')\n",
    "matcher = RegexMatcher()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('matches')\\\n",
    "    .setStrategy(\"MATCH_ALL\")\\\n",
    "    .setExternalRules('wiki_regexes.csv', '~')\n",
    "finisher = Finisher()\\\n",
    "    .setInputCols(['matches'])\\\n",
    "    .setOutputCols(['links'])\n",
    "\n",
    "pipeline = Pipeline()\\\n",
    "    .setStages([assembler, matcher, finisher])\\\n",
    "    .fit(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kVTHYxakmxeU"
   },
   "outputs": [],
   "source": [
    "extracted = pipeline.transform(sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8BL6yeq9jQbY"
   },
   "source": [
    "Now, we could define a relationship based on just links occurring anywhere. For now, we will stick to the related links only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tPvPW5MKmzsS",
    "outputId": "0e9165ad-89c8-4a66-8f62-cb57b4862582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4430953\n"
     ]
    }
   ],
   "source": [
    "links = extracted.select('title', 'section','links').collect()\n",
    "links = [(r['title'], r['section'], link) for r in links for link in r['links']]\n",
    "links = list(set(links))\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jSqnuDkUm1XI",
    "outputId": "09cf97f2-a91f-43a5-cf67-253feec20cd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21816\n"
     ]
    }
   ],
   "source": [
    "related = [(l[0], l[2]) for l in links if l[1] == 'RELATED PAGES']\n",
    "related = [(e1, e2.strip('[').strip(']').split('|')[-1]) for e1, e2 in related]\n",
    "related = list(set([(e1, e2) for e1, e2 in related]))\n",
    "print(len(related))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4n_YJvWUjQPB"
   },
   "source": [
    "Now, we have extracted our entities, redirects, and related links. Let's create CSVs for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VbG3dPb6m3mJ"
   },
   "outputs": [],
   "source": [
    "entities_df = pd.Series(entities, name='entity').to_frame()\n",
    "entities_df.index.name = 'id'\n",
    "entities_df.to_csv('wiki-entities.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Clp7o_4im5Dv"
   },
   "outputs": [],
   "source": [
    "e2id = entities_df.reset_index().set_index('entity')['id'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Av0AHkvQm6SA"
   },
   "outputs": [],
   "source": [
    "redirect_df = []\n",
    "for e1, e2 in redirects:\n",
    "    if e1 in e2id and e2 in e2id:\n",
    "        redirect_df.append((e2id[e1], e2id[e2]))\n",
    "redirect_df = pd.DataFrame(redirect_df, columns=['id1', 'id2'])\n",
    "redirect_df.to_csv('wiki-redirects.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nM43xPNvm7on"
   },
   "outputs": [],
   "source": [
    "related_df = []\n",
    "for e1, e2 in related:\n",
    "    if e1 in e2id and e2 in e2id:\n",
    "        related_df.append((e2id[e1], e2id[e2]))\n",
    "related_df = pd.DataFrame(related_df, columns=['id1', 'id2'])\n",
    "related_df.to_csv('wiki-related.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GzC5F6IJjQAY"
   },
   "source": [
    "Now that we have our CSVs, we can copy them to ./nj/import/ and import them using the following:\n",
    "\n",
    "**Note**: these take a long time (multiple hours)\n",
    "\n",
    "* Load entities\n",
    "\n",
    "```\n",
    "LOAD CSV WITH HEADERS FROM \"file:/wiki-entities.csv\" AS csvLine\n",
    "CREATE (e:Entity {id: toInteger(csvLine.id), entity: csvLine.entity})\n",
    "```\n",
    "\n",
    "* Load \"REDIRECTED\" relationship\n",
    "\n",
    "```\n",
    "USING PERIODIC COMMIT 500\n",
    "LOAD CSV WITH HEADERS FROM \"file:///wiki-redirects.csv\" AS csvLine\n",
    "MATCH (entity1:Entity {id: toInteger(csvLine.id1)}),(entity2:Entity {id: toInteger(csvLine.id2)})\n",
    "CREATE (entity1)-[:REDIRECTED {conxn: \"redirected\"}]->(entity2)\n",
    "```\n",
    "\n",
    "* Load \"RELATED\" relationship\n",
    "\n",
    "```\n",
    "USING PERIODIC COMMIT 500\n",
    "LOAD CSV WITH HEADERS FROM \"file:///wiki-related.csv\" AS csvLine\n",
    "MATCH (entity1:Entity {id: toInteger(csvLine.id1)}),(entity2:Entity {id: toInteger(csvLine.id2)})\n",
    "CREATE (entity1)-[:RELATED {conxn: \"related\"}]->(entity2)\n",
    "```\n",
    "\n",
    "Let's go see what we can query. We will get all entities related to \"Language\", and related to entities that are related to \"Language\" (i.e., second-order relations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WTY7-QzanMpu"
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iRNhCKLznNu_"
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "MATCH (e:Entity {entity: 'Language'})\n",
    "RETURN e\n",
    "UNION ALL\n",
    "MATCH (:Entity {entity: 'Language'})--(e:Entity)\n",
    "RETURN e\n",
    "UNION ALL\n",
    "MATCH (:Entity {entity: 'Language'})--(e1:Entity)--(e:Entity)\n",
    "RETURN e\n",
    "'''\n",
    "payload = {'query': query, 'params': {}}\n",
    "endpoint = 'http://localhost:7474/db/data/cypher'\n",
    "\n",
    "response = requests.post(endpoint, json=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6DO6MrZTnPFI",
    "outputId": "8117d0f0-e53b-4539-df40-535c399110a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gVwViPp0nQFP",
    "outputId": "2514d7f9-a6ba-464a-916d-f13d93ac7a55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alphabet',\n",
       " 'Alphabet (computer science)',\n",
       " 'Alphabet (computer science)',\n",
       " 'American English',\n",
       " 'An audio-visual way into English through pictures',\n",
       " 'Angla lingvo',\n",
       " 'Anglish language',\n",
       " 'Australian English',\n",
       " 'BE',\n",
       " 'BE 1,500',\n",
       " 'BE 805',\n",
       " 'BE-850',\n",
       " 'Basic English',\n",
       " 'Basic English international wordlist',\n",
       " 'Basic English picture wordlist',\n",
       " 'Basic Roman',\n",
       " 'Basic Roman',\n",
       " 'British English',\n",
       " 'Canadian English',\n",
       " 'Canadian English',\n",
       " 'Canadian French',\n",
       " 'Classical English language',\n",
       " 'Computer language',\n",
       " 'Dialect',\n",
       " 'English (language)',\n",
       " 'English Language',\n",
       " 'English as a foreign language',\n",
       " 'English as a second language',\n",
       " 'English langauge',\n",
       " 'English language',\n",
       " 'English language',\n",
       " 'English language.',\n",
       " 'English languages',\n",
       " 'English languge',\n",
       " 'English the Global Language',\n",
       " 'English vocabulary',\n",
       " 'English-language',\n",
       " 'English-speaking',\n",
       " 'English-speaking countries',\n",
       " 'Enska',\n",
       " 'Formal language',\n",
       " 'Formal language',\n",
       " 'Full English',\n",
       " 'Ghoti',\n",
       " 'Grammer',\n",
       " 'Historical linguistics',\n",
       " 'Idioma inglés',\n",
       " 'Indian English',\n",
       " 'Inglés',\n",
       " 'International auxiliary language',\n",
       " 'Jamaican English',\n",
       " 'Language',\n",
       " 'Language families and languages',\n",
       " 'Language family',\n",
       " 'Language science',\n",
       " 'Languages',\n",
       " 'Linguist',\n",
       " 'Linguistics',\n",
       " 'Linguists',\n",
       " 'List of endangered languages',\n",
       " 'List of languages',\n",
       " 'Local language',\n",
       " 'Multilingualism',\n",
       " 'Natural language',\n",
       " 'New Zealand English',\n",
       " 'Newman language',\n",
       " 'Nostratic',\n",
       " 'Orthography',\n",
       " 'Pakistani English',\n",
       " 'Phonology',\n",
       " 'Plain English Campaign',\n",
       " 'Programming language',\n",
       " 'Scottish English',\n",
       " 'Second language',\n",
       " 'Semantic',\n",
       " 'Semantics',\n",
       " 'Semantics',\n",
       " 'Simplified English',\n",
       " 'Spelling',\n",
       " 'Spoken English',\n",
       " 'Standard English',\n",
       " 'Syntax',\n",
       " 'Testing English as a foreign language',\n",
       " 'The English Langauge',\n",
       " 'The English Language',\n",
       " 'The English language',\n",
       " 'Vowel',\n",
       " 'Wikipedia:How to write Simple English pages',\n",
       " 'Writing',\n",
       " '英語']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related = json.loads(response.content)\n",
    "related = [entity[0]['data']['entity'] \n",
    "           for entity in related['data']]\n",
    "related = sorted(related)\n",
    "related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping Neo4j.. stopped\n"
     ]
    }
   ],
   "source": [
    "# !nj/bin/neo4j stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUT4ZUm1jPti"
   },
   "source": [
    "We have processed a wikidump and have created a basic graph in Neo4j. The next steps in this project would be to extract some more node types and relationships. It would also be good to find a way to attach a weight to the edges. This would allow us to return better results from our query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ebbS1X10jPdV"
   },
   "source": [
    "## Test and Measure the Solution\n",
    "\n",
    " We now have an initial implementation, so let's go through metrics.\n",
    "\n",
    " ### Business Metrics\n",
    "\n",
    " This will depend on the specific use case of this application. If this knowledge base is used for organizing a company's internal information, then we can look at usage rates. This is not a great metric, since it does not tell us that the system is actually helping the business—only that it is getting used. Let's consider a hypothetical scenario.\n",
    "\n",
    "Using our example, the sales engineer can query for a feature they want to demo and get related features. Hopefully, this will decrease the help tickets. This is a business-level metric we can monitor.\n",
    "\n",
    "If we implement this system and do not see sufficient change in the business metrics, we still need metrics to help us understand if the problem is with the basic idea of the application or if it is with the quality of the knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jn3SMLv3jOz-"
   },
   "source": [
    "### Model-Centric Metrics\n",
    " Measuring the quality of a collection is not as straightforward as measuring a classifier. Let's consider what intuitions we have about what should be in the knowledge base and turn these intuitions into metrics.\n",
    "\n",
    "* Sparsity versus density: if too many entities have no relationship to any other entity, they decrease the usefulness of the knowledge base; similarly, relationships that are ubiquitous cost resources and provide little benefit. Following are some simple metrics that can be used to measure connectivity.\n",
    "  * Average number of relationships per entity\n",
    "  * Proportion of entities with no relationships\n",
    "  * Ratio of occurrences of a relationship to a fully connected graph\n",
    "* The entities and relationships that people query are ones we must focus on. Similarly, relationships that are almost never used may be superfluous. Once the system is deployed and queries are logged, we can monitor the following to learn about usage.\n",
    "  * Number of queries where an entity was not found\n",
    "  * Number of relationships that are not queried in a time period (day, week, month)\n",
    "\n",
    "The benefit of having an intermediate step of outputting CSVs is that we don't need to do a large extraction from the database—we can calculate these graph metrics using the CSV data.\n",
    "\n",
    "Now that we have some idea of how to measure the quality of the knowledge base, let's talk about measuring the infrastructure.\n",
    "\n",
    "### Infrastructure Metrics\n",
    "\n",
    " We will want to make sure that our single-server approach is sufficient. For a company that is small- to medium-sized, this should be fine. If the company were large, or if the application were intended for much broader use, we would want to consider replication. That is, we would have multiple servers with database, and the users would be redirected through a load balancer.\n",
    "\n",
    "With Neo4j you can look at system info by querying `:sysinfo`. This will give you information about the amount of data being used.\n",
    "\n",
    "For an application like this, you would want to monitor response time when queried and update time when adding new entities or relationships.\n",
    "\n",
    "### Process Metrics\n",
    "\n",
    "On top of the generic process metrics, for this project you want to monitor how long it takes for someone to be able to update the graph. There are a few ways that this graph is likely to be updated.\n",
    "\n",
    "* Periodic updates to capture wiki updates\n",
    "* Adding a new type of relationship\n",
    "* Adding properties to entities or relationships\n",
    "\n",
    "The first of these is the most important to monitor. The whole point of this application is to keep sales engineers up to date, so this data has to keep up to date. Ideally, this process should be monitored. The next two are important to monitor because the hope of this project is to decrease the workload on developers and data scientists. We don't want to replace the work needed to support sales efforts with effort maintaining this application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1bTpmCitq55x"
   },
   "source": [
    "## Review\n",
    "\n",
    "Many of the review steps from sentiments will apply to this application too. You will still want to do the architecture review and the code review. The model review will look different in this situation. Instead of reviewing a machine learning model, you will be reviewing the data model. In building a knowledge graph, you need to balance the needs of performance while structuring the data in a way that makes sense for the domain. This is not a new problem; in fact, traditional relational databases have many ways of balancing these needs.\n",
    "\n",
    "There are some common structural problems that you can watch out for. First, there is a node type that has only one or two properties; you may want to consider making it a property of the nodes it connects to. For example, we could define a name-type node and have it connect to entities, but this would needlessly complicate the graph.\n",
    "\n",
    "Deployment will be easier with this kind of application, unless it is customer facing. Your backup plan should be more concerned with communicating with users than with substituting a \"simpler\" version.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this chapter, we explored creating an application that is not machine learning based. One of the most valuable things we can do with NLP is make it easier for people to access the information inside. This can, of course, be done by building models, but it can also be done by organizing the information. In search_engine, we will look into building an application that uses search to help people organize and access information in text. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPD6y9+0ZIclv+/AWm1EI0g",
   "include_colab_link": true,
   "name": "3.13_Building_Knowledge_Bases.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
