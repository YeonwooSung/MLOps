{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HzgTx-7OuEnm"
   },
   "outputs": [],
   "source": [
    "! mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "dgBCSO9Fr-M5",
    "outputId": "6ecefdc7-4cb1-4233-e898-0f397ff35813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-20 21:23:42--  https://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/mini_newsgroups.tar.gz\n",
      "archive.ics.uci.edu (archive.ics.uci.edu) 해석 중... 128.195.10.252\n",
      "다음으로 연결 중: archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... 연결했습니다.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 200 OK\n",
      "길이: 지정하지 않음\n",
      "저장 위치: `mini_newsgroups.tar.gz'\n",
      "\n",
      "mini_newsgroups.tar     [    <=>             ]   1.77M  1.74MB/s    /  1.0s    \n",
      "\n",
      "2023-08-20 21:23:44 (1.74 MB/s) - `mini_newsgroups.tar.gz' 저장함 [1860687]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/mini_newsgroups.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B7DAiInZuIsG"
   },
   "outputs": [],
   "source": [
    "! tar xzf mini_newsgroups.tar.gz -C ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "nAMhZwEW4M3A",
    "outputId": "1cd10ec4-ec0b-49df-cb8e-a278428f3fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-20 21:23:45--  https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
      "archive.ics.uci.edu (archive.ics.uci.edu) 해석 중... 128.195.10.252\n",
      "다음으로 연결 중: archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 200 OK\n",
      "길이: 지정하지 않음\n",
      "저장 위치: `iris.data'\n",
      "\n",
      "iris.data               [ <=>                ]   4.44K  --.-KB/s    /  0s      \n",
      "\n",
      "2023-08-20 21:23:45 (51.7 MB/s) - `iris.data' 저장함 [4551]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8N8Uqqj4MnM"
   },
   "outputs": [],
   "source": [
    "! mv iris.data ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-3.4.1-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.7\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spark-nlp\n",
      "  Downloading spark_nlp-5.0.2-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.1/502.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-5.0.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spark-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YofQ9lTttiGS"
   },
   "source": [
    "## NLP on Apache Spark\n",
    "It's no longer news that there is a data deluge. Every day, people and devices are creating huge amounts of data. Text data is definitely one of the main kinds of data that humans produce. People write millions of comments, product reviews, Reddit messages, and tweets per day. This data is incredibly valuable—for both research and commerce. Because of the scale at which this data is created, our approach to working with it has changed.\n",
    "\n",
    "Most of the original research in NLP was done on small data sets with hundreds or thousands of documents. You may think that it would be easier to build NLP applications now that we have so much more text data with which to build better models. However, these pieces of text have different pragmatics and are of different varieties, so leveraging them is more complicated from a data-science perspective. From the software engineering perspective, big data introduces many challenges. Structured data has predictable size and organization, which makes it easier to store and distribute efficiently. Text data is much less consistent. This makes parallelizing and distributing work more important and potentially more complex. Distributed computing frameworks like Spark help us manage these challenges and complexities.\n",
    "\n",
    "In this chapter, we will discuss the Apache Spark and Spark NLP. First, we will cover some basic concepts that will help us understand distributed computing. Then, we will talk briefly about the history of distributed computing. We will talk about some important modules in Spark—Spark SQL and MLlib. This will give us the background and context needed to talk about Spark NLP in technical detail.\n",
    "\n",
    "Now, we'll cover some technical concepts that will be helpful in understanding how Spark works. The explanations will be high level. If you are interested in maximizing performance, I suggest looking more into these topics. For the general audience, I hope this material will give you the intuition necessary to help make decisions when designing and building Spark-based applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VYAKO09Ztj1N"
   },
   "source": [
    "## Parallelism, Concurrency, Distributing Computation\n",
    "Let's start by defining some terms.  A process can be thought of as a running program.  A process executes its code using an allotted portion of memory also known as a memory space. A thread is a sequence of execution steps within a process that the operating system can schedule. Sharing data between processes generally requires copying data between the different memory spaces.  When a Java or Scala program is run, the Java Virtual Machine (JVM) is the process. The threads of a process share access to the same memory space, which they access concurrently.\n",
    "\n",
    " Concurrent access of data can be tricky. For example, let's say we want to generate word counts. If two threads are working on this process, it's possible for us to get the wrong count. Consider the following program (written in pseudo-Python). In this program, we will use a thread pool.  A thread pool is a way to separate partitioning work from scheduling. We allocate a certain number of threads, and then we go through our data asking for threads for the pool. The operating system can then schedule the work.\n",
    "\n",
    "```\n",
    "0:  def word_count(tokenized_documents): # list of lists of tokens\n",
    "1:      word_counts = {}\n",
    "2:      thread_pool = ThreadPool()\n",
    "3:      i = 0\n",
    "4:      for thread in thread_pool\n",
    "5:          run thread:\n",
    "6:              while i < len(tokenized_documents):\n",
    "7:                  doc = tokenized_documents[i]\n",
    "8:                  i += 1\n",
    "9:                  for token in doc:\n",
    "10:                     old_count = word_counts.get(token, 0)\n",
    "11:                     word_counts[token] = old_count + 1\n",
    "12:     return word_counts\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ox34D_1Sto1f"
   },
   "source": [
    "This looks reasonable, but we see that the code under `run thread` references data in the shared memory space like `i` and `word_counts`. tableone shows the execution of this program with two `thread`s in the `ThreadPool` starting at line 6.\n",
    "\n",
    "_Two `threads` in the `ThreadPool`_\n",
    "\n",
    "time|thread1|thread2|i|valid_state\n",
    "----|-------|-------|-|-----------\n",
    "0|while i < len(tokenized_documents)||0|yes\n",
    "1||while i < len(tokenized_documents)|0|yes\n",
    "2|doc = tokenized_documents[i]||0|yes\n",
    "3||doc = tokenized_documents[i]|0|NO\n",
    "\n",
    "At time 3, `thread2` will be retrieving `tokenized_documents[0]`, while `thread1` is already set to work on the first document.  This program has a race condition, in which we can get incorrect results depending on the sequence of operations done in the different threads. Avoiding these problems generally involves writing code that is safe for concurrent access. For example, we can pause `thread2` until `thread1` is finished updating by  locking on `tokenized_documents`. If you look at the code, there is another race condition, on `i`. If thread1 takes the last document, `tokenized_documents[N-1]`, `thread2` starts its while-loop check, `thread1` updates `i`, then `thread2` uses `i`. We will be accessing `tokenized_documents[N]`, which doesn't exist. So let's lock on `i`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVdEHp-ltpCk"
   },
   "source": [
    "```\n",
    "0:  def word_count(tokenized_documents): # list of lists of tokens\n",
    "1:      word_counts = {}\n",
    "2:      thread_pool = ThreadPool()\n",
    "3:      i = 0\n",
    "4:      for thread in thread_pool\n",
    "5:          run thread:\n",
    "6:              while True:\n",
    "7:                  lock i:\n",
    "8:                      if i < len(tokenized_documents)\n",
    "9:                          doc = tokenized_documents[i]\n",
    "10:                         i += 1\n",
    "11:                     else:\n",
    "12:                         break\n",
    "13:                 for token in doc:\n",
    "14:                     lock word_counts:\n",
    "15:                         old_count = word_counts.get(token, 0)\n",
    "16:                         word_counts[token] = old_count + 1\n",
    "17:     return word_counts\n",
    "```\n",
    "\n",
    "Now, we are locking on `i` and checking `i` in the loop. We also lock on `word_counts` so that if two threads want to update the counts of the same word, they won't accidentally pull a stale value for `old_count`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJU_0w8atp3R"
   },
   "source": [
    "time|thread1|thread2|i|valid_state\n",
    "----|-------|-------|-|-----------\n",
    "0|lock i||0|yes\n",
    "1|if i < len(tokenized_documents)|blocked|0|yes\n",
    "2|doc = tokenized_documents[i]|blocked|0|yes\n",
    "3|i += 1|blocked|0|yes\n",
    "4||lock i|1|yes\n",
    "5|lock word_counts||1|yes\n",
    "6||if i < len(tokenized_documents)|1|yes\n",
    "7|old_count = word_counts.get(token, 0)||1|yes\n",
    "8||doc = tokenized_documents[i]|1|yes\n",
    "9|word_counts[token] = old_count + 1||1|yes\n",
    "10||i += 1|1|yes\n",
    "11|lock word_counts||2|yes\n",
    "12|old_count = word_counts.get(token, 0)|blocked|2|yes\n",
    "13|word_counts[token] = old_count + 1|blocked|2|yes\n",
    "14||lock word_counts|2|yes\n",
    "15|blocked\t|old_count = word_counts.get(token, 0)|2|yes\n",
    "16|blocked\t|word_counts[token] = old_count + 1|2|yes\n",
    "\n",
    "We fixed the problem, but at the cost of frequently blocking one of the threads. This means that we are getting less advantage of the parallelism. It would be better to design our algorithm so that the threads don't share state. We will see an example of this when we talk about MapReduce. \n",
    "\n",
    "Sometimes, parallelizing on one machine is not sufficient, so we distribute the work across many machines grouped together in a cluster. When all the work is done on a machine, we are bringing the data (in memory or on disk) to the code, but when distributing work we are bringing the code to the data. Distributing the work of a program across a cluster means we have new concerns. We don't have access to a shared memory space, so we need to be more thoughtful in how we design our algorithms. Although processes on different machines don't share a common memory space, we still need to consider concurrency because the threads of the process on a given machine of the cluster still share common (local) memory space. Fortunately, modern frameworks like Spark mostly take care of these concerns, but it's still good to keep this in mind when designing your programs.\n",
    "\n",
    "Programs that work on text data often find some form of parallelization helpful because processing the text into structured data is often the most time-consuming stage of a program. Most NLP pipelines ultimately output structured numeric data, which means that the data that's loaded, the text, can often be much larger than the data that is output. Unfortunately, because of the complexity of NLP algorithms, text processing in distributed frameworks is generally limited to basic techniques. Fortunately, we have Spark NLP, which we will discuss shortly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4zracfS7tqtI"
   },
   "source": [
    "### Parallelization Before Apache Hadoop\n",
    "HTCondor is a framework developed at the University of Wisconsin–Madison starting in 1988. It boasts an impressive catalog of uses. It was used by NASA, the Human Genome Project, and the Large Hadron Collider. Technically, it's not just a framework for distributing computation—it also can manage resources. In fact, it can be used with other frameworks for distributing computation. It was built with the idea that machines in the cluster may be owned by different users, so work can be scheduled based on available resources. This is from a time when clusters of computers were not as available.\n",
    "\n",
    "GNU parallel and pexec are UNIX tools that can be used to parallelize work on a single machine, as well as across machines. This requires that the distributable portions of work be run from the command line. These tools allow us to utilize resources across machines, but it doesn't help with parallelizing our algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9G1uNCa_tq6p"
   },
   "source": [
    "### MapReduce and Apache Hadoop\n",
    "We can represent distributed computation with two operations: map and reduce. The map operation can be used to transform, filter, or sort data. The reduce operation can be used to group or summarize data. Let's return to our word count example to see how we can use these two operations for a basic NLP task.\n",
    "\n",
    "```\n",
    "def map(docs):\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            yield (token, 1)\n",
    "\n",
    "def reduce(records):\n",
    "    word_counts = {}\n",
    "    for token, count in records:\n",
    "        word_counts[token] = word_counts.get(token, 0) + count\n",
    "    for word, count in word_counts.items():\n",
    "        yield (word, count)\n",
    "```\n",
    "\n",
    "The data is loaded in partitions, with some documents going to each `mapper` process on the cluster. There can be multiple `mapper`s per machine. Each `mapper` runs the map function on their documents and saves the results to the disk. After all of the `mapper`s have completed, the data from the `mapper` stage is shuffled so that, all the records with the same key (`word` in this case), are in the same partition. This data is now sorted so that within a partition, all the records are ordered by key. Finally, the sorted data is loaded and the `reduce` step is called for each partition `reducer` process combining all of the counts. In between stages, the data is saved to the disk.\n",
    "\n",
    "MapReduce can express most distributed algorithms, but some are difficult or downright awkward in this framework. This is why abstractions to MapReduce were developed rather quickly.\n",
    "\n",
    "Apache Hadoop is the popular open source implementation of MapReduce along with a distributed file system,  Hadoop Distributed File System (HDFS). To write a Hadoop program, you need to select or define an input format, mapper, reducer, and output format. There have been many libraries and frameworks to allow higher levels of implementing a program.\n",
    "\n",
    " Apache Pig is a framework for expressing MapReduce programs in procedural code. Its procedural nature makes implementing extract, transform, load (ETL) programs very convenient and straightforward. However, other types of programs, model training programs for example, are much more difficult.  The language that Apache Pig uses is called Pig Latin. There is some overlap with SQL, so if someone knows SQL well, learning Pig Latin is easy.\n",
    "\n",
    " Apache Hive is a data warehousing framework originally built on Hadoop. Hive allows users to write SQL that is executed with MapReduce. Now, Hive can run using other distributed frameworks in addition to Hadoop, including Spark.\n",
    "\n",
    "### Apache Spark\n",
    "Spark is a project started by Matei Zaharia. Spark is a distributed computing framework. There are important differences in how Spark and Hadoop process data. Spark allows users to write arbitrary code against distributed data. Currently, there are official APIs for Spark in Scala, Java, Python, and R. Spark does not save intermediate data to disk. Usually, Spark-based programs will keep data in memory, though this can be changed by configuration to also utilize the disk. This allows for quicker processing but can require more scale out (more machines), or more scale up (machines with more memory).\n",
    "\n",
    "Let's look at `word_count` in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2VtYbneyysy"
   },
   "source": [
    "## Architecture of Apache Spark\n",
    "Spark is organized around a driver that is running the program, a master that manages resources and distributes work, and workers that execute computation. There are a number of possible masters. Spark ships with its own master, which is what is used in standalone and local modes. You can also use Apache YARN or Apache Mesos. In my experience, Apache YARN is the most common choice for enterprise systems.\n",
    "\n",
    "Let's take a more detailed look at Spark's architecture.\n",
    "\n",
    "### Physical Architecture\n",
    "We start our program on the submitting machine which submits an application. This driver runs the application on the client machine and sends jobs to the  spark master to be distributed to the workers. The spark master may not be a completely separate machine. That machine may also be doing work on the cluster and so would be a worker as well. Also, you may be running your program on the spark master, and so it could also be the client machine.\n",
    "\n",
    "There are two modes that you can use to start a Spark application: cluster mode and client mode. If the machine submitting the application is the same machine that runs the application, you are in client mode, because you are submitting from the client machine. Otherwise, you are in cluster mode. Generally, you use client mode if your machine is inside the cluster and cluster mode if it is not (see Figures #client and #cluster).\n",
    "\n",
    "![Physical architecture (client mode)](https://i.imgur.com/ZiuOmLo.png)  \n",
    "_Physical architecture (client mode)_\n",
    "\n",
    "![Physical architecture (cluster mode)](https://i.imgur.com/lMltdz3.png)  \n",
    "_Physical architecture (cluster mode)_\n",
    "\n",
    "You can also run Spark in local mode in which, as the name implies, client machine, spark master, and worker are all the same machine. This is very useful for developing and testing Spark applications. It is also useful if you want to parallelize work on one machine.\n",
    "\n",
    "Now that we have an idea of the physical architecture, let's look at the logical architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BG0sLPj3yzI_"
   },
   "source": [
    "### Logical Architecture\n",
    "\n",
    "In looking at the logical architecture of Spark, we will treat the client machine, spark master, and worker as if they are different (see architecture). The driver is a JVM process that will submit work to the spark master. If the program is a Java or Scala program, then it is also the process running the program. If the program is in Python or R, then the driver process is a separate process from that running the program.\n",
    "\n",
    "![Logical architecture](https://i.imgur.com/WaM6zvN.png)  \n",
    "_Logical architecture_\n",
    "\n",
    "The JVM processes on the workers are called executors. The work to be done is defined on the driver and submitted to the spark master, which orchestrates the executors to do the work. The next step in understanding Spark is understanding how the data is distributed.\n",
    "\n",
    "#### RDDs\n",
    "\n",
    "Spark distributes data in resilient distributed datasets (RDDs). RDDs allow users to work with distributed data almost as if it were a collection located in the driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4sjUgZQXyzNn"
   },
   "source": [
    "#### Partitioning\n",
    "\n",
    "In Spark, data is partitioned across the cluster. There are usually more partitions than executors. This allows for each thread on each executor to be utilized. Spark will distribute the data in the RDD evenly across the cluster into the default number of partitions. We can specify the number of partitions, and we can specify a field to partition by. This can be very useful when your algorithm requires some degree of locality—for example, having all the tweets from a single user.\n",
    "\n",
    "#### Serialization\n",
    "Any code that is shipped to the data should refer only to serializable objects. `NotSerializableException` errors are common and can be nearly inscrutable to those new to Spark.  When we map over an `RDD`, we are creating a `Function` and sending it to the machines with the data. A function is the code that defines it, and the data needed in the definition. This second part is called the closure of the function. Identifying what objects are needed for a function is a complicated task, and sometimes extraneous objects can be captured. If you are having problems with serializability, there are a couple of possible solutions. The following are questions that can help you find the right solution:\n",
    "\n",
    "* Are you using your own custom classes? Make sure that they are serializable.\n",
    "* Are you loading a resource? Perhaps your distributed code should load it lazily so that it is loaded on each executor, instead of being loaded on the driver and shipped to the executors.\n",
    "* Are Spark objects (`SparkSession`, `RDD`s) being captured in a closure? This can happen when you are defining your function anonymously. If your function is defined anonymously, perhaps you can define it elsewhere.\n",
    "\n",
    "These tips can help find common errors, but the solution to this problem is something that can be determined only on a case-by-case basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L32XtqkiyzSS"
   },
   "source": [
    "#### Ordering\n",
    "\n",
    "When working with distributed data, there is not necessarily a guaranteed order to the items in your data. When writing your code, keep in mind that the data exists in partitions across the cluster.\n",
    "\n",
    "This is not to say that we cannot define an order. We can define an order on the partitions by an index. In this situation, the \"first\" element of the `RDD` will be the first element of the first partition. Furthermore, let's say we want to order an `RDD[Int]` ascending by value. Using our ordering on partitions, we can shuffle the data such that all elements in partition `i` are less than all elements in partition `i+1`. From here, we can sort each partition. Now we have a sorted `RDD`. This is an expensive operation, however.\n",
    "\n",
    "#### Output and logging\n",
    "\n",
    "When writing functions that are used to transform data, it is often useful to print statements or, preferably, to log statements to look at the state of variables in the function. In a distributed context, this is more complicated because the function is not running on the same machine as the program. Accessing the logs and the stdout generally depends on the configuration of the cluster and which master you are using. In some situations, it may suffice to run your program on a small set of data in local mode.\n",
    "\n",
    "#### Spark jobs\n",
    "\n",
    "A Spark-based program will have a `SparkSession`, which is how the driver talks to the master.  Before Spark version 2.x, the `SparkContext` was used for this. There is still a `SparkContext`, but it is part of the `SparkSession` now. This `SparkSession` represents the `App`. The `App` submits `jobs` to the master. The `jobs` are broken into `stages`, which are logical groupings of work in the job. The `stages` are broken into `tasks`, which represent the work to be done on each partition.\n",
    "\n",
    "![Spark jobs](https://i.imgur.com/Lmfc5Nc.png)  \n",
    "_Spark jobs_\n",
    "\n",
    "Not every operation on data will start a job. Spark is lazy—in a good way. Execution is done only when a result is needed. This allows the work done to be organized into an execution plan to improve efficiency. There are certain operations, sometimes referred to as actions, that will cause execution immediately. These are operations that return specific values, e.g. `aggregate`. There are also operations where further execution planning becomes impossible until computed, e.g. `zipWithIndex`.\n",
    "\n",
    "  This execution plan is often referred to as the Directed Acyclic Graph (DAG). Data in Spark is defined by its DAG, which includes its sources and whatever operations were run to generate it. This allows Spark to remove data from memory as necessary without losing reference to the data. If data that was generated and removed is referred to later, it will be regenerated. This can be time-consuming. Fortunately, we can instruct Spark to keep data if we need to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVDozVzNyzWE"
   },
   "source": [
    "#### Persisting\n",
    "\n",
    "The basic way of persisting data in Spark is with the persist method. This creates a checkpoint. You can also use the cache method and provide options for configuring how the data will be persisted. The data will still be generated lazily, but once it is generated, it will be kept.\n",
    "\n",
    "Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YT9sA32I2uxJ"
   },
   "outputs": [],
   "source": [
    "from operator import concat, itemgetter, methodcaller\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9hk7cOg82wxl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/20 21:26:54 WARN Utils: Your hostname, Yeonwooui-MacBookPro.local resolves to a loopback address: 127.0.0.1; using 172.30.1.60 instead (on interface en0)\n",
      "23/08/20 21:26:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/ywsung/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/ywsung/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f8e9d460-b3f1-4cb1-b4af-e885dbc46775;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/ywsung/Desktop/spark-nlp-book-master/venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.0.2 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.2 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.15.0 in central\n",
      "downloading https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/spark-nlp_2.12/5.0.2/spark-nlp_2.12-5.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.johnsnowlabs.nlp#spark-nlp_2.12;5.0.2!spark-nlp_2.12.jar (2925ms)\n",
      "downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.2/config-1.4.2.jar ...\n",
      "\t[SUCCESSFUL ] com.typesafe#config;1.4.2!config.jar(bundle) (321ms)\n",
      "downloading https://repo1.maven.org/maven2/org/rocksdb/rocksdbjni/6.29.5/rocksdbjni-6.29.5.jar ...\n",
      "\t[SUCCESSFUL ] org.rocksdb#rocksdbjni;6.29.5!rocksdbjni.jar (2368ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.828/aws-java-sdk-bundle-1.11.828.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.828!aws-java-sdk-bundle.jar (7342ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/universal-automata/liblevenshtein/3.0.0/liblevenshtein-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.github.universal-automata#liblevenshtein;3.0.0!liblevenshtein.jar (310ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/cloud/google-cloud-storage/2.20.1/google-cloud-storage-2.20.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.cloud#google-cloud-storage;2.20.1!google-cloud-storage.jar (334ms)\n",
      "downloading https://repo1.maven.org/maven2/com/navigamez/greex/1.0/greex-1.0.jar ...\n",
      "\t[SUCCESSFUL ] com.navigamez#greex;1.0!greex.jar (297ms)\n",
      "downloading https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/tensorflow-cpu_2.12/0.4.4/tensorflow-cpu_2.12-0.4.4.jar ...\n",
      "\t[SUCCESSFUL ] com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4!tensorflow-cpu_2.12.jar (11645ms)\n",
      "downloading https://repo1.maven.org/maven2/com/microsoft/onnxruntime/onnxruntime/1.15.0/onnxruntime-1.15.0.jar ...\n",
      "\t[SUCCESSFUL ] com.microsoft.onnxruntime#onnxruntime;1.15.0!onnxruntime.jar (3005ms)\n",
      "downloading https://repo1.maven.org/maven2/it/unimi/dsi/fastutil/7.0.12/fastutil-7.0.12.jar ...\n",
      "\t[SUCCESSFUL ] it.unimi.dsi#fastutil;7.0.12!fastutil.jar (1022ms)\n",
      "downloading https://repo1.maven.org/maven2/org/projectlombok/lombok/1.16.8/lombok-1.16.8.jar ...\n",
      "\t[SUCCESSFUL ] org.projectlombok#lombok;1.16.8!lombok.jar (370ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/guava/guava/31.1-jre/guava-31.1-jre.jar ...\n",
      "\t[SUCCESSFUL ] com.google.guava#guava;31.1-jre!guava.jar(bundle) (393ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/guava/failureaccess/1.0.1/failureaccess-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.guava#failureaccess;1.0.1!failureaccess.jar(bundle) (297ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar ...\n",
      "\t[SUCCESSFUL ] com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava!listenablefuture.jar (297ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/errorprone/error_prone_annotations/2.18.0/error_prone_annotations-2.18.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.errorprone#error_prone_annotations;2.18.0!error_prone_annotations.jar (299ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/j2objc/j2objc-annotations/1.3/j2objc-annotations-1.3.jar ...\n",
      "\t[SUCCESSFUL ] com.google.j2objc#j2objc-annotations;1.3!j2objc-annotations.jar (300ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/http-client/google-http-client/1.43.0/google-http-client-1.43.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.http-client#google-http-client;1.43.0!google-http-client.jar (310ms)\n",
      "downloading https://repo1.maven.org/maven2/io/opencensus/opencensus-contrib-http-util/0.31.1/opencensus-contrib-http-util-0.31.1.jar ...\n",
      "\t[SUCCESSFUL ] io.opencensus#opencensus-contrib-http-util;0.31.1!opencensus-contrib-http-util.jar (300ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/http-client/google-http-client-jackson2/1.43.0/google-http-client-jackson2-1.43.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.http-client#google-http-client-jackson2;1.43.0!google-http-client-jackson2.jar (303ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/http-client/google-http-client-gson/1.43.0/google-http-client-gson-1.43.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.http-client#google-http-client-gson;1.43.0!google-http-client-gson.jar (301ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api-client/google-api-client/2.2.0/google-api-client-2.2.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api-client#google-api-client;2.2.0!google-api-client.jar (323ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-codec/commons-codec/1.15/commons-codec-1.15.jar ...\n",
      "\t[SUCCESSFUL ] commons-codec#commons-codec;1.15!commons-codec.jar (346ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/oauth-client/google-oauth-client/1.34.1/google-oauth-client-1.34.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.oauth-client#google-oauth-client;1.34.1!google-oauth-client.jar (314ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/http-client/google-http-client-apache-v2/1.43.0/google-http-client-apache-v2-1.43.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.http-client#google-http-client-apache-v2;1.43.0!google-http-client-apache-v2.jar (313ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/apis/google-api-services-storage/v1-rev20220705-2.0.0/google-api-services-storage-v1-rev20220705-2.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0!google-api-services-storage.jar (319ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/gson/gson/2.10.1/gson-2.10.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.gson#gson;2.10.1!gson.jar (326ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/cloud/google-cloud-core/2.12.0/google-cloud-core-2.12.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.cloud#google-cloud-core;2.12.0!google-cloud-core.jar (320ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-context/1.53.0/grpc-context-1.53.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-context;1.53.0!grpc-context.jar (312ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/auto/value/auto-value-annotations/1.10.1/auto-value-annotations-1.10.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.auto.value#auto-value-annotations;1.10.1!auto-value-annotations.jar (311ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/auto/value/auto-value/1.10.1/auto-value-1.10.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.auto.value#auto-value;1.10.1!auto-value.jar (524ms)\n",
      "downloading https://repo1.maven.org/maven2/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar ...\n",
      "\t[SUCCESSFUL ] javax.annotation#javax.annotation-api;1.3.2!javax.annotation-api.jar (311ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.2/commons-logging-1.2.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.2!commons-logging.jar (311ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/cloud/google-cloud-core-http/2.12.0/google-cloud-core-http-2.12.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.cloud#google-cloud-core-http;2.12.0!google-cloud-core-http.jar (311ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/http-client/google-http-client-appengine/1.43.0/google-http-client-appengine-1.43.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.http-client#google-http-client-appengine;1.43.0!google-http-client-appengine.jar (315ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api/gax-httpjson/0.108.2/gax-httpjson-0.108.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api#gax-httpjson;0.108.2!gax-httpjson.jar (315ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/cloud/google-cloud-core-grpc/2.12.0/google-cloud-core-grpc-2.12.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.cloud#google-cloud-core-grpc;2.12.0!google-cloud-core-grpc.jar (310ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-alts/1.53.0/grpc-alts-1.53.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-alts;1.53.0!grpc-alts.jar (329ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-grpclb/1.53.0/grpc-grpclb-1.53.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-grpclb;1.53.0!grpc-grpclb.jar (321ms)\n",
      "downloading https://repo1.maven.org/maven2/org/conscrypt/conscrypt-openjdk-uber/2.5.2/conscrypt-openjdk-uber-2.5.2.jar ...\n",
      "\t[SUCCESSFUL ] org.conscrypt#conscrypt-openjdk-uber;2.5.2!conscrypt-openjdk-uber.jar (485ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-auth/1.53.0/grpc-auth-1.53.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-auth;1.53.0!grpc-auth.jar (310ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-protobuf/1.53.0/grpc-protobuf-1.53.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-protobuf;1.53.0!grpc-protobuf.jar (310ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-protobuf-lite/1.53.0/grpc-protobuf-lite-1.53.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-protobuf-lite;1.53.0!grpc-protobuf-lite.jar (304ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-core/1.53.0/grpc-core-1.53.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-core;1.53.0!grpc-core.jar (354ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api/gax/2.23.2/gax-2.23.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api#gax;2.23.2!gax.jar (332ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api/gax-grpc/2.23.2/gax-grpc-2.23.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api#gax-grpc;2.23.2!gax-grpc.jar (316ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/auth/google-auth-library-credentials/1.16.0/google-auth-library-credentials-1.16.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.auth#google-auth-library-credentials;1.16.0!google-auth-library-credentials.jar (305ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/auth/google-auth-library-oauth2-http/1.16.0/google-auth-library-oauth2-http-1.16.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.auth#google-auth-library-oauth2-http;1.16.0!google-auth-library-oauth2-http.jar (332ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api/api-common/2.6.2/api-common-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api#api-common;2.6.2!api-common.jar (315ms)\n",
      "downloading https://repo1.maven.org/maven2/io/opencensus/opencensus-api/0.31.1/opencensus-api-0.31.1.jar ...\n",
      "\t[SUCCESSFUL ] io.opencensus#opencensus-api;0.31.1!opencensus-api.jar (347ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api/grpc/proto-google-iam-v1/1.9.2/proto-google-iam-v1-1.9.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api.grpc#proto-google-iam-v1;1.9.2!proto-google-iam-v1.jar (312ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.21.12/protobuf-java-3.21.12.jar ...\n",
      "\t[SUCCESSFUL ] com.google.protobuf#protobuf-java;3.21.12!protobuf-java.jar(bundle) (421ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java-util/3.21.12/protobuf-java-util-3.21.12.jar ...\n",
      "\t[SUCCESSFUL ] com.google.protobuf#protobuf-java-util;3.21.12!protobuf-java-util.jar(bundle) (319ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api/grpc/proto-google-common-protos/2.14.2/proto-google-common-protos-2.14.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api.grpc#proto-google-common-protos;2.14.2!proto-google-common-protos.jar (401ms)\n",
      "downloading https://repo1.maven.org/maven2/org/threeten/threetenbp/1.6.5/threetenbp-1.6.5.jar ...\n",
      "\t[SUCCESSFUL ] org.threeten#threetenbp;1.6.5!threetenbp.jar (340ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api/grpc/proto-google-cloud-storage-v2/2.20.1-alpha/proto-google-cloud-storage-v2-2.20.1-alpha.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha!proto-google-cloud-storage-v2.jar (357ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api/grpc/grpc-google-cloud-storage-v2/2.20.1-alpha/grpc-google-cloud-storage-v2-2.20.1-alpha.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha!grpc-google-cloud-storage-v2.jar (310ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api/grpc/gapic-google-cloud-storage-v2/2.20.1-alpha/gapic-google-cloud-storage-v2-2.20.1-alpha.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha!gapic-google-cloud-storage-v2.jar (313ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.14.2/jackson-core-2.14.2.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-core;2.14.2!jackson-core.jar(bundle) (341ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (308ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-api/1.53.0/grpc-api-1.53.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-api;1.53.0!grpc-api.jar (320ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-stub/1.53.0/grpc-stub-1.53.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-stub;1.53.0!grpc-stub.jar (319ms)\n",
      "downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.31.0/checker-qual-3.31.0.jar ...\n",
      "\t[SUCCESSFUL ] org.checkerframework#checker-qual;3.31.0!checker-qual.jar (327ms)\n",
      "downloading https://repo1.maven.org/maven2/io/perfmark/perfmark-api/0.26.0/perfmark-api-0.26.0.jar ...\n",
      "\t[SUCCESSFUL ] io.perfmark#perfmark-api;0.26.0!perfmark-api.jar (307ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/android/annotations/4.1.1.4/annotations-4.1.1.4.jar ...\n",
      "\t[SUCCESSFUL ] com.google.android#annotations;4.1.1.4!annotations.jar (304ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/mojo/animal-sniffer-annotations/1.22/animal-sniffer-annotations-1.22.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.mojo#animal-sniffer-annotations;1.22!animal-sniffer-annotations.jar (311ms)\n",
      "downloading https://repo1.maven.org/maven2/io/opencensus/opencensus-proto/0.2.0/opencensus-proto-0.2.0.jar ...\n",
      "\t[SUCCESSFUL ] io.opencensus#opencensus-proto;0.2.0!opencensus-proto.jar (351ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-services/1.53.0/grpc-services-1.53.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-services;1.53.0!grpc-services.jar (345ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/re2j/re2j/1.6/re2j-1.6.jar ...\n",
      "\t[SUCCESSFUL ] com.google.re2j#re2j;1.6!re2j.jar (318ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-netty-shaded/1.53.0/grpc-netty-shaded-1.53.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-netty-shaded;1.53.0!grpc-netty-shaded.jar (677ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-googleapis/1.53.0/grpc-googleapis-1.53.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-googleapis;1.53.0!grpc-googleapis.jar (306ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-xds/1.53.0/grpc-xds-1.53.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-xds;1.53.0!grpc-xds.jar (777ms)\n",
      "downloading https://repo1.maven.org/maven2/dk/brics/automaton/automaton/1.11-8/automaton-1.11-8.jar ...\n",
      "\t[SUCCESSFUL ] dk.brics.automaton#automaton;1.11-8!automaton.jar (320ms)\n",
      ":: resolution report :: resolve 78139ms :: artifacts dl 50891ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.2 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.0.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.15.0 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   75  |   75  |   75  |   3   ||   72  |   72  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f8e9d460-b3f1-4cb1-b4af-e885dbc46775\n",
      "\tconfs: [default]\n",
      "\t72 artifacts copied, 0 already retrieved (687837kB/492ms)\n",
      "23/08/20 21:29:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "packages = ','.join([\n",
    "    \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.0.2\",\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"spark-nlp-book-p1c3\") \\\n",
    "    .config(\"spark.jars.packages\", packages)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qTnaiFGd2zr2"
   },
   "outputs": [],
   "source": [
    "def has_moon(text):\n",
    "    if 'moon' in text:\n",
    "        sleep(1)\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "15Rps9or1UgZ",
    "outputId": "7697f362-c7e7-4b78-a5aa-86416bf4847d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This appears quickly because the previous operations are all lazy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "This appears slowly since the count method will call has_moon which sleeps\n",
      "Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!bb3.andrew.cmu.edu!news.sei.cmu.edu!cis.ohi\n",
      "This appears quickly because has_moon will not be called due to the data being persisted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# RDD containing filepath-text pairs\n",
    "path = os.path.join('data', 'mini_newsgroups', 'sci.space')\n",
    "text_pairs = spark.sparkContext\\\n",
    "    .wholeTextFiles(path) \n",
    "texts = text_pairs.map(itemgetter(1))\n",
    "lower_cased = texts.map(methodcaller('lower'))\n",
    "moon_texts = texts.filter(has_moon).persist()\n",
    "print('This appears quickly because the previous operations are '\n",
    "      'all lazy')\n",
    "print(moon_texts.count())\n",
    "print('This appears slowly since the count method will call '\n",
    "      'has_moon which sleeps')\n",
    "print(moon_texts.reduce(concat)[:100])\n",
    "print('This appears quickly because has_moon will not be '\n",
    "      'called due to the data being persisted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rItFYkup28xs"
   },
   "source": [
    "Now that we have an idea of how Spark works, let's go back to our word-count problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2023.8.8-cp311-cp311-macosx_11_0_arm64.whl (289 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.3/289.3 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.3.2 nltk-3.8.1 regex-2023.8.8 tqdm-4.66.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aeQtbQ2_1Wll"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from operator import add\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "kvb4Iavz2_1z",
    "outputId": "c32035d9-024a-4ad3-f43c-eee6fd469990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "filepath-text pair of first document\n",
      "('file:/Users/ywsung/Desktop/spark-nlp-book-master/jupyter/data/mini_newsgroups/sci.space/62480', \"Xref: cantaloupe.srv.cs.cmu.edu sci.environment:30751 misc.consumers:69257 misc.invest:42674 sci.astro:36052 talk.environment:12249 talk.politics.space:2873 sci.space:62480 rec.backcountry:32316 misc.rural:6339 misc.headlines:42268 misc.legal:63427\\nNewsgroups: sci.environment,misc.consumers,misc.invest,sci.astro,talk.environment,talk.politics.space,sci.space,rec.backcountry,misc.rural,misc.headlines,k12.chat.teacher,misc.legal\\nPath: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!europa.eng.gtefsd.com!fs7.ece.cmu.edu!fs7.ECE.CMU.EDU!loss\\nFrom: loss@fs7.ECE.CMU.EDU (Doug Loss)\\nSubject: Re: Space Marketing would be wonderfull.\\nMessage-ID: <C76BMs.8x5@fs7.ece.cmu.edu>\\nSender: news@fs7.ece.cmu.edu (USENET News System)\\nOrganization: Electrical and Computer Engineering, Carnegie Mellon\\nReferences: <C75A0p.8DK@news.cso.uiuc.edu> <pgf.737604987@srl03.cacs.usl.edu> <C760Dv.K75@agora.rain.com>\\nDate: Mon, 17 May 1993 13:47:13 GMT\\nLines: 17\\n\\nIn article <C760Dv.K75@agora.rain.com> jhart@agora.rain.com (Jim Hart) writes:\\n>\\n>[...]\\n>\\n>Astronomers have been using the night sky for thousands of years --\\n>they own it.  If they don't complain now against scenic trespassers\\n>(eg light polluters), they will lose their common-law right of ownership.\\n>\\n   Is English (American, Canadian, etc.) common law recognized as\\nlegally binding under international law?  After all, we're talking about\\nsomething that by its very nature isn't limited to the territory of one\\nnation.\\n\\nDoug Loss\\nloss@husky.bloomu.edu\\n\\n\\n\")\n"
     ]
    }
   ],
   "source": [
    "# RDD containing filepath-text pairs\n",
    "texts = spark.sparkContext.wholeTextFiles(path) \n",
    "print('\\n\\nfilepath-text pair of first document')\n",
    "print(texts.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "99l4bgDe3BV5",
    "outputId": "ac24662f-5601-4b02-f509-14a47684f2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "tokenized text of first document\n",
      "['Xref', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'sci', 'environment', '30751', 'misc', 'consumers', '69257', 'misc', 'invest', '42674', 'sci', 'astro', '36052', 'talk', 'environment', '12249', 'talk', 'politics', 'space', '2873', 'sci', 'space', '62480', 'rec', 'backcountry', '32316', 'misc', 'rural', '6339', 'misc', 'headlines', '42268', 'misc', 'legal', '63427', 'Newsgroups', 'sci', 'environment', 'misc', 'consumers', 'misc', 'invest', 'sci', 'astro', 'talk', 'environment', 'talk', 'politics', 'space', 'sci', 'space', 'rec', 'backcountry', 'misc', 'rural', 'misc', 'headlines', 'k12', 'chat', 'teacher', 'misc', 'legal', 'Path', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'das', 'news', 'harvard', 'edu', 'noc', 'near', 'net', 'howland', 'reston', 'ans', 'net', 'europa', 'eng', 'gtefsd', 'com', 'fs7', 'ece', 'cmu', 'edu', 'fs7', 'ECE', 'CMU', 'EDU', 'loss', 'From', 'loss', 'fs7', 'ECE', 'CMU', 'EDU', 'Doug', 'Loss', 'Subject', 'Re', 'Space', 'Marketing', 'would', 'be', 'wonderfull', 'Message', 'ID', 'C76BMs', '8x5', 'fs7', 'ece', 'cmu', 'edu', 'Sender', 'news', 'fs7', 'ece', 'cmu', 'edu', 'USENET', 'News', 'System', 'Organization', 'Electrical', 'and', 'Computer', 'Engineering', 'Carnegie', 'Mellon', 'References', 'C75A0p', '8DK', 'news', 'cso', 'uiuc', 'edu', 'pgf', '737604987', 'srl03', 'cacs', 'usl', 'edu', 'C760Dv', 'K75', 'agora', 'rain', 'com', 'Date', 'Mon', '17', 'May', '1993', '13', '47', '13', 'GMT', 'Lines', '17', 'In', 'article', 'C760Dv', 'K75', 'agora', 'rain', 'com', 'jhart', 'agora', 'rain', 'com', 'Jim', 'Hart', 'writes', 'Astronomers', 'have', 'been', 'using', 'the', 'night', 'sky', 'for', 'thousands', 'of', 'years', 'they', 'own', 'it', 'If', 'they', 'don', 't', 'complain', 'now', 'against', 'scenic', 'trespassers', 'eg', 'light', 'polluters', 'they', 'will', 'lose', 'their', 'common', 'law', 'right', 'of', 'ownership', 'Is', 'English', 'American', 'Canadian', 'etc', 'common', 'law', 'recognized', 'as', 'legally', 'binding', 'under', 'international', 'law', 'After', 'all', 'we', 're', 'talking', 'about', 'something', 'that', 'by', 'its', 'very', 'nature', 'isn', 't', 'limited', 'to', 'the', 'territory', 'of', 'one', 'nation', 'Doug', 'Loss', 'loss', 'husky', 'bloomu', 'edu']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+', gaps=False)\n",
    "tokenized_texts = texts.map(\n",
    "    lambda path_text: tokenizer.tokenize(path_text[1]))\n",
    "print('\\n\\ntokenized text of first document')\n",
    "print(tokenized_texts.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "yrlw1Gib3Coz",
    "outputId": "b9aab2d0-1601-4fbb-c2cd-4737b86817e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "document-level counts of first document\n",
      "[('misc', 10), ('edu', 9), ('sci', 6), ('cmu', 5), ('fs7', 5), ('environment', 4), ('talk', 4), ('space', 4), ('com', 4), ('news', 3)]\n"
     ]
    }
   ],
   "source": [
    "# This is the equivalent place that the previous implementations \n",
    "# started\n",
    "document_token_counts = tokenized_texts.map(Counter)\n",
    "print('\\n\\ndocument-level counts of first document')\n",
    "print(document_token_counts.first().most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "eZEbtGxk3D96",
    "outputId": "141e63c1-e557-4f1f-83e5-5e7a7e8c89c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "word counts\n",
      "[('the', 1648), ('of', 804), ('edu', 784), ('to', 770), ('and', 641), ('a', 615), ('in', 402), ('is', 352), ('for', 305), ('cmu', 288)]\n"
     ]
    }
   ],
   "source": [
    "word_counts = token_counts = document_token_counts.reduce(add)\n",
    "print('\\n\\nword counts')\n",
    "print(word_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6cqJ7p9s3GPo"
   },
   "source": [
    "As you see, we use the map and reduce methods here. Spark allows you to implement MapReduce-style programs, but you can also implement in many other ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Crtr94y3JTF"
   },
   "source": [
    "#### Python and R\n",
    "\n",
    "Spark is primarily implemented in Scala. The Java API is there to allow more idiomatic Java use of Spark.     There is also a Python API (PySpark) and an R API (SparkR). Spark-based programs implemented in Scala or Java run on the same JVM that serves as the driver. Programs implemented in PySpark or SparkR run in Python and R processes, respectively, with the SparkSession ultimately in a different process. This generally does not affect the performance, unless we use functions defined in Python or R.\n",
    "\n",
    "As can be seen in the previous example, when we are tokenizing, counting, and combining counts we are calling Python code to process our data. This is accomplished by the JVM process serializing and shipping the data to the Python process, which is then deserialized, processed, serialized, and shipped back to the JVM to be deserialized. This adds a lot of extra work to our job. When using PySpark or SparkR it will be faster to use internal Spark functions whenever possible.\n",
    "\n",
    "Not using custom functions in Python or R seems restrictive when using `RDD`s, but most likely, your work will be using the `DataFrame`s and `DataSet`s that we discuss in the next section  .  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CjGyqDiA3Pzd"
   },
   "source": [
    "## Spark SQL and Spark MLlib\n",
    "\n",
    "Since the release of Spark 2,  the primary intended way to work with data within Spark is through the `Dataset`. The `Dataset[T]` is an object that allows us to treat our distributed data as tables. The type parameter `T` is the type used to represent the rows of the table. There is a special kind of `Dataset` in which the type of the rows is `Row`, which allows us to have tabular data without defining new classes—this does come at the cost of losing some type safety. The examples we'll be using will generally be with `DataFrame`s, since they are the best way to work with data in PySpark.\n",
    "\n",
    "The `Dataset` and `DataFrame` are defined in the Spark SQL module, since one of the greatest benefits is the ability to express many operations with SQL.   The prebuilt user-defined functions (`UDF`s) are available in all the APIs. This allows us to do most kinds of processing in the non-JVM languages with the same efficiency as if we were using Scala or Java.\n",
    "\n",
    "Another module we need to introduce before we begin talking about Spark NLP is MLlib. MLlib is a module for doing machine learning on Spark. Before Spark 2, all the MLlib algorithms were implemented on `RDD`s. Since then, a new version of MLlib was defined using `Dataset`s and `DataFrame`s. MLlib is similar in design, at a high level, to other machine learning libraries, with a notion of transformers, models, and pipelines.\n",
    "\n",
    "Before we talk about MLlib, let's load some data into a `DataFrame`, since MLlib is built using `DataFrame`s.  We will be using the Iris data set, which is often used as an example in data science. It's small, easy to understand, and can work for clustering and classification examples. It is structured data, so it doesn't give us any text data to work with. Table-like structure is generally designed around structured data, so this data will help us explore the API before getting into using Spark for text.\n",
    "\n",
    "The `iris.data` file does not have a header, so we have to tell Spark what the columns are when they are loaded. Let's construct a schema.  The schema is the definition of the columns and their types in the `DataFrame`. The most common task is to build a model to predict what class an iris flower is (I. virginica, I. setosa, or I. versicolor) based on its sepals and petals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.0.3-cp311-cp311-macosx_11_0_arm64.whl (10.7 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ywsung/Desktop/spark-nlp-book-master/venv/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Collecting numpy>=1.21.0\n",
      "  Using cached numpy-1.25.2-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ywsung/Desktop/spark-nlp-book-master/venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-1.25.2 pandas-2.0.3 pytz-2023.3 tzdata-2023.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iarB5A523E_5"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "od1TGFPv36cS"
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('sepal_length', DoubleType(), nullable=False),\n",
    "    StructField('sepal_width', DoubleType(), nullable=False),\n",
    "    StructField('petal_length', DoubleType(), nullable=False),\n",
    "    StructField('petal_width', DoubleType(), nullable=False),\n",
    "    StructField('class', StringType(), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uXkOH7Fv3_oJ"
   },
   "outputs": [],
   "source": [
    "iris = spark.read.csv('./data/iris.data', schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "k8mb_RIV4gG5",
    "outputId": "001fbfff-4432-44f3-e69c-fdc0e8186d91"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/20 21:31:33 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>5.843333333333335</td>\n",
       "      <td>3.0540000000000007</td>\n",
       "      <td>3.7586666666666693</td>\n",
       "      <td>1.1986666666666672</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>0.8280661279778637</td>\n",
       "      <td>0.43359431136217375</td>\n",
       "      <td>1.764420419952262</td>\n",
       "      <td>0.7631607417008414</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>7.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary        sepal_length          sepal_width        petal_length  \\\n",
       "0   count                 150                  150                 150   \n",
       "1    mean   5.843333333333335   3.0540000000000007  3.7586666666666693   \n",
       "2  stddev  0.8280661279778637  0.43359431136217375   1.764420419952262   \n",
       "3     min                 4.3                  2.0                 1.0   \n",
       "4     max                 7.9                  4.4                 6.9   \n",
       "\n",
       "          petal_width           class  \n",
       "0                 150             150  \n",
       "1  1.1986666666666672            None  \n",
       "2  0.7631607417008414            None  \n",
       "3                 0.1     Iris-setosa  \n",
       "4                 2.5  Iris-virginica  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2F9XRpbd4p9w"
   },
   "source": [
    "Let's start by looking at some of the summary stats for the Iris setosa class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "Xy9s9xWP4k9R",
    "outputId": "b2cfda21-4008-4d3d-9691-1e1686b79019"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             class\n",
       "0   Iris-virginica\n",
       "1      Iris-setosa\n",
       "2  Iris-versicolor"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.select('class').distinct().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbsRnoJe4qiv"
   },
   "source": [
    "We can register a `DataFrame`, which will allow us to interact with it purely through SQL. We will be registering our `DataFrame` as a temporary table. This means that the table will exist only for the lifetime of our `App`, and it will be available only through our `App`'s `SparkSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQeIiXFk4nov"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ywsung/Desktop/spark-nlp-book-master/venv/lib/python3.11/site-packages/pyspark/sql/dataframe.py:330: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "iris.registerTempTable('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "2Nr8T9Lb42Ng",
    "outputId": "dc6cfc0d-5f74-411b-94a0-2814722b80dd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT *\n",
    "FROM iris\n",
    "LIMIT 5\n",
    "''').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "niPjvvGV45Qe"
   },
   "source": [
    "Let's look at some of the fields grouped by their class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "Tc847MrT43HQ",
    "outputId": "837314c2-b010-447e-c0ef-24bdfc37caa5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>min(sepal_length)</th>\n",
       "      <th>avg(sepal_length)</th>\n",
       "      <th>max(sepal_length)</th>\n",
       "      <th>min(sepal_width)</th>\n",
       "      <th>avg(sepal_width)</th>\n",
       "      <th>max(sepal_width)</th>\n",
       "      <th>min(petal_length)</th>\n",
       "      <th>avg(petal_length)</th>\n",
       "      <th>max(petal_length)</th>\n",
       "      <th>min(petal_width)</th>\n",
       "      <th>avg(petal_width)</th>\n",
       "      <th>max(petal_width)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>4.9</td>\n",
       "      <td>6.588</td>\n",
       "      <td>7.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.974</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.552</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.026</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.006</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.418</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.464</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.936</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.770</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.260</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.326</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             class  min(sepal_length)  avg(sepal_length)  max(sepal_length)  \\\n",
       "0   Iris-virginica                4.9              6.588                7.9   \n",
       "1      Iris-setosa                4.3              5.006                5.8   \n",
       "2  Iris-versicolor                4.9              5.936                7.0   \n",
       "\n",
       "   min(sepal_width)  avg(sepal_width)  max(sepal_width)  min(petal_length)  \\\n",
       "0               2.2             2.974               3.8                4.5   \n",
       "1               2.3             3.418               4.4                1.0   \n",
       "2               2.0             2.770               3.4                3.0   \n",
       "\n",
       "   avg(petal_length)  max(petal_length)  min(petal_width)  avg(petal_width)  \\\n",
       "0              5.552                6.9               1.4             2.026   \n",
       "1              1.464                1.9               0.1             0.244   \n",
       "2              4.260                5.1               1.0             1.326   \n",
       "\n",
       "   max(petal_width)  \n",
       "0               2.5  \n",
       "1               0.6  \n",
       "2               1.8  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT \n",
    "    class, \n",
    "    min(sepal_length), avg(sepal_length), max(sepal_length),\n",
    "    min(sepal_width), avg(sepal_width), max(sepal_width),\n",
    "    min(petal_length), avg(petal_length), max(petal_length),\n",
    "    min(petal_width), avg(petal_width), max(petal_width)\n",
    "FROM iris\n",
    "GROUP BY class\n",
    "''').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMOw8zNL48P3"
   },
   "source": [
    "### Transformers\n",
    "\n",
    "A `Transformer` is a piece of logic that transforms the data without needing to learn or fit anything from the data. A good way to understand transformers is that they represent functions that we wish to map over our data. All stages of a pipeline have parameters so that we can make sure that the transformation is being applied to the right fields and with the desired configuration. Let's look at a few examples.\n",
    "\n",
    "#### SQLTransformer\n",
    "\n",
    "The `SQLTransformer` has only one parameter—`statement`—which is the SQL statement that will be executed against our `DataFrame`. Let's use an `SQLTransformer` to do the group-by we performed previously. table0306 shows the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kXQnJvqm46sY"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "statement = '''\n",
    "SELECT \n",
    "    class, \n",
    "    min(sepal_length), avg(sepal_length), max(sepal_length),\n",
    "    min(sepal_width), avg(sepal_width), max(sepal_width),\n",
    "    min(petal_length), avg(petal_length), max(petal_length),\n",
    "    min(petal_width), avg(petal_width), max(petal_width)\n",
    "FROM iris\n",
    "GROUP BY class\n",
    "'''\n",
    "\n",
    "sql_transformer = SQLTransformer(statement=statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "nnzBVd1w5Xw4",
    "outputId": "cd6543ec-ffea-4551-ed01-86b0978f2605"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>min(sepal_length)</th>\n",
       "      <th>avg(sepal_length)</th>\n",
       "      <th>max(sepal_length)</th>\n",
       "      <th>min(sepal_width)</th>\n",
       "      <th>avg(sepal_width)</th>\n",
       "      <th>max(sepal_width)</th>\n",
       "      <th>min(petal_length)</th>\n",
       "      <th>avg(petal_length)</th>\n",
       "      <th>max(petal_length)</th>\n",
       "      <th>min(petal_width)</th>\n",
       "      <th>avg(petal_width)</th>\n",
       "      <th>max(petal_width)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>4.9</td>\n",
       "      <td>6.588</td>\n",
       "      <td>7.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.974</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.552</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.026</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.006</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.418</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.464</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.936</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.770</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.260</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.326</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             class  min(sepal_length)  avg(sepal_length)  max(sepal_length)  \\\n",
       "0   Iris-virginica                4.9              6.588                7.9   \n",
       "1      Iris-setosa                4.3              5.006                5.8   \n",
       "2  Iris-versicolor                4.9              5.936                7.0   \n",
       "\n",
       "   min(sepal_width)  avg(sepal_width)  max(sepal_width)  min(petal_length)  \\\n",
       "0               2.2             2.974               3.8                4.5   \n",
       "1               2.3             3.418               4.4                1.0   \n",
       "2               2.0             2.770               3.4                3.0   \n",
       "\n",
       "   avg(petal_length)  max(petal_length)  min(petal_width)  avg(petal_width)  \\\n",
       "0              5.552                6.9               1.4             2.026   \n",
       "1              1.464                1.9               0.1             0.244   \n",
       "2              4.260                5.1               1.0             1.326   \n",
       "\n",
       "   max(petal_width)  \n",
       "0               2.5  \n",
       "1               0.6  \n",
       "2               1.8  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_transformer.transform(iris).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cg8MfQqg5dyc"
   },
   "source": [
    "We get the same output as when we ran the SQL command.\n",
    "\n",
    "SQLTransformer is useful when you have preprocessing or restructuring that you need to perform on your data before other steps in the pipeline. Now let's look at a transformer that works on one field and returns the original data with a new field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0u134bux5fkc"
   },
   "source": [
    "#### Binarizer\n",
    "\n",
    "The `Binarizer` is a `Transformer` that applies a threshold to a numeric field, turning it into 0s (when below the threshold) and 1s (when above the threshold). It takes three parameters:\n",
    "\n",
    "* inputCol  \n",
    "The column to be binarized\n",
    "* outputCol  \n",
    "The column containing the binarized values\n",
    "* threshold  \n",
    "The threshold we will apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "khg_A1gK5cbS"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "binarizer = Binarizer(\n",
    "    inputCol='sepal_length', \n",
    "    outputCol='sepal_length_above_5', \n",
    "    threshold=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "BBsbfx395v0A",
    "outputId": "d34c1e88-e0f5-4c84-e761-eb09558bcf14"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "      <th>sepal_length_above_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class  \\\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa   \n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa   \n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa   \n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa   \n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa   \n",
       "\n",
       "   sepal_length_above_5  \n",
       "0                   1.0  \n",
       "1                   0.0  \n",
       "2                   0.0  \n",
       "3                   0.0  \n",
       "4                   0.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarizer.transform(iris).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gPilfKc85xsG"
   },
   "source": [
    "Unlike the `SQLTransformer`, the `Binarizer` returns a modified version of the input `DataFrame`. Almost all Transformers behave this way.\n",
    "\n",
    "The `Binarizer` is used when you want to convert a real valued property into a class. For example, if we want to mark social media posts as \"viral\" and \"not-viral\" we could use a `Binarizer` on the views property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aXAFplFK53sH"
   },
   "source": [
    "#### VectorAssembler\n",
    "\n",
    "Another import Transformer is the VectorAssembler. It takes a list of numeric and vector-valued columns and constructs a single vector. This is useful because all MLlib's machine learning algorithms expect a single vector-valued input column for features. The VectorAssembler takes two parameters:\n",
    "\n",
    "* inputCols  \n",
    "The list of columns to be assembled\n",
    "* outputCol  \n",
    "The column containing the new vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g70IjXs558yQ"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'sepal_length', 'sepal_width', \n",
    "        'petal_length', 'petal_width'\n",
    "    ], \n",
    "    outputCol='features'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eTTAhTU35-rJ"
   },
   "source": [
    "Let's persist this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q4EPffJx5-QO"
   },
   "outputs": [],
   "source": [
    "iris_w_vecs = assembler.transform(iris).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "KqWV8YQF6A1A",
    "outputId": "01e4aadf-5dc0-4152-c7f2-8584660a65f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.1, 3.5, 1.4, 0.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.9, 3.0, 1.4, 0.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.7, 3.2, 1.3, 0.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.6, 3.1, 1.5, 0.2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.0, 3.6, 1.4, 0.2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class  \\\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa   \n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa   \n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa   \n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa   \n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa   \n",
       "\n",
       "               features  \n",
       "0  [5.1, 3.5, 1.4, 0.2]  \n",
       "1  [4.9, 3.0, 1.4, 0.2]  \n",
       "2  [4.7, 3.2, 1.3, 0.2]  \n",
       "3  [4.6, 3.1, 1.5, 0.2]  \n",
       "4  [5.0, 3.6, 1.4, 0.2]  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_w_vecs.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HOAHYDh96DVb"
   },
   "source": [
    "### Estimators and Models\n",
    "\n",
    "`Estimator`s allow us to create transformations that are informed by our data. Classification models (e.g., decision trees) and regression models (e.g., linear regressions) are prominent examples, but some preprocessing algorithms are like this as well. For example, preprocessing that needs to know the whole vocabulary first will be `Estimator`s. The `Estimator` is fit with a `DataFrame` and returns a `Model`, which is a kind of Transformer. The `Model`s created from classifier and regression `Estimator`s are `PredictionModel`s.\n",
    "\n",
    "This is a similar design to scikit-learn, with the exception that in scikit-learn when we call `fit` we mutate the estimator instead of creating a new object. There are pros and cons to this, as there always are when debating mutability. Idiomatic Scala strongly prefers immutability.\n",
    "\n",
    "Let's look at some examples of `Estimator`s and `Model`s.\n",
    "\n",
    "#### MinMaxScaler\n",
    "\n",
    "The MinMaxScaler allows us to scale our data to be between 0 and 1. It takes four parameters:\n",
    "\n",
    "* inputCol  \n",
    "The column to be scaled\n",
    "* outputCol  \n",
    "The column containing the scaled values\n",
    "* max  \n",
    "The new maximum value (optional, default = 1)\n",
    "* min  \n",
    "The new minimum value (optional, default = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dc5dBhtZ6B4w"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol='features', \n",
    "    outputCol='petal_length_scaled'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wepcwUUX6h0v"
   },
   "outputs": [],
   "source": [
    "scaler_model = scaler.fit(iris_w_vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "YqiwVXvr6iu_",
    "outputId": "ccfbd608-f1df-497c-df50-74d07dc44356"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "      <th>features</th>\n",
       "      <th>petal_length_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.1, 3.5, 1.4, 0.2]</td>\n",
       "      <td>[0.22222222222222213, 0.625, 0.067796610169491...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.9, 3.0, 1.4, 0.2]</td>\n",
       "      <td>[0.1666666666666668, 0.41666666666666663, 0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.7, 3.2, 1.3, 0.2]</td>\n",
       "      <td>[0.11111111111111119, 0.5, 0.05084745762711865...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.6, 3.1, 1.5, 0.2]</td>\n",
       "      <td>[0.08333333333333327, 0.4583333333333333, 0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.0, 3.6, 1.4, 0.2]</td>\n",
       "      <td>[0.19444444444444448, 0.6666666666666666, 0.06...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class  \\\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa   \n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa   \n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa   \n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa   \n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa   \n",
       "\n",
       "               features                                petal_length_scaled  \n",
       "0  [5.1, 3.5, 1.4, 0.2]  [0.22222222222222213, 0.625, 0.067796610169491...  \n",
       "1  [4.9, 3.0, 1.4, 0.2]  [0.1666666666666668, 0.41666666666666663, 0.06...  \n",
       "2  [4.7, 3.2, 1.3, 0.2]  [0.11111111111111119, 0.5, 0.05084745762711865...  \n",
       "3  [4.6, 3.1, 1.5, 0.2]  [0.08333333333333327, 0.4583333333333333, 0.08...  \n",
       "4  [5.0, 3.6, 1.4, 0.2]  [0.19444444444444448, 0.6666666666666666, 0.06...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_model.transform(iris_w_vecs).limit(5).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCn3_GG76lmz"
   },
   "source": [
    "Notice that the `petal_length_scaled` column now has values between 0 and 1. This can help some training algorithms, specifically those that have difficulty combining features of different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QsGomFZx6n-P"
   },
   "source": [
    "#### StringIndexer\n",
    "\n",
    "Let's build a model! We will try and predict the class from the other features, and we will use a decision tree. First, though, we must convert our target into index values.\n",
    "\n",
    "The `StringIndexer` `Estimator` will turn our class values into indices. We want to do this to simplify some of the downstream processing. It is simpler to implement most training algorithms with the assumption that the target is a number. The `StringIndexer` takes four parameters:\n",
    "\n",
    "* inputCol  \n",
    "The column to be indexed\n",
    "* outputCol  \n",
    "The column containing the indexed values\n",
    "* handleInvalid  \n",
    "The policy for how the model should handle values not seen by the estimator (optional, default = error)\n",
    "* stringOrderType  \n",
    "How to order the values to make the indexing deterministic (optional, default = frequencyDesc)\n",
    "\n",
    "We will also want an `IndexToString` `Transformer`. This will let us map our predictions, which will be indices, back to string values. IndexToString takes three parameters:\n",
    "\n",
    "* inputCol  \n",
    "The column to be mapped\n",
    "* outputCol  \n",
    "The column containing the mapped values\n",
    "* labels  \n",
    "The mapping from index to value, usually generated by StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXK1Egxc6kAO"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "\n",
    "indexer = StringIndexer(inputCol='class', outputCol='class_ix')\n",
    "indexer_model = indexer.fit(iris_w_vecs)\n",
    "\n",
    "index2string = IndexToString(\n",
    "    inputCol=indexer_model.getOrDefault('outputCol'), \n",
    "    outputCol='pred_class', \n",
    "    labels=indexer_model.labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dNd2mh-S679n"
   },
   "outputs": [],
   "source": [
    "iris_indexed = indexer_model.transform(iris_w_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b3Fh42X36-O7"
   },
   "source": [
    "Now we are ready to train our `DecisionTreeClassifier`. This `Estimator` has many parameters, so I recommend you become familiar with the APIs. They are all well documented in the [PySpark API documentation](https://spark.apache.org/docs/latest/api/python/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQL3f3oc6873"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt_clfr = DecisionTreeClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol='class_ix',\n",
    "    maxDepth=5,\n",
    "    impurity='gini',\n",
    "    seed=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rmJmMg77OPX"
   },
   "outputs": [],
   "source": [
    "dt_clfr_model = dt_clfr.fit(iris_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4oVTGw9E7PGn"
   },
   "outputs": [],
   "source": [
    "iris_w_pred = dt_clfr_model.transform(iris_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "Lx6b899H7RoH",
    "outputId": "aad3013a-9879-4d1e-9cef-1f58d552e44d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "      <th>features</th>\n",
       "      <th>class_ix</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.1, 3.5, 1.4, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.9, 3.0, 1.4, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.7, 3.2, 1.3, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.6, 3.1, 1.5, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.0, 3.6, 1.4, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class  \\\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa   \n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa   \n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa   \n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa   \n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa   \n",
       "\n",
       "               features  class_ix     rawPrediction      probability  \\\n",
       "0  [5.1, 3.5, 1.4, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "1  [4.9, 3.0, 1.4, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "2  [4.7, 3.2, 1.3, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "3  [4.6, 3.1, 1.5, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "4  [5.0, 3.6, 1.4, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "\n",
       "   prediction  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_w_pred.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O2zQHR8i7UB-"
   },
   "source": [
    "Now we need to map the predicted classes back to their string form using our `IndexToString`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iqnw--uk7S0P"
   },
   "outputs": [],
   "source": [
    "iris_w_pred_class = index2string.transform(iris_w_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "2l40t2Vl7X31",
    "outputId": "5f3fa9da-b01e-48ca-e5e5-60ce9868fabe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "      <th>features</th>\n",
       "      <th>class_ix</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "      <th>pred_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.1, 3.5, 1.4, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.9, 3.0, 1.4, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.7, 3.2, 1.3, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.6, 3.1, 1.5, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.0, 3.6, 1.4, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class  \\\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa   \n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa   \n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa   \n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa   \n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa   \n",
       "\n",
       "               features  class_ix     rawPrediction      probability  \\\n",
       "0  [5.1, 3.5, 1.4, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "1  [4.9, 3.0, 1.4, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "2  [4.7, 3.2, 1.3, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "3  [4.6, 3.1, 1.5, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "4  [5.0, 3.6, 1.4, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "\n",
       "   prediction   pred_class  \n",
       "0         0.0  Iris-setosa  \n",
       "1         0.0  Iris-setosa  \n",
       "2         0.0  Iris-setosa  \n",
       "3         0.0  Iris-setosa  \n",
       "4         0.0  Iris-setosa  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_w_pred_class.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9M6aBVpm7Z-N"
   },
   "source": [
    "How well did our model fit the data? Let's see how many predictions match the true class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_3ruUra7d5y"
   },
   "source": [
    "### Evaluators\n",
    "\n",
    "The evaluation options in MLlib are still limited compared to libraries like scikit-learn, but they can be useful if you are looking to create an easy-to-run training pipeline that calculates metrics.\n",
    "\n",
    "In our example, we are trying to solve a multiclass prediction problem, so we will use the `MulticlassClassificationEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pDefCgpo7Y2e"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='class_ix', \n",
    "    metricName='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "N-1SaVgK7jmW",
    "outputId": "2f2b03b8-6655-4bcb-bf2b-10897a41c688"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(iris_w_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xFE0inS57mxw"
   },
   "source": [
    "This seems too good. What if we are overfit? Perhaps we should try using cross-validation to evaluate our models. Before we do that, let's organize stages into a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ng0U11uo7qZy"
   },
   "source": [
    "### Pipelines\n",
    "\n",
    "Pipelines are a special kind of Estimator that takes a list of Transformers and Estimators and allows us to use them as a single Estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BTlhmX0Y7lS3"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages=[assembler, indexer, dt_clfr, index2string]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3CYSiXjY7u12"
   },
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "SlQuCNcG7wGe",
    "outputId": "b64ca260-630d-4ffc-8758-38898628bd7f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "      <th>features</th>\n",
       "      <th>class_ix</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "      <th>pred_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.1, 3.5, 1.4, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.9, 3.0, 1.4, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.7, 3.2, 1.3, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[4.6, 3.1, 1.5, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>[5.0, 3.6, 1.4, 0.2]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[50.0, 0.0, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class  \\\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa   \n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa   \n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa   \n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa   \n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa   \n",
       "\n",
       "               features  class_ix     rawPrediction      probability  \\\n",
       "0  [5.1, 3.5, 1.4, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "1  [4.9, 3.0, 1.4, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "2  [4.7, 3.2, 1.3, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "3  [4.6, 3.1, 1.5, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "4  [5.0, 3.6, 1.4, 0.2]       0.0  [50.0, 0.0, 0.0]  [1.0, 0.0, 0.0]   \n",
       "\n",
       "   prediction   pred_class  \n",
       "0         0.0  Iris-setosa  \n",
       "1         0.0  Iris-setosa  \n",
       "2         0.0  Iris-setosa  \n",
       "3         0.0  Iris-setosa  \n",
       "4         0.0  Iris-setosa  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model.transform(iris).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_f-wAMr7yoI"
   },
   "source": [
    "#### Cross validation\n",
    "Now that we have a `Pipeline` and an `Evaluator` we can create a `CrossValidator`. The `CrossValidator` itself is also an `Estimator`. When we call fit, it will fit our `pipeline` to each fold of data, and calculate the metric determined by our `Evaluator`. `CrossValidator` takes five parameters:\n",
    "\n",
    "* estimator  \n",
    "The `Estimator` to be tuned\n",
    "* estimatorParamMaps  \n",
    "The hyperparameter values to try in a hyperparameter grid search\n",
    "* evaluator  \n",
    "The `Evaluator` that calculates the metric\n",
    "* numFolds  \n",
    "The number of folds to split the data into\n",
    "* seed  \n",
    "A seed for making the splits reproducible\n",
    "\n",
    "We will make a trivial hyperparameter grid here, since we are only interested in estimating how well our model does on data it has not seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GdXmuDHj7w-P"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(dt_clfr.maxDepth, [5]).\\\n",
    "    build()\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator, \n",
    "    numFolds=3, \n",
    "    seed=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SHltwkM8CqZ"
   },
   "outputs": [],
   "source": [
    "cv_model = cv.fit(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FUZs-fHz8EvK"
   },
   "source": [
    "Now, we can see how the model does when trained on two-thirds and evaluated on one-third. The `avgMetrics` in `cv_model` contains the average value of the designated metric across folds for each point in the hyperparameter grid tested. In our case, there is only one point in the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "E8bipoJ78Dyu",
    "outputId": "6d363955-e70f-4374-c758-bf84493c5545"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9470457079152732]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_model.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vTDKEZ3D8Jve"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4g-ER3_8NUw"
   },
   "source": [
    "Keep in mind that 95% accuracy is much more believable than 100%.\n",
    "\n",
    "There are many other `Transformer`s, `Estimator`s, and `Model`s. We will look into more as we continue, but for now, there is one more thing we need to discuss—saving our pipelines. \n",
    "\n",
    "#### Serialization of models\n",
    "\n",
    "MLlib allows us to save `Pipelines` so that we can use them later. We can also save individual `Transformers` and `Models`, but we will often want to keep all the stages of a `Pipeline` together. Generally speaking, we use separate programs for building models and using models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3KIXD5g8IPw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline_model.write().overwrite().save('pipeline.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "L-RrvPhE8aj1",
    "outputId": "db996e65-d805-4622-d153-b379e0353b5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline.model/metadata:\n",
      "_SUCCESS   part-00000\n",
      "\n",
      "pipeline.model/stages:\n",
      "\u001b[1m\u001b[36m0_VectorAssembler_85645b5e0ede\u001b[m\u001b[m        \u001b[1m\u001b[36m2_DecisionTreeClassifier_db8b122b211b\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36m1_StringIndexer_755ce8c1c6d2\u001b[m\u001b[m          \u001b[1m\u001b[36m3_IndexToString_dac21c58661f\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! ls pipeline.model/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CDL8Agwr8dbb"
   },
   "source": [
    "## NLP Libraries\n",
    "\n",
    "There are two kinds of NLP libraries, generally speaking: functionality libraries and annotation libraries.\n",
    "\n",
    "### Functionality Libraries\n",
    "\n",
    "A functionality library is a collection of functions built for specific NLP tasks and techniques. Often, the functions are built without assuming that other functions will be used first. This means that functions like part-of-speech (POS) tagging will also perform tokenization. These libraries are good for research because it is often much easier to implement novel functions. On the other hand, because there is no unifying design, the performance of these libraries is generally much worse than that of annotation libraries.\n",
    "\n",
    "The Natural Language Tool Kit (NLTK) is a great functionality library.  It was originally created by Edward Loper. The landmark NLP book Natural Language Processing with Python (O'Reilly) was written by Steven Bird, Ewan Klein, and Edward Loper. I strongly recommend that book to anyone learning NLP. There a many useful and interesting modules in NLTK. It is, and will likely remain, the best NLP library for teaching NLP. The functions are not necessarily implemented with runtime performance or other productionization concerns in mind. If you are working on a research project and using a data set manageable on a single machine, you should consider NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCQSbWMt8knB"
   },
   "source": [
    "### Annotation Libraries\n",
    "   Annotation libraries are libraries in which all the functionality is built around a document-annotation model. There are three objects to keep in mind with annotation libraries: document, annotation, and annotator. The idea behind annotation libraries is to augment the incoming data with the results of our NLP functions.\n",
    "\n",
    "* Document  \n",
    "The document is the representation of the piece of text we wish to process. Naturally, the document must contain the text. Additionally, we often want to have an identifier associated with each document so that we can store our augmented data as structured data. This identifier will often be a title if the texts we are processing have titles.\n",
    "* Annotation  \n",
    "The annotation is the representation of the output of our NLP functions. For the annotation we need to have a type so that later processing knows how to interpret the annotations. Annotations also need to store their location within the document. For example, let's say the word \"pacing\" occurs 134 characters into the document. It will have 134 as the start, and 140 as the end. The lemma annotation for \"pacing\" will have the location. Some annotation libraries also have a concept of document-level annotation that does not have a location. There will be additional fields, depending on the type. Simple annotations like tokens generally don't have extra fields. Stem annotations usually have the stem that was extracted for the range of the text.\n",
    "* Annotator  \n",
    "The annotator is the object that contains the logic for using the NLP function. The annotator will often require configuration or external data sets. Additionally, there are model-based annotators. One of the benefits of an annotation library is that annotators can take advantage of the work done by previous annotators. This naturally creates a notion of a pipeline of annotators.\n",
    "\n",
    "#### spaCy\n",
    "\n",
    "spaCy is an \"industrial strength\" NLP library. I will give a brief description, but I encourage you to go and read their fantastic documentation. spaCy combines the document model just described with a model for the language being processed (English, Spanish, etc.), which has allowed spaCy to support multiple languages in a way that is easy for developers to use. Much of its functionality is implemented in Python to get the speed of native code. If you are working in an environment that is using only Python, and you are unlikely to run distributed processes, then spaCy is a great choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gMV8HhcT8zcD"
   },
   "source": [
    "### NLP in Other Libraries\n",
    "\n",
    "There are some non-NLP libraries that have some NLP functionality. It is often in machine learning libraries to support machine learning on text data.\n",
    "\n",
    "* scikit-learn  \n",
    "A Python machine learning library that has functionality for extracting features from text. This functionality is generally of the bag-of-words kind of processing. The way these processes are built allows them to easily take advantage of more NLP-focused libraries.\n",
    "* Lucene  \n",
    "A Java document search framework that has some text-processing functionality necessary for building a search engine. We will use Lucene later on when we talk about information retrieval.\n",
    "* Gensim  \n",
    "A topic-modeling library (and it performs other distributional semantics techniques). Like spaCy, it is partially implemented in Cython, and like scikit-learn, it allows plug-and-play text processing in its API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m5-K0vny88Ex"
   },
   "source": [
    "## Spark NLP\n",
    "\n",
    "The Spark NLP library was originally designed in early 2017 as an annotation library native to Spark to take full advantage of Spark SQL and MLlib modules. The inspiration came from trying to use Spark to distribute other NLP libraries, which were generally not implemented with concurrency or distributed computing in mind.\n",
    "\n",
    "### Annotation Library\n",
    " Spark NLP has the same concepts as any other annotation library but differs in how it stores annotations. Most annotation libraries store the annotations in the document object, but Spark NLP creates columns for the different types of annotations.\n",
    "\n",
    "The annotators are implemented as `Transformers`, `Estimators`, and `Models`. Let's take a look at some examples.\n",
    "\n",
    "### Stages\n",
    "\n",
    "One of the design principles of Spark NLP is easy interoperability with the existing algorithms in MLlib. Because there is no notion of documents or annotations in MLlib there are transformers for turning text columns into documents and converting annotations into vanilla Spark SQL data types. The usual usage pattern is as follows:\n",
    "\n",
    "1. Load data with Spark SQL.\n",
    "2. Create document column.\n",
    "3. Process with Spark NLP.\n",
    "4. Convert annotations of interest into Spark SQL data types.\n",
    "5. Run additional MLlib stages.\n",
    "\n",
    "We have already looked at how to load data with Spark SQL and how to use MLlib stages in the standard Spark library, so we will look at the middle three stages now. First, we will look at the DocumentAssembler (stage 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FPIftf4H9NYN"
   },
   "source": [
    "#### Transformers\n",
    "\n",
    "To explore these five stages  we will again use the mini_newsgroups data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yocWLnFr8bkc"
   },
   "outputs": [],
   "source": [
    "from sparknlp import DocumentAssembler, Finisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EUiwFbpl9SHO"
   },
   "outputs": [],
   "source": [
    "# RDD containing filepath-text pairs\n",
    "texts = spark.sparkContext.wholeTextFiles(path)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('path', StringType()),\n",
    "    StructField('text', StringType()),\n",
    "])\n",
    "\n",
    "texts = spark.createDataFrame(texts, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "a7sR_06c9TSF",
    "outputId": "48a454d6-03f9-4e42-b4f4-c30a84b3b1aa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.environmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.misc:8162 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "1  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "2  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "3  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "4  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "\n",
       "                                                text  \n",
       "0  Xref: cantaloupe.srv.cs.cmu.edu sci.environmen...  \n",
       "1  Xref: cantaloupe.srv.cs.cmu.edu sci.misc:8162 ...  \n",
       "2  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...  \n",
       "3  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...  \n",
       "4  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPPQpNsV9Vpu"
   },
   "source": [
    "#### DocumentAssembler\n",
    "\n",
    "The DocumentAssembler takes five parameters (see table0314):\n",
    "\n",
    "* inputCol  \n",
    "The column containing the text of the document\n",
    "* outputCol  \n",
    "The name of the column containing the newly constructed document\n",
    "* idCol  \n",
    "The name of the column containing the identifier (optional)\n",
    "* metadataCol  \n",
    "The name of a Map-type column that represents document metadata (optional)\n",
    "* trimAndClearNewLines  \n",
    "Determines whether to remove new line characters and trim strings (optional, default = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Ek4VTEG9UQF"
   },
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler()\\\n",
    "    .setInputCol('text')\\\n",
    "    .setOutputCol('document')\\\n",
    "    .setIdCol('path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qaOFSlsT9etd"
   },
   "outputs": [],
   "source": [
    "docs = document_assembler.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "GsFEAbk19gLd",
    "outputId": "5fdfb6c0-0838-4bc6-801d-8e0ac39b9493"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.environmen...</td>\n",
       "      <td>[(document, 0, 1537, Xref: cantaloupe.srv.cs.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.misc:8162 ...</td>\n",
       "      <td>[(document, 0, 1141, Xref: cantaloupe.srv.cs.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[(document, 0, 1871, Newsgroups: sci.space\\nPa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[(document, 0, 754, Newsgroups: sci.space\\nPat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[(document, 0, 1676, Newsgroups: sci.space\\nPa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "1  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "2  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "3  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "4  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Xref: cantaloupe.srv.cs.cmu.edu sci.environmen...   \n",
       "1  Xref: cantaloupe.srv.cs.cmu.edu sci.misc:8162 ...   \n",
       "2  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "3  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "4  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "\n",
       "                                            document  \n",
       "0  [(document, 0, 1537, Xref: cantaloupe.srv.cs.c...  \n",
       "1  [(document, 0, 1141, Xref: cantaloupe.srv.cs.c...  \n",
       "2  [(document, 0, 1871, Newsgroups: sci.space\\nPa...  \n",
       "3  [(document, 0, 754, Newsgroups: sci.space\\nPat...  \n",
       "4  [(document, 0, 1676, Newsgroups: sci.space\\nPa...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "MF6edG1V9hP1",
    "outputId": "d31cb75f-f6eb-4833-c2af-e910bd669ae3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotatorType': 'document',\n",
       " 'begin': 0,\n",
       " 'end': 1537,\n",
       " 'result': \"Xref: cantaloupe.srv.cs.cmu.edu sci.environment:30751 misc.consumers:69257 misc.invest:42674 sci.astro:36052 talk.environment:12249 talk.politics.space:2873 sci.space:62480 rec.backcountry:32316 misc.rural:6339 misc.headlines:42268 misc.legal:63427\\nNewsgroups: sci.environment,misc.consumers,misc.invest,sci.astro,talk.environment,talk.politics.space,sci.space,rec.backcountry,misc.rural,misc.headlines,k12.chat.teacher,misc.legal\\nPath: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!europa.eng.gtefsd.com!fs7.ece.cmu.edu!fs7.ECE.CMU.EDU!loss\\nFrom: loss@fs7.ECE.CMU.EDU (Doug Loss)\\nSubject: Re: Space Marketing would be wonderfull.\\nMessage-ID: <C76BMs.8x5@fs7.ece.cmu.edu>\\nSender: news@fs7.ece.cmu.edu (USENET News System)\\nOrganization: Electrical and Computer Engineering, Carnegie Mellon\\nReferences: <C75A0p.8DK@news.cso.uiuc.edu> <pgf.737604987@srl03.cacs.usl.edu> <C760Dv.K75@agora.rain.com>\\nDate: Mon, 17 May 1993 13:47:13 GMT\\nLines: 17\\n\\nIn article <C760Dv.K75@agora.rain.com> jhart@agora.rain.com (Jim Hart) writes:\\n>\\n>[...]\\n>\\n>Astronomers have been using the night sky for thousands of years --\\n>they own it.  If they don't complain now against scenic trespassers\\n>(eg light polluters), they will lose their common-law right of ownership.\\n>\\n   Is English (American, Canadian, etc.) common law recognized as\\nlegally binding under international law?  After all, we're talking about\\nsomething that by its very nature isn't limited to the territory of one\\nnation.\\n\\nDoug Loss\\nloss@husky.bloomu.edu\\n\\n\\n\",\n",
       " 'metadata': {'sentence': '0',\n",
       "  'id': 'file:/Users/ywsung/Desktop/spark-nlp-book-master/jupyter/data/mini_newsgroups/sci.space/62480'},\n",
       " 'embeddings': []}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.first()['document'][0].asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X8jIEJFf9kvh"
   },
   "source": [
    "### Annotators\n",
    "\n",
    "Now we look at stage 3—the annotators. This is the heart of the NLP work. So let's look at some of the Annotators available in Spark NLP.\n",
    "\n",
    "We will look at some commonly used annotators:\n",
    "\n",
    "* `SentenceDetector`\n",
    "* `Tokenizer`\n",
    "* `Lemmatizer`\n",
    "* `PerceptronApproach` (POSTagger)\n",
    "\n",
    "#### SentenceDetector\n",
    "\n",
    "The SentenceDetector uses a rule-based algorithm inspired by Kevin Dias's Ruby implementation. It takes the following parameters\n",
    "\n",
    "* inputCols  \n",
    "A list of columns to sentence-tokenize.\n",
    "* outputCol  \n",
    "The name of the new sentence column.\n",
    "* useAbbrevations  \n",
    "Determines whether to apply abbreviations at sentence detection.\n",
    "* useCustomBoundsOnly  \n",
    "Determines whether to only utilize custom bounds for sentence detection.\n",
    "* explodeSentences  \n",
    "Determines whether to explode each sentence into a different row, for better parallelization. Defaults to false.\n",
    "* customBounds  \n",
    "Characters used to explicitly mark sentence bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWKNV7bV9ixl"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import SentenceDetector\n",
    "\n",
    "sent_detector = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "SdJ5Nq-c97xF",
    "outputId": "b4d27239-f63d-4d80-b28c-1294a2dab17d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>document</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.environmen...</td>\n",
       "      <td>[(document, 0, 1537, Xref: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 666, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.misc:8162 ...</td>\n",
       "      <td>[(document, 0, 1141, Xref: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 775, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[(document, 0, 1871, Newsgroups: sci.space\\nPa...</td>\n",
       "      <td>[(document, 0, 969, Newsgroups: sci.space\\nPat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[(document, 0, 754, Newsgroups: sci.space\\nPat...</td>\n",
       "      <td>[(document, 0, 681, Newsgroups: sci.space\\nPat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[(document, 0, 1676, Newsgroups: sci.space\\nPa...</td>\n",
       "      <td>[(document, 0, 316, Newsgroups: sci.space\\nPat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "1  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "2  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "3  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "4  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Xref: cantaloupe.srv.cs.cmu.edu sci.environmen...   \n",
       "1  Xref: cantaloupe.srv.cs.cmu.edu sci.misc:8162 ...   \n",
       "2  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "3  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "4  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "\n",
       "                                            document  \\\n",
       "0  [(document, 0, 1537, Xref: cantaloupe.srv.cs.c...   \n",
       "1  [(document, 0, 1141, Xref: cantaloupe.srv.cs.c...   \n",
       "2  [(document, 0, 1871, Newsgroups: sci.space\\nPa...   \n",
       "3  [(document, 0, 754, Newsgroups: sci.space\\nPat...   \n",
       "4  [(document, 0, 1676, Newsgroups: sci.space\\nPa...   \n",
       "\n",
       "                                           sentences  \n",
       "0  [(document, 0, 666, Xref: cantaloupe.srv.cs.cm...  \n",
       "1  [(document, 0, 775, Xref: cantaloupe.srv.cs.cm...  \n",
       "2  [(document, 0, 969, Newsgroups: sci.space\\nPat...  \n",
       "3  [(document, 0, 681, Newsgroups: sci.space\\nPat...  \n",
       "4  [(document, 0, 316, Newsgroups: sci.space\\nPat...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_detector.transform(docs)\n",
    "sentences.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "29gzaJ2Z995D"
   },
   "source": [
    "#### Tokenizer\n",
    "\n",
    "A Tokenizer is a fundamental Annotator. Almost all text-based data processing begins with some form of tokenization. Most classical NLP algorithms expect tokens as the basic input. Many deep learning algorithms are being developed that take characters as basic input. Most NLP applications still use tokenization. The Spark NLP Tokenizer is a little more sophisticated than just a regular expression-based tokenizer. It has a number of parameters. The following are some of the basic ones (see table0316 for the results):\n",
    "\n",
    "* inputCols  \n",
    "A list of columns to tokenize.\n",
    "* outputCol  \n",
    "The name of the new token column.\n",
    "* targetPattern  \n",
    "Basic regex rule to identify a candidate for tokenization. Defaults to \\S+ which means anything not a space (optional).\n",
    "* prefixPattern  \n",
    "Regular expression (regex) to identify subtokens that come in the beginning of the token. Regex has to start with \\A and must contain groups (). Each group will become a separate token within the prefix. Defaults to nonletter characters—for example, quotes or parentheses (optional).\n",
    "* suffixPattern  \n",
    "Regex to identify subtokens that are in the end of the token. Regex has to end with \\z and must contain groups (). Each group will become a separate token within the prefix. Defaults to nonletter characters—for example, quotes or parentheses (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QVupoF4n98wt"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols(['sentences'])\\\n",
    "    .setOutputCol('tokens')\\\n",
    "    .fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "gU3iypI_-JwE",
    "outputId": "078fed66-fb77-449e-f0f9-176e9784d36f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>document</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.environmen...</td>\n",
       "      <td>[(document, 0, 1537, Xref: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 666, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.misc:8162 ...</td>\n",
       "      <td>[(document, 0, 1141, Xref: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 775, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[(document, 0, 1871, Newsgroups: sci.space\\nPa...</td>\n",
       "      <td>[(document, 0, 969, Newsgroups: sci.space\\nPat...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0'}, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[(document, 0, 754, Newsgroups: sci.space\\nPat...</td>\n",
       "      <td>[(document, 0, 681, Newsgroups: sci.space\\nPat...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0'}, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[(document, 0, 1676, Newsgroups: sci.space\\nPa...</td>\n",
       "      <td>[(document, 0, 316, Newsgroups: sci.space\\nPat...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0'}, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "1  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "2  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "3  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "4  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Xref: cantaloupe.srv.cs.cmu.edu sci.environmen...   \n",
       "1  Xref: cantaloupe.srv.cs.cmu.edu sci.misc:8162 ...   \n",
       "2  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "3  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "4  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "\n",
       "                                            document  \\\n",
       "0  [(document, 0, 1537, Xref: cantaloupe.srv.cs.c...   \n",
       "1  [(document, 0, 1141, Xref: cantaloupe.srv.cs.c...   \n",
       "2  [(document, 0, 1871, Newsgroups: sci.space\\nPa...   \n",
       "3  [(document, 0, 754, Newsgroups: sci.space\\nPat...   \n",
       "4  [(document, 0, 1676, Newsgroups: sci.space\\nPa...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [(document, 0, 666, Xref: cantaloupe.srv.cs.cm...   \n",
       "1  [(document, 0, 775, Xref: cantaloupe.srv.cs.cm...   \n",
       "2  [(document, 0, 969, Newsgroups: sci.space\\nPat...   \n",
       "3  [(document, 0, 681, Newsgroups: sci.space\\nPat...   \n",
       "4  [(document, 0, 316, Newsgroups: sci.space\\nPat...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...  \n",
       "1  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...  \n",
       "2  [(token, 0, 9, Newsgroups, {'sentence': '0'}, ...  \n",
       "3  [(token, 0, 9, Newsgroups, {'sentence': '0'}, ...  \n",
       "4  [(token, 0, 9, Newsgroups, {'sentence': '0'}, ...  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.transform(sentences)\n",
    "tokens.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zbowbqm6-Lwk"
   },
   "source": [
    "There are some `Annotators` that require additional resources. Some require reference data, like the following example, the lemmatizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "khm7EB4C-QM-"
   },
   "source": [
    "#### Lemmatizer\n",
    "\n",
    "The lemmatizer finds the lemmas for the tokens. Lemmas are the entry words in dictionaries. For example, \"cats\" lemmatizes to \"cat,\" and \"oxen\" lemmatizes to \"ox.\" Loading the lemmatizer requires a dictionary and the following three parameters:\n",
    "\n",
    "* inputCols  \n",
    "A list of columns to tokenize\n",
    "* outputCol  \n",
    "The name of the new token column\n",
    "* dictionary  \n",
    "The resource to be loaded as the lemma dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0eNjBa0-Kxk"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import Lemmatizer\n",
    "\n",
    "lemmatizer = Lemmatizer() \\\n",
    "  .setInputCols([\"tokens\"]) \\\n",
    "  .setOutputCol(\"lemma\") \\\n",
    "  .setDictionary('../en_lemmas.txt', '\\t', ',')\\\n",
    "  .fit(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "jH-klgbV-Wpz",
    "outputId": "c56166aa-846f-4c47-b204-5b8b451d92f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>document</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.environmen...</td>\n",
       "      <td>[(document, 0, 1537, Xref: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 666, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.misc:8162 ...</td>\n",
       "      <td>[(document, 0, 1141, Xref: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 775, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[(document, 0, 1871, Newsgroups: sci.space\\nPa...</td>\n",
       "      <td>[(document, 0, 969, Newsgroups: sci.space\\nPat...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0'}, ...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0'}, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[(document, 0, 754, Newsgroups: sci.space\\nPat...</td>\n",
       "      <td>[(document, 0, 681, Newsgroups: sci.space\\nPat...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0'}, ...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0'}, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[(document, 0, 1676, Newsgroups: sci.space\\nPa...</td>\n",
       "      <td>[(document, 0, 316, Newsgroups: sci.space\\nPat...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0'}, ...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0'}, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "1  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "2  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "3  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "4  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Xref: cantaloupe.srv.cs.cmu.edu sci.environmen...   \n",
       "1  Xref: cantaloupe.srv.cs.cmu.edu sci.misc:8162 ...   \n",
       "2  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "3  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "4  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "\n",
       "                                            document  \\\n",
       "0  [(document, 0, 1537, Xref: cantaloupe.srv.cs.c...   \n",
       "1  [(document, 0, 1141, Xref: cantaloupe.srv.cs.c...   \n",
       "2  [(document, 0, 1871, Newsgroups: sci.space\\nPa...   \n",
       "3  [(document, 0, 754, Newsgroups: sci.space\\nPat...   \n",
       "4  [(document, 0, 1676, Newsgroups: sci.space\\nPa...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [(document, 0, 666, Xref: cantaloupe.srv.cs.cm...   \n",
       "1  [(document, 0, 775, Xref: cantaloupe.srv.cs.cm...   \n",
       "2  [(document, 0, 969, Newsgroups: sci.space\\nPat...   \n",
       "3  [(document, 0, 681, Newsgroups: sci.space\\nPat...   \n",
       "4  [(document, 0, 316, Newsgroups: sci.space\\nPat...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...   \n",
       "1  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...   \n",
       "2  [(token, 0, 9, Newsgroups, {'sentence': '0'}, ...   \n",
       "3  [(token, 0, 9, Newsgroups, {'sentence': '0'}, ...   \n",
       "4  [(token, 0, 9, Newsgroups, {'sentence': '0'}, ...   \n",
       "\n",
       "                                               lemma  \n",
       "0  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...  \n",
       "1  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...  \n",
       "2  [(token, 0, 9, Newsgroups, {'sentence': '0'}, ...  \n",
       "3  [(token, 0, 9, Newsgroups, {'sentence': '0'}, ...  \n",
       "4  [(token, 0, 9, Newsgroups, {'sentence': '0'}, ...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas = lemmatizer.transform(tokens)\n",
    "lemmas.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-tfE16_Q_Bmy"
   },
   "source": [
    "#### POS tagger\n",
    "\n",
    "There are also Annotators that require models as resources. For example, the POS tagger uses a perceptron model, so it is called PerceptronApproach. The PerceptronApproach has five parameters:\n",
    "\n",
    "* inputCols  \n",
    "A list of columns to tag\n",
    "* outputCol  \n",
    "The name of the new tag column\n",
    "* posCol  \n",
    "Column of Array of POS tags that match tokens\n",
    "* corpus  \n",
    "POS tags delimited corpus; needs \"delimiter\" in options\n",
    "* nIterations  \n",
    "Number of iterations in training, converges to better accuracy\n",
    "\n",
    "We will load a pretrained model here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5Y3jCTf-_0n"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import PerceptronModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "JUd68mQ1_N_y",
    "outputId": "be1abe00-3b09-4467-ab6a-11c6534e265d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_anc download started this may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/20 21:36:01 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n",
      "23/08/20 21:36:01 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n",
      "23/08/20 21:36:01 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n",
      "23/08/20 21:36:01 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n",
      "23/08/20 21:36:01 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n",
      "23/08/20 21:36:01 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n",
      "23/08/20 21:36:01 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n",
      "23/08/20 21:36:01 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate size to download 3.9 MB\n",
      "[ / ]pos_anc download started this may take some time.\n",
      "Approximate size to download 3.9 MB\n",
      "[ — ]Download done! Loading the resource.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "pos_tagger = PerceptronModel.pretrained() \\\n",
    "  .setInputCols([\"tokens\", \"sentences\"]) \\\n",
    "  .setOutputCol(\"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "lCFlMYyD_PBL",
    "outputId": "a4410bba-a387-490d-bf20-5979c38e0142"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>document</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.environmen...</td>\n",
       "      <td>[(document, 0, 1537, Xref: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 666, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(pos, 0, 3, NNP, {'sentence': '0', 'word': 'X...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.misc:8162 ...</td>\n",
       "      <td>[(document, 0, 1141, Xref: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 775, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(pos, 0, 3, NNP, {'sentence': '0', 'word': 'X...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[(document, 0, 1871, Newsgroups: sci.space\\nPa...</td>\n",
       "      <td>[(document, 0, 969, Newsgroups: sci.space\\nPat...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0'}, ...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0'}, ...</td>\n",
       "      <td>[(pos, 0, 9, NNP, {'sentence': '0', 'word': 'N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[(document, 0, 754, Newsgroups: sci.space\\nPat...</td>\n",
       "      <td>[(document, 0, 681, Newsgroups: sci.space\\nPat...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0'}, ...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0'}, ...</td>\n",
       "      <td>[(pos, 0, 9, NNP, {'sentence': '0', 'word': 'N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[(document, 0, 1676, Newsgroups: sci.space\\nPa...</td>\n",
       "      <td>[(document, 0, 316, Newsgroups: sci.space\\nPat...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0'}, ...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0'}, ...</td>\n",
       "      <td>[(pos, 0, 9, NNP, {'sentence': '0', 'word': 'N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "1  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "2  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "3  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "4  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Xref: cantaloupe.srv.cs.cmu.edu sci.environmen...   \n",
       "1  Xref: cantaloupe.srv.cs.cmu.edu sci.misc:8162 ...   \n",
       "2  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "3  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "4  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "\n",
       "                                            document  \\\n",
       "0  [(document, 0, 1537, Xref: cantaloupe.srv.cs.c...   \n",
       "1  [(document, 0, 1141, Xref: cantaloupe.srv.cs.c...   \n",
       "2  [(document, 0, 1871, Newsgroups: sci.space\\nPa...   \n",
       "3  [(document, 0, 754, Newsgroups: sci.space\\nPat...   \n",
       "4  [(document, 0, 1676, Newsgroups: sci.space\\nPa...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [(document, 0, 666, Xref: cantaloupe.srv.cs.cm...   \n",
       "1  [(document, 0, 775, Xref: cantaloupe.srv.cs.cm...   \n",
       "2  [(document, 0, 969, Newsgroups: sci.space\\nPat...   \n",
       "3  [(document, 0, 681, Newsgroups: sci.space\\nPat...   \n",
       "4  [(document, 0, 316, Newsgroups: sci.space\\nPat...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...   \n",
       "1  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...   \n",
       "2  [(token, 0, 9, Newsgroups, {'sentence': '0'}, ...   \n",
       "3  [(token, 0, 9, Newsgroups, {'sentence': '0'}, ...   \n",
       "4  [(token, 0, 9, Newsgroups, {'sentence': '0'}, ...   \n",
       "\n",
       "                                               lemma  \\\n",
       "0  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...   \n",
       "1  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...   \n",
       "2  [(token, 0, 9, Newsgroups, {'sentence': '0'}, ...   \n",
       "3  [(token, 0, 9, Newsgroups, {'sentence': '0'}, ...   \n",
       "4  [(token, 0, 9, Newsgroups, {'sentence': '0'}, ...   \n",
       "\n",
       "                                                 pos  \n",
       "0  [(pos, 0, 3, NNP, {'sentence': '0', 'word': 'X...  \n",
       "1  [(pos, 0, 3, NNP, {'sentence': '0', 'word': 'X...  \n",
       "2  [(pos, 0, 9, NNP, {'sentence': '0', 'word': 'N...  \n",
       "3  [(pos, 0, 9, NNP, {'sentence': '0', 'word': 'N...  \n",
       "4  [(pos, 0, 9, NNP, {'sentence': '0', 'word': 'N...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postags = pos_tagger.transform(lemmas)\n",
    "postags.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lAAqz_pO_TLX"
   },
   "source": [
    "### Pretrained Pipelines\n",
    "\n",
    "We saw earlier how we can organize multiple MLlib stages into a Pipeline. Using Pipelines is especially useful in NLP tasks because there are often many stages between loading the raw text and extracting structured data.\n",
    "\n",
    "Spark NLP has pretrained pipelines that can be used to process text. This doesn't mean that you do not need to tune pipelines for application. But it is often convenient to begin experimenting with a prebuilt NLP pipeline and find what needs tuning.\n",
    "\n",
    "#### Explain document ML pipeline\n",
    "\n",
    "The BasicPipeline does sentence splitting, tokenization, lemmatization, stemming, and POS tagging. If you want to get a quick look at some text data, this is a great pipeline to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "H8Yt5ByW_QVr",
    "outputId": "c83932e5-f4e8-4bf8-bf78-1b0310bf51eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_ml download started this may take some time.\n",
      "Approx size to download 9 MB\n",
      "[ | ]explain_document_ml download started this may take some time.\n",
      "Approximate size to download 9 MB\n",
      "[ — ]Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "pipeline = PretrainedPipeline('explain_document_ml', lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "aKC7kAht_dib",
    "outputId": "0e7c2ee3-1a84-4d6c-854a-ccabdc1b360e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>document</th>\n",
       "      <th>sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>spell</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>stems</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.environmen...</td>\n",
       "      <td>[(document, 0, 1537, Xref: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 666, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, pref, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(token, 0, 3, pref, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(token, 0, 3, pref, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(pos, 0, 3, NN, {'sentence': '0', 'word': 'pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.misc:8162 ...</td>\n",
       "      <td>[(document, 0, 1141, Xref: cantaloupe.srv.cs.c...</td>\n",
       "      <td>[(document, 0, 775, Xref: cantaloupe.srv.cs.cm...</td>\n",
       "      <td>[(token, 0, 3, Xref, {'sentence': '0'}, []), (...</td>\n",
       "      <td>[(token, 0, 3, tref, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(token, 0, 3, tref, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(token, 0, 3, tref, {'sentence': '0', 'confid...</td>\n",
       "      <td>[(pos, 0, 3, NN, {'sentence': '0', 'word': 'tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[(document, 0, 1871, Newsgroups: sci.space\\nPa...</td>\n",
       "      <td>[(document, 0, 969, Newsgroups: sci.space\\nPat...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0'}, ...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0', '...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0', '...</td>\n",
       "      <td>[(token, 0, 9, newsgroup, {'sentence': '0', 'c...</td>\n",
       "      <td>[(pos, 0, 9, NNP, {'sentence': '0', 'word': 'N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[(document, 0, 754, Newsgroups: sci.space\\nPat...</td>\n",
       "      <td>[(document, 0, 681, Newsgroups: sci.space\\nPat...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0'}, ...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0', '...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0', '...</td>\n",
       "      <td>[(token, 0, 9, newsgroup, {'sentence': '0', 'c...</td>\n",
       "      <td>[(pos, 0, 9, NNP, {'sentence': '0', 'word': 'N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[(document, 0, 1676, Newsgroups: sci.space\\nPa...</td>\n",
       "      <td>[(document, 0, 316, Newsgroups: sci.space\\nPat...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0'}, ...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0', '...</td>\n",
       "      <td>[(token, 0, 9, Newsgroups, {'sentence': '0', '...</td>\n",
       "      <td>[(token, 0, 9, newsgroup, {'sentence': '0', 'c...</td>\n",
       "      <td>[(pos, 0, 9, NNP, {'sentence': '0', 'word': 'N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "1  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "2  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "3  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "4  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Xref: cantaloupe.srv.cs.cmu.edu sci.environmen...   \n",
       "1  Xref: cantaloupe.srv.cs.cmu.edu sci.misc:8162 ...   \n",
       "2  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "3  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "4  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "\n",
       "                                            document  \\\n",
       "0  [(document, 0, 1537, Xref: cantaloupe.srv.cs.c...   \n",
       "1  [(document, 0, 1141, Xref: cantaloupe.srv.cs.c...   \n",
       "2  [(document, 0, 1871, Newsgroups: sci.space\\nPa...   \n",
       "3  [(document, 0, 754, Newsgroups: sci.space\\nPat...   \n",
       "4  [(document, 0, 1676, Newsgroups: sci.space\\nPa...   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  [(document, 0, 666, Xref: cantaloupe.srv.cs.cm...   \n",
       "1  [(document, 0, 775, Xref: cantaloupe.srv.cs.cm...   \n",
       "2  [(document, 0, 969, Newsgroups: sci.space\\nPat...   \n",
       "3  [(document, 0, 681, Newsgroups: sci.space\\nPat...   \n",
       "4  [(document, 0, 316, Newsgroups: sci.space\\nPat...   \n",
       "\n",
       "                                               token  \\\n",
       "0  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...   \n",
       "1  [(token, 0, 3, Xref, {'sentence': '0'}, []), (...   \n",
       "2  [(token, 0, 9, Newsgroups, {'sentence': '0'}, ...   \n",
       "3  [(token, 0, 9, Newsgroups, {'sentence': '0'}, ...   \n",
       "4  [(token, 0, 9, Newsgroups, {'sentence': '0'}, ...   \n",
       "\n",
       "                                               spell  \\\n",
       "0  [(token, 0, 3, pref, {'sentence': '0', 'confid...   \n",
       "1  [(token, 0, 3, tref, {'sentence': '0', 'confid...   \n",
       "2  [(token, 0, 9, Newsgroups, {'sentence': '0', '...   \n",
       "3  [(token, 0, 9, Newsgroups, {'sentence': '0', '...   \n",
       "4  [(token, 0, 9, Newsgroups, {'sentence': '0', '...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  [(token, 0, 3, pref, {'sentence': '0', 'confid...   \n",
       "1  [(token, 0, 3, tref, {'sentence': '0', 'confid...   \n",
       "2  [(token, 0, 9, Newsgroups, {'sentence': '0', '...   \n",
       "3  [(token, 0, 9, Newsgroups, {'sentence': '0', '...   \n",
       "4  [(token, 0, 9, Newsgroups, {'sentence': '0', '...   \n",
       "\n",
       "                                               stems  \\\n",
       "0  [(token, 0, 3, pref, {'sentence': '0', 'confid...   \n",
       "1  [(token, 0, 3, tref, {'sentence': '0', 'confid...   \n",
       "2  [(token, 0, 9, newsgroup, {'sentence': '0', 'c...   \n",
       "3  [(token, 0, 9, newsgroup, {'sentence': '0', 'c...   \n",
       "4  [(token, 0, 9, newsgroup, {'sentence': '0', 'c...   \n",
       "\n",
       "                                                 pos  \n",
       "0  [(pos, 0, 3, NN, {'sentence': '0', 'word': 'pr...  \n",
       "1  [(pos, 0, 3, NN, {'sentence': '0', 'word': 'tr...  \n",
       "2  [(pos, 0, 9, NNP, {'sentence': '0', 'word': 'N...  \n",
       "3  [(pos, 0, 9, NNP, {'sentence': '0', 'word': 'N...  \n",
       "4  [(pos, 0, 9, NNP, {'sentence': '0', 'word': 'N...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.transform(texts).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o82tgVLv_ees"
   },
   "outputs": [],
   "source": [
    "text = texts.first()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "id": "fCvfeIzy_g8D",
    "outputId": "6df32850-cf6a-4476-eb80-675926364268"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('own', 'own', 'own'),\n",
       " ('it', 'it', 'it'),\n",
       " ('.', '.', '.'),\n",
       " ('If', 'if', 'If'),\n",
       " ('they', 'thei', 'they'),\n",
       " (\"don't\", \"don't\", \"don't\"),\n",
       " ('complain', 'complain', 'complain'),\n",
       " ('now', 'now', 'now'),\n",
       " ('against', 'against', 'against'),\n",
       " ('scenic', 'scenic', 'scenic'),\n",
       " ('trespassers', 'trespass', 'trespasser'),\n",
       " ('>(eg', '>(eg', '>(eg'),\n",
       " ('light', 'light', 'light'),\n",
       " ('polluters', 'pollut', 'polluter'),\n",
       " ('),', '),', '),'),\n",
       " ('they', 'thei', 'they'),\n",
       " ('will', 'will', 'will'),\n",
       " ('lose', 'lose', 'lose'),\n",
       " ('their', 'their', 'they'),\n",
       " ('common-law', 'common-law', 'common-law')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations = pipeline.annotate(text)\n",
    "list(zip(\n",
    "    annotations['token'], \n",
    "    annotations['stems'], \n",
    "    annotations['lemmas']\n",
    "))[100:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNJkHpxA_jp8"
   },
   "source": [
    "There are many other pipelines, and there is additional information [available](https://nlp.johnsnowlabs.com/docs/en/pipelines).\n",
    "\n",
    "Now let's talk about how we will perform step 4, converting the annotations into native Spark SQL types using the Finisher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIm6p2iw_qM_"
   },
   "source": [
    "### Finisher\n",
    "\n",
    "The annotations are useful for composing NLP steps, but we generally want to take some specific information out to process. The `Finisher` handles most of these use cases. If you want to get a list of tokens (or stems, or what have you) to use in downstream MLlib stages, the `Finisher` can do this (see table0320). Let's look at the parameters:\n",
    "\n",
    "* inputCols  \n",
    "Name of input annotation cols\n",
    "* outputCols  \n",
    "Name of finisher output cols\n",
    "* valueSplitSymbol  \n",
    "Character separating annotations\n",
    "* annotationSplitSymbol  \n",
    "Character separating annotations\n",
    "* cleanAnnotations  \n",
    "Determines whether to remove annotation columns\n",
    "* includeMetadata  \n",
    "Annotation metadata format\n",
    "* outputAsArray  \n",
    "Finisher generates an Array with the results instead of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0OGNNL4_iKC"
   },
   "outputs": [],
   "source": [
    "finisher = Finisher()\\\n",
    "    .setInputCols(['tokens', 'lemma'])\\\n",
    "    .setOutputCols(['tokens', 'lemmata'])\\\n",
    "    .setCleanAnnotations(True)\\\n",
    "    .setOutputAsArray(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tJTzbG5_2na"
   },
   "outputs": [],
   "source": [
    "custom_pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sent_detector,\n",
    "    tokenizer,\n",
    "    lemmatizer,\n",
    "    finisher\n",
    "]).fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "jq23jmJi_3mq",
    "outputId": "491ff008-9fb0-40f8-a8e4-df463f5b6dc5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.environmen...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.envir...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.envir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.misc:8162 ...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.misc:...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.misc:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[Newsgroups, :, sci.space, Path, :, cantaloupe...</td>\n",
       "      <td>[Newsgroups, :, sci.space, Path, :, cantaloupe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[Newsgroups, :, sci.space, Path, :, cantaloupe...</td>\n",
       "      <td>[Newsgroups, :, sci.space, Path, :, cantaloupe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[Newsgroups, :, sci.space, Path, :, cantaloupe...</td>\n",
       "      <td>[Newsgroups, :, sci.space, Path, :, cantaloupe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "1  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "2  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "3  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "4  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Xref: cantaloupe.srv.cs.cmu.edu sci.environmen...   \n",
       "1  Xref: cantaloupe.srv.cs.cmu.edu sci.misc:8162 ...   \n",
       "2  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "3  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "4  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.envir...   \n",
       "1  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.misc:...   \n",
       "2  [Newsgroups, :, sci.space, Path, :, cantaloupe...   \n",
       "3  [Newsgroups, :, sci.space, Path, :, cantaloupe...   \n",
       "4  [Newsgroups, :, sci.space, Path, :, cantaloupe...   \n",
       "\n",
       "                                             lemmata  \n",
       "0  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.envir...  \n",
       "1  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.misc:...  \n",
       "2  [Newsgroups, :, sci.space, Path, :, cantaloupe...  \n",
       "3  [Newsgroups, :, sci.space, Path, :, cantaloupe...  \n",
       "4  [Newsgroups, :, sci.space, Path, :, cantaloupe...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_pipeline.transform(texts).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sp02kNkt_40K"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jRnUE19R_6Nq"
   },
   "outputs": [],
   "source": [
    "stopwords = StopWordsRemover.loadDefaultStopWords('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8l61JlZ_7ZC"
   },
   "outputs": [],
   "source": [
    "larger_pipeline = Pipeline(stages=[\n",
    "    custom_pipeline,\n",
    "    StopWordsRemover(\n",
    "        inputCol='lemmata', \n",
    "        outputCol='terms', \n",
    "        stopWords=stopwords)\n",
    "]).fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "PhPy7oyi_80K",
    "outputId": "0757436a-4324-4c81-c1ae-af7f1125915f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmata</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.environmen...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.envir...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.envir...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.envir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu sci.misc:8162 ...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.misc:...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.misc:...</td>\n",
       "      <td>[Xref, :, cantaloupe.srv.cs.cmu.edu, sci.misc:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[Newsgroups, :, sci.space, Path, :, cantaloupe...</td>\n",
       "      <td>[Newsgroups, :, sci.space, Path, :, cantaloupe...</td>\n",
       "      <td>[Newsgroups, :, sci.space, Path, :, cantaloupe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[Newsgroups, :, sci.space, Path, :, cantaloupe...</td>\n",
       "      <td>[Newsgroups, :, sci.space, Path, :, cantaloupe...</td>\n",
       "      <td>[Newsgroups, :, sci.space, Path, :, cantaloupe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/Users/ywsung/Desktop/spark-nlp-book-mast...</td>\n",
       "      <td>Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...</td>\n",
       "      <td>[Newsgroups, :, sci.space, Path, :, cantaloupe...</td>\n",
       "      <td>[Newsgroups, :, sci.space, Path, :, cantaloupe...</td>\n",
       "      <td>[Newsgroups, :, sci.space, Path, :, cantaloupe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "1  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "2  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "3  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "4  file:/Users/ywsung/Desktop/spark-nlp-book-mast...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Xref: cantaloupe.srv.cs.cmu.edu sci.environmen...   \n",
       "1  Xref: cantaloupe.srv.cs.cmu.edu sci.misc:8162 ...   \n",
       "2  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "3  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "4  Newsgroups: sci.space\\nPath: cantaloupe.srv.cs...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.envir...   \n",
       "1  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.misc:...   \n",
       "2  [Newsgroups, :, sci.space, Path, :, cantaloupe...   \n",
       "3  [Newsgroups, :, sci.space, Path, :, cantaloupe...   \n",
       "4  [Newsgroups, :, sci.space, Path, :, cantaloupe...   \n",
       "\n",
       "                                             lemmata  \\\n",
       "0  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.envir...   \n",
       "1  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.misc:...   \n",
       "2  [Newsgroups, :, sci.space, Path, :, cantaloupe...   \n",
       "3  [Newsgroups, :, sci.space, Path, :, cantaloupe...   \n",
       "4  [Newsgroups, :, sci.space, Path, :, cantaloupe...   \n",
       "\n",
       "                                               terms  \n",
       "0  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.envir...  \n",
       "1  [Xref, :, cantaloupe.srv.cs.cmu.edu, sci.misc:...  \n",
       "2  [Newsgroups, :, sci.space, Path, :, cantaloupe...  \n",
       "3  [Newsgroups, :, sci.space, Path, :, cantaloupe...  \n",
       "4  [Newsgroups, :, sci.space, Path, :, cantaloupe...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "larger_pipeline.transform(texts).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fste5GTS__PC"
   },
   "source": [
    "Now that we have reviewed Spark and Spark NLP, we are almost ready to start building an NLP application. There is an extra benefit to learning an annotation library—it helps you understand how to structure a pipeline for NLP. This knowledge will be applicable even if you are using other technologies.\n",
    "\n",
    "The only topic left for us to cover is deep learning, which we cover in the next chapter.  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPyFaFiU0BTzUWEmuo57HPB",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "1.3_NLP_on_Apache_Spark.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
