{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.4_Deep_Learning_Basics.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOrULl3eRHKC1SBGg8yRNIO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexander-n-thomas/spark-nlp-book-prod/blob/master/1_4_Deep_Learning_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsDg2y5KBJUs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "2a9c019e-9495-4050-d54e-8059e640c9a5"
      },
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed pyspark==2.4.4\n",
        "\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed spark-nlp==2.5.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_252\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~18.04-b09)\n",
            "OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\n",
            "Collecting pyspark==2.4.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 56kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 41.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130388 sha256=20f6389e3f52d331fcb4d9620d300aeae4e5f6160c6505c2e3bc299c89b9e181\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.4\n",
            "Collecting spark-nlp==2.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/b4/db653f8080a446de8ce981b262d85c85c61de7e920930726da0d1c6b4c65/spark_nlp-2.5.1-py2.py3-none-any.whl (121kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piOZeCIk4PVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mkdir -p data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAMhZwEW4M3A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "95edd9b5-c566-41db-e497-334d2c42da04"
      },
      "source": [
        "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-23 13:33:42--  https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4551 (4.4K) [application/x-httpd-php]\n",
            "Saving to: ‘iris.data’\n",
            "\n",
            "\riris.data             0%[                    ]       0  --.-KB/s               \riris.data           100%[===================>]   4.44K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-07-23 13:33:42 (84.2 MB/s) - ‘iris.data’ saved [4551/4551]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8N8Uqqj4MnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mv iris.data ./data"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZYWj6F3A-Sb",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning Basics\n",
        "  In this chapter we will cover the basics of deep learning. The goal of this chapter is to create a foundation for us to discuss how to apply deep learning to NLP. There are new deep learning techniques being developed every month, and we will cover some of the newer techniques in later chapters, which is why we need this foundation. In the beginning of this chapter we will cover some of the history of the artificial neural network, and we will work through some example networks representing logical operators. This will help us build a solid foundation for thinking about artificial neural networks.\n",
        "\n",
        "  Fundamentally, deep learning is a field of study of artificial neural networks, or ANNs. The first appearance of artificial neural networks in academic literature was in a paper called A Logical Calculus of the Ideas Immanent in Nervous Activity, by   Warren S. McCulloch and Walter Pitts in 1943. Their work was an attempt to explain how the brain worked from a cyberneticist perspective. Their work would become the root of modern neuroscience and modern artificial neural networks.\n",
        "\n",
        "An ANN is a biologically inspired algorithm. ANNs are not realistic representations of how a brain learns, although from time to time news stories still hype this. We are still learning many things about how the brain processes information. As new discoveries are made, there is often an attempt to represent real neurological structures and processes in terms of ANNs, like the concept of receptive fields inspiring convolutional neural networks. Despite this, it cannot be overstated how far we are from building an artificial brain.\n",
        "\n",
        "  In 1957,  Frank Rosenblatt created the perceptron algorithm. Initially, there were high hopes about the perceptron. When evaluating, the single layer perceptron does the following:\n",
        "\n",
        "1. $n$ inputs, $x_1,...,x_n$\n",
        "2. Each input is multiplied by a weight, $x_i w_i$\n",
        "3. These products are then summed with the bias term, $s=b+\\sum^{n}_{i=1}{x_i w_i}$\n",
        "4. This sum is then run through an activation function, which returns 0 or 1, $\\hat{y}=f(s)$\n",
        "  * The Heaviside step function, $H$, is often used\n",
        "\n",
        "\\begin{equation}\n",
        "H(x) := \n",
        "\\begin{cases} \n",
        "0, & \\text{if}\\ x<0 \\\\ \n",
        "1, & \\text{if}\\ x>0 \n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "This can also be expressed through linear algebra.\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\vec{x} &= <x_1, ..., x_n>\\\\ \n",
        "\\vec{w} &= <w_1, ..., w_n>\\\\ \n",
        "y &= H(\\vec{x} \\cdot \\vec{w}+ b) \n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "This can be visualized with the diagram below\n",
        "\n",
        "![Perceptron](https://i.imgur.com/zOxDHyZ.png)  \n",
        "_Perceptron_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTQS7FsYA-PF",
        "colab_type": "text"
      },
      "source": [
        "In 1969, Marvin Minsky and Seymour Papert showed the limitations of the algorithm. The perceptron could not represent the exclusive \"or\" operator XOR. The  difficulty here is that a simple perceptron cannot solve problems that do not have linear separability. In terms of binary classification, a linearly separable problem is one in which the two classes can be separated by a single line, or plane in higher dimensions. To better understand this in terms of neural networks, let's look at some examples.\n",
        "\n",
        "We will try and create some perceptrons representing logical functions by hand, to explore the XOR problem. Imagine that we want to train networks to perform some basic logical functions. The inputs will be 0s and 1s.\n",
        "\n",
        "If we want to implement the NOT operator, what would we do? In this case, there is no . We want the following function:\n",
        "\n",
        "\\begin{equation}\n",
        "NOT(x) := \n",
        "\\begin{cases} \n",
        "0, & \\text{if}\\ x=1 \\\\ \n",
        "1, & \\text{if}\\ x=0 \n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "This gives us two equations to work with.\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned} \n",
        "H(0 \\cdot w_1 + b) &= 1 \\\\\n",
        "H(1 \\cdot w_1 + b) &= 0 \n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "So let's see if we can find values that satisfy these equations.\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned} \n",
        "H(0 \\cdot w_1 + b) &= 1\\\\ \n",
        "0 \\cdot w_1 + b &> 0\\\\ \n",
        "b &> 0\\\\ \n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "So we know b must be positive.\n",
        "\n",
        "\\begin{aligned} \n",
        "H(1 \\cdot w_1 + b) &= 0\\\\ \n",
        "1 \\cdot w_1 + b &< 0\\\\ \n",
        "w_1 &< -b \n",
        "\\end{aligned}\n",
        "\n",
        "So  must be a negative number less than . An infinite number of values fit this, so the perceptron can easily represent NOT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y56AcEP5DS97",
        "colab_type": "text"
      },
      "source": [
        "Now let's represent the OR operator. This requires two inputs. We want the following function:\n",
        "\n",
        "\\begin{equation}\n",
        "OR(x_1, x_2) := \n",
        "\\begin{cases} \n",
        "1, & \\text{if}\\ x_1=1, x_2=1 \\\\ \n",
        "1, & \\text{if}\\ x_1=1, x_2=0 \\\\ \n",
        "1, & \\text{if}\\ x_1=0, x_2=1 \\\\ \n",
        "0, & \\text{if}\\ x_1=0, x_2=0 \\\\ \n",
        "\\end{cases} \n",
        "\\end{equation}\n",
        "\n",
        "We have a few more equations here; let's start with the last case.\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned} \n",
        "H(0 \\cdot w_1 + 0 \\cdot w_2 + b) &= 0\\\\ \n",
        "0 \\cdot w_1 + 0 \\cdot w_2 + b &< 0\\\\ \n",
        "b &< 0 \n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "So  must be negative. Now let's handle the second case.\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned} \n",
        "H(1 \\cdot w_1 + 0 \\cdot w_2 + b) &= 1\\\\ \n",
        "1 \\cdot w_1 + 0 \\cdot w_2 + b &> 0\\\\ \n",
        "w_1 &> -b \n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "So  must be larger than , and so it is a positive number. The same will work for case 3. For case 1, if  and  then . So again, there are an infinite number of values. A perceptron can represent OR."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U-c2IKTA-Ls",
        "colab_type": "text"
      },
      "source": [
        "Let's look at XOR now.\n",
        "\n",
        "\\begin{equation}\n",
        "XOR(x_1, x_2) := \n",
        "\\begin{cases} \n",
        "0, & \\text{if}\\ x_1=1, x_2=1 \\\\ \n",
        "1, & \\text{if}\\ x_1=1, x_2=0 \\\\ \n",
        "1, & \\text{if}\\ x_1=0, x_2=1 \\\\ \n",
        "0, & \\text{if}\\ x_1=0, x_2=0 \\\\ \n",
        "\\end{cases} \n",
        "\\end{equation}\n",
        "\n",
        "So we have four equations:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned} \n",
        "H(1 \\cdot w_1 + 1 \\cdot w_2 + b) &= 0\\\\ \n",
        "H(1 \\cdot w_1 + 0 \\cdot w_2 + b) &= 1\\\\ \n",
        "H(0 \\cdot w_1 + 1 \\cdot w_2 + b) &= 1\\\\ \n",
        "H(0 \\cdot w_1 + 0 \\cdot w_2 + b) &= 0\\\\ \n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "Cases 2 to 4 are the same as for OR, so this implies the following:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned} \n",
        "b &< 0\\\\ \n",
        "w_1 &> -b\\\\ \n",
        "w_2 &> -b\\\\ \n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "However, when we look at case 1, it falls apart. We cannot add the two weights, either of which are larger than  to  and get a negative number. So XOR is not representable with the perceptron. In fact, the perceptron can solve only linearly separable classification problems. Linearly separable problems are problems that can be solved by drawing a single line (or plane for higher dimensions). XOR is not linearly separable.\n",
        "\n",
        "However, this problem can be solved by having multiple layers, but this was difficult given the computational capability of the time. The limitations of the single-layer perceptron network caused research to turn toward other machine-learning approaches. In the 1980s there was renewed interest when hardware made multilayer perceptron networks more feasible\n",
        "\n",
        "![Multilayer perceptron](https://i.imgur.com/ujWjFk4.png)  \n",
        "_Multilayer perceptron_\n",
        "\n",
        "Now that we are dealing with modern neural networks there are some more options for us to consider:\n",
        "\n",
        "1. The output is not necessarily 0 or 1. It could be real valued, or even a vector of values.\n",
        "2. There are several activation functions to choose from.\n",
        "3. Now that we have hidden layers, we will have a matrix of weights between each layer.\n",
        "\n",
        "Look at how we would calculate the output for a neural network with one hidden layer:\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{y} = g(W^{(2)} \\cdot f(W^{(1)} \\cdot \\vec{x} + \\vec{b}^{(1)}) + \\vec{b}^{(2)})\n",
        "\\end{equation}\n",
        "\n",
        "We could repeat this for many layers if we wish. And now that we have hidden layers, we're going to have a lot more parameters—so solving for them by hand won't do. We are going to need to talk about gradient descent and backpropagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHT7fOouA-Ie",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Descent\n",
        "\n",
        "In gradient descent, we start with a loss function. The loss function is a way of assigning a loss, also referred to as a cost, to an undesired output. Let's represent our model with the function , where  represents our parameters , and  is an input. There are many options for a loss function; let's use squared error for now.\n",
        "\n",
        "\\begin{equation}\n",
        "SE(\\Theta) = \\big(y-F(\\vec{x}; \\Theta)\\big)^2\n",
        "\\end{equation}\n",
        "\n",
        "Naturally, the higher the value the worse the loss. So we can also imagine this loss function as a surface. We want to find the lowest point in this surface. To find it, we start from some point and find the slope along each dimension—the gradient . We then want to adjust each parameter so that it decreases the error. So if parameter  has a positive slope, we want to decrease the parameter, and if it has a negative slope we want to increase the parameter. So how do we calculate the gradient? We take the partial derivative for each parameter.\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla SE(\\Theta) = <\\frac{\\partial}{\\partial \\theta_1} E(\\Theta), ..., \\frac{\\partial}{\\partial \\theta_k} SE(\\Theta)>\n",
        "\\end{equation}\n",
        "\n",
        "We calculate partial derivatives for \\theta_i by holding the other parameters constant and taking the derivative with respect to . This will give us the slope for each parameter. We can use these slopes to update the parameters by subtracting the slope from the parameter value.\n",
        "\n",
        "If we overcorrect a parameter we might overshoot the minimal point, but the weaker our updates, the slower we learn from examples. To control the learning rate we use a hyperparameter. I'll use  for this learning rate, but you may also see it represented by other characters (often Greek). The update looks like this:\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_j = \\theta_j - r \\frac{\\partial}{\\partial \\theta_j} SE(\\Theta)\n",
        "\\end{equation}\n",
        "\n",
        "If we do this for each example, training on a million examples will take a prohibitively long time, so let's use an error function based on a batch of examples—a mean squared error.\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned} \n",
        "MSE(\\Theta) &= \\frac{1}{n} \\sum_{i=1}^{M}{SE(\\Theta)}\\\\ \n",
        "&= \\frac{1}{n} \\sum_{i=1}^{M}{\\big(y-F(\\vec{x}_i; \\Theta)\\big)^2}\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned} \n",
        "\\theta_j &= \\theta_j - r \\frac{\\partial}{\\partial \\theta_j} MSE(\\Theta)\\\\ \n",
        "&= \\theta_j - r \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{n} \\sum_{i=1}^{M}{SE(\\Theta)} \n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "The gradient is a linear operator, so we can distribute it under the sum.\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned} \n",
        "\\theta_j &= \\theta_j - r \\frac{\\partial}{\\partial \\theta_j} MSE(\\Theta)\\\\ \n",
        "&= \\theta_j - r \\frac{1}{n} \\sum_{i=1}^{M} \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{n} {SE(\\Theta)}\\\\ \n",
        "&= \\theta_j - r \\frac{1}{n} \\sum_{i=1}^{M} \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{n} {\\big(y-F(\\vec{x}; \\Theta)\\big)^2}\\\\ \n",
        "&= \\theta_j - r \\frac{1}{n} \\sum_{i=1}^{M} \\frac{2}{n} {\\big(y-F(\\vec{x}; \\Theta)\\big) \\frac{\\partial}{\\partial \\theta_j} F(\\vec{x}; \\Theta)} \n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "This will change if you use a different loss function. We will go over loss functions as we come across them in the rest of the book. The value of $\\frac{\\partial}{\\partial \\theta_j} F(\\vec{x}; \\Theta)$ will depend on your model. If it is a neural network, it will depend on your activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4JWhrZ-A-Fc",
        "colab_type": "text"
      },
      "source": [
        "### Backpropagation\n",
        "\n",
        "Backpropagation is an algorithm for training neural networks. It is essentially an implementation of chain rule from calculus. To talk about backpropagation, we must first talk about forward propagation.\n",
        "\n",
        "To build a solid intuition, we will proceed with two parallel descriptions of neural networks: mathematical and numpy. The mathematical description will help us understand what is happening on a theoretical level. The numpy description will help us understand how this can be implemented.\n",
        "\n",
        "We will again be using the Iris data set. This data is really too small for a realistic use of deep learning, but it will help us explore backpropagation. Let's remind ourselves about the Iris data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLx1FnUd3-zm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.special import softmax"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vx7-paN4AsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('./data/iris.data', names=[\n",
        "    'sepal_length',\n",
        "    'sepal_width',\n",
        "    'petal_length',\n",
        "    'petal_width',\n",
        "    'class',\n",
        "])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avnoU8Hc4Vkx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "545d5924-fea7-4ef2-de5e-e437c98462dd"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal_length  sepal_width  petal_length  petal_width        class\n",
              "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
              "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
              "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
              "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
              "4           5.0          3.6           1.4          0.2  Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxY2_ZTNA-CZ",
        "colab_type": "text"
      },
      "source": [
        "Now, let's define our network.\n",
        "\n",
        "We know we will have 4 inputs (the number of our features), so our input layer has a length of 4. There are 3 outputs (the number of our classes), so our output layer must have a length of 3. We do whatever we want for the layers in between, and we will use 6 and 5 for the first and second hidden layers, respectively. A lot of research has gone into how to construct your network. You will likely want to explore research for different use cases and approaches. As is so common in NLP and machine learning in general, one size does not fit all."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evD77ZV-4ZCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_sizes = [4, 6, 5, 3]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axbSAZAOA9_M",
        "colab_type": "text"
      },
      "source": [
        "We will define our inputs, X, and our labels, Y.  We one-hot encode the classes. In short, one-hot encoding is when we represent a categorical variable as a collection of binary variables. Let's look at the one-hot–encoded DataFrame. The results are in the tables below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaXkFHtE4m8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.drop(columns=['class'])\n",
        "Y = pd.get_dummies(df['class'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovAtt04F4puT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "6b7092ad-f626-48e2-d4eb-5e52781bab70"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal_length  sepal_width  petal_length  petal_width\n",
              "0           5.1          3.5           1.4          0.2\n",
              "1           4.9          3.0           1.4          0.2\n",
              "2           4.7          3.2           1.3          0.2\n",
              "3           4.6          3.1           1.5          0.2\n",
              "4           5.0          3.6           1.4          0.2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgkbqXEk4qId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "55675e4c-ac49-4fa9-9292-eba7d9a3d4be"
      },
      "source": [
        "Y.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Iris-setosa</th>\n",
              "      <th>Iris-versicolor</th>\n",
              "      <th>Iris-virginica</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Iris-setosa  Iris-versicolor  Iris-virginica\n",
              "0            1                0               0\n",
              "1            1                0               0\n",
              "2            1                0               0\n",
              "3            1                0               0\n",
              "4            1                0               0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rPpmEIOA97y",
        "colab_type": "text"
      },
      "source": [
        "As we can see, each possible value of the `class` column has become a column itself. For a given row, if the value of class was, say, `iris-versicolor`, then the `iris-versicolor` column will have value 1, and the others will have 0.\n",
        "\n",
        "In mathematical terms, this is what our network looks like:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned} \n",
        "W^{(1)} &= 5 \\times 4\\ \\text{matrix} \\\\ \n",
        "b^{(1)} &= 5 \\times 1\\ \\text{vector} \\\\ \n",
        "f_1 &= tanh \\\\ \n",
        "W^{(2)} &= 6 \\times 5\\ \\text{matrix} \\\\ \n",
        "b^{(2)} &= 6 \\times 1\\ \\text{vector} \\\\ \n",
        "f_2 &= tanh \\\\ \n",
        "W^{(3)} &= 3 \\times 5\\ \\text{matrix} \\\\ \n",
        "b^{(3)} &= 3 \\times 1\\ \\text{vector} \\\\ \n",
        "f_3 &= tanh \\\\ \n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "There are many ways to initialize parameters. It might seem easy to set all the parameters to 0, but this does not work. If all the weights are 0, then the output of forward propagation is unaffected by the input, making learning impossible. Here, we will be randomly initializing them. If you want to learn about more sophisticated initialization techniques, there are links in the resources. We can, however, set the bias terms to 0, since they are not associated with an input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNqHDykL42uz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(123)\n",
        "W_1 = np.random.randn(layer_sizes[1], layer_sizes[0])\n",
        "b_1 = np.zeros((layer_sizes[1], 1))\n",
        "f_1 = np.tanh\n",
        "W_2 = np.random.randn(layer_sizes[2], layer_sizes[1])\n",
        "b_2 = np.zeros((layer_sizes[2], 1))\n",
        "f_2 = np.tanh\n",
        "W_3 = np.random.randn(layer_sizes[3], layer_sizes[2])\n",
        "b_3 = np.zeros((layer_sizes[3], 1))\n",
        "f_3 = lambda H: np.apply_along_axis(softmax, axis=0, arr=H)\n",
        "\n",
        "layers = [\n",
        "    (W_1, b_1, f_1),\n",
        "    (W_2, b_2, f_2),\n",
        "    (W_3, b_3, f_3),\n",
        "]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUYiYjyDA94p",
        "colab_type": "text"
      },
      "source": [
        "Now, we will implement forward propagation.\n",
        "\n",
        "Mathematically, this is what our network is doing:\n",
        "\n",
        "\\begin{equation} \n",
        "\\begin{aligned} \n",
        "X &= 3 \\times M\\ matrix \\\\ \n",
        "H^{(1)} &= W^{(1)} \\cdot X + b^{(1)} \\\\ \n",
        "V^{(1)} &= f_1(H^{(1)}) = \\text{tanh}(H^{(1)}) \\\\ \n",
        "H^{(2)} &= W^{(2)} \\cdot V^{(1)} + b^{(2)} \\\\ \n",
        "V^{(2)} &= f_2(H^{(2)}) = \\text{tanh}(H^{(2)}) \\\\ \n",
        "H^{(3)} &= W^{(3)} \\cdot V^{(2)} + b^{(3)} \\\\ \n",
        "\\hat Y &= f_3(H^{(3)}) = \\text{softmax}(H^{(1)})\n",
        "\\end{aligned} \n",
        "\\end{equation}\n",
        "\n",
        "Where $\\text{softmax}$ is defined as\n",
        "\n",
        "\\begin{equation} \n",
        "\\text{softmax}(\\vec x) = < \\begin{array}{c} ... \\\\ \\frac{e^{x_j}}{\\sum_{i=0}^{K} e^{x_i}} \\\\ ... \\end{array} > \n",
        "\\end{equation}\n",
        "\n",
        "The following code shows how forward propagation works with an arbitrary number of layers. In this function, X is the input (one example per row). The argument `layers` is a list of weight matrix, bias term, and activation function triplets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U7SGBYS8IPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(X, layers):\n",
        "    V = X.T\n",
        "    Hs = []\n",
        "    Vs = []\n",
        "    for W, b, f in layers:\n",
        "        H = W @ V\n",
        "        H = np.add(H, b)\n",
        "        Hs.append(H)\n",
        "        V = f(H)\n",
        "        Vs.append(V)\n",
        "    return V, Hs, Vs"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "525EecBDA91X",
        "colab_type": "text"
      },
      "source": [
        "Now we need to talk about our loss function. As we described previously, the loss function is the function we use to calculate how the model did on a given batch of data. We will be using `log-loss`.\n",
        "\n",
        "\\begin{equation} \n",
        "L = - \\sum_{k}^{K}(Y \\circ log(\\hat Y)) \n",
        "\\end{equation}\n",
        "\n",
        "  The symbol ∘ represents elementwise multiplication, also known as the Hadamard product. The following function safely calculates the `log-loss`. We need to make sure that our predicted probabilities are between 0 and 1, but neither 0 nor 1. This is why we need the `eps` argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0zG7IcN9HWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_loss(Y, Y_hat, eps=10**-15):\n",
        "    # we need to protect against calling log(0), so we seet an \n",
        "    # epsilon, and define our predicted probabilities to be between\n",
        "    # epsilon and 1 - epsilon\n",
        "    min_max_p = np.maximum(np.minimum(Y_hat, 1-eps), eps)\n",
        "    log_losses = -np.sum(np.multiply(np.log(min_max_p), Y), axis=0)\n",
        "    return np.sum(log_losses) / Y.shape[1]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQtqVJMe9Ir_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb5aff92-5005-4c37-f630-8ffa9bce0f0f"
      },
      "source": [
        "Y_hat, Hs, Vs = forward(X, layers)\n",
        "loss = log_loss(Y.T, Y_hat)\n",
        "loss"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.4781844247149367"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOm_b1xxA9yD",
        "colab_type": "text"
      },
      "source": [
        "Now we see how forward propagation works and how to calculate the loss. To use gradient descent, we need to be able to calculate the gradient of the loss with respect to the individual parameters.\n",
        "\n",
        "\\begin{equation} \n",
        "\\begin{aligned} \n",
        "\\frac{\\partial L}{\\partial W^{(3)}} &= \\frac{1}{M} \\frac{\\partial L}{\\partial \\hat Y} \\cdot \\frac{\\partial \\hat Y}{\\partial W^{(3)}} \\\\ \n",
        "&= \\frac{1}{M} \\frac{\\partial L}{\\partial \\hat Y} \\cdot \\frac{\\partial \\hat Y}{\\partial H^{(3)}} \\cdot \\frac{\\partial H^{(3)}}{\\partial W^{(3)}}\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "The combination of `log-loss` and `softmax` gives us a friendly expression for $\\frac{\\partial L}{\\partial \\hat Y} \\cdot \\frac{\\partial \\hat Y}{\\partial H^{(3)}}$.\n",
        "\n",
        "\\begin{equation} \n",
        "\\begin{aligned} \n",
        "\\frac{\\partial L}{\\partial W^{(3)}} &= \\frac{1}{M} (\\hat Y - Y) \\cdot \\frac{\\partial H^{(3)}}{\\partial W^{(3)}} \\\\ \n",
        "&= \\frac{1}{M} (\\hat Y - Y) \\cdot V^{(2)T}\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "The gradient for the bias term is derived in the same way. Instead of taking the output from the earlier layer, it is multiplied (dot product) by a vector of all 1s.\n",
        "\n",
        "\\begin{equation} \n",
        "\\begin{aligned} \n",
        "\\frac{\\partial L}{\\partial b^{(3)}} &= \\frac{1}{M} (\\hat Y - Y) \\cdot \\vec 1 \\\\ \n",
        "&= \\frac{1}{M} \\sum_j^M \\hat y_j - y_j \n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "Let's see what this looks like in code. We will use names that parallel the mathematical terms. First we can define $\\frac{\\partial L}{\\partial H^{(3)}}$. We need to remember to transpose Y, so it has the same dimensions as `Y_hat`.\n",
        "\n",
        "Let's look at the gradient values for $\\frac{\\partial L}{\\partial W^{(3)}}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeLEG_Yc9_DN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "b951fdd1-77ef-45ad-c769-83ffe5608a47"
      },
      "source": [
        "dL_dH_3 = Y_hat - Y.values.T\n",
        "dH_3_dW_3 = Vs[1]\n",
        "dL_dW_3 = (1 / len(Y)) * dL_dH_3 @ dH_3_dW_3.T\n",
        "print(dL_dW_3.shape)\n",
        "dL_dW_3"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.010773</td>\n",
              "      <td>-0.008965</td>\n",
              "      <td>0.210314</td>\n",
              "      <td>-0.210140</td>\n",
              "      <td>0.207157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.084970</td>\n",
              "      <td>-0.214219</td>\n",
              "      <td>0.123530</td>\n",
              "      <td>-0.122504</td>\n",
              "      <td>0.126386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.074197</td>\n",
              "      <td>0.223184</td>\n",
              "      <td>-0.333843</td>\n",
              "      <td>0.332644</td>\n",
              "      <td>-0.333543</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3         4\n",
              "0  0.010773 -0.008965  0.210314 -0.210140  0.207157\n",
              "1 -0.084970 -0.214219  0.123530 -0.122504  0.126386\n",
              "2  0.074197  0.223184 -0.333843  0.332644 -0.333543"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrehADE_A9u2",
        "colab_type": "text"
      },
      "source": [
        "Now let's calculate the gradient for the bias term"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzHlhgL4-EOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6aab954d-f30c-44a5-af9e-144d812ff172"
      },
      "source": [
        "dH_3_db_3 = np.ones(len(Y))\n",
        "dL_db_3 = (1 / len(Y)) * dL_dH_3 @ dH_3_db_3\n",
        "print(dL_db_3.shape)\n",
        "dL_db_3"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.21081745, -0.12346054,  0.334278  ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRAO4R2WA9rq",
        "colab_type": "text"
      },
      "source": [
        "Let's look a layer further. To calculate the gradient for the $\\frac{\\partial L}{\\partial W_{(2)}}$, we will need to continue applying the chain rule. As you can see, this derivation gets complicated quickly.\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned} \n",
        "\\frac{\\partial L}{\\partial W^{(2)}} &= \\frac{1}{M} \\frac{\\partial L}{\\partial \\hat Y} \\cdot \\frac{\\partial \\hat Y}{\\partial W^{(2)}} \\\\ \n",
        "&= \\frac{1}{M} \\frac{\\partial L}{\\partial \\hat Y} \\cdot \\frac{\\partial \\hat Y}{\\partial H^{(3)}} \\cdot \\frac{\\partial H^{(3)}}{\\partial W^{(2)}} \\\\ \n",
        "&= \\frac{1}{M} \\frac{\\partial L}{\\partial \\hat Y} \\cdot \\frac{\\partial \\hat Y}{\\partial H^{(3)}} \\cdot \\frac{\\partial H^{(3)}}{\\partial V^{(2)}} \\cdot \\frac{\\partial V^{(3)}}{\\partial W^{(2)}} \\\\ \n",
        "&= \\frac{1}{M} \\frac{\\partial L}{\\partial \\hat Y} \\cdot \\frac{\\partial \\hat Y}{\\partial H^{(3)}} \\cdot \\frac{\\partial H^{(3)}}{\\partial V^{(2)}} \\cdot \\frac{\\partial V^{(2)}}{\\partial H^{(2)}} \\cdot \\frac{\\partial H^{(2)}}{\\partial W^{(2)}}\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "We know part of this.\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned} \n",
        "\\frac{\\partial L}{\\partial W^{(2)}} &= \\frac{1}{M} (\\hat Y - Y) \\cdot \\frac{\\partial H^{(3)}}{\\partial V^{(2)}} \\cdot \\frac{\\partial V^{(2)}}{\\partial H^{(2)}} \\cdot \\frac{\\partial H^{(2)}}{\\partial W^{(2)}} \\\\ \n",
        "&= \\frac{1}{M} (\\hat Y - Y) \\cdot W^{(3)} \\cdot (1 - V^{(2)} \\circ V^{(2)}) \\cdot V^{(1)T}\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "We can calculate this. Notice here that we need to keep track of intermediate values. Use these values returned from the forward propagation step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iBBp1cp_TeU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "01b957fc-b982-45d5-8f3a-c5f0c4fde88d"
      },
      "source": [
        "dH_3_dV_2 = W_3\n",
        "dV_2_dH_2 = 1 - np.power(Vs[1], 2)\n",
        "dH_2_dW_2 = Vs[0]\n",
        "dL_dH_2 = np.multiply((dL_dH_3.T @ dH_3_dV_2).T, dV_2_dH_2)\n",
        "dL_dW_2 = (1 / len(Y)) * dL_dH_2 @ dH_2_dW_2.T\n",
        "print(dL_dW_2.shape)\n",
        "dL_dW_2"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.302449</td>\n",
              "      <td>-0.194403</td>\n",
              "      <td>0.314719</td>\n",
              "      <td>0.317461</td>\n",
              "      <td>0.317539</td>\n",
              "      <td>0.317538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.049117</td>\n",
              "      <td>-0.001843</td>\n",
              "      <td>-0.055560</td>\n",
              "      <td>-0.055613</td>\n",
              "      <td>-0.055634</td>\n",
              "      <td>-0.055636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000722</td>\n",
              "      <td>0.000503</td>\n",
              "      <td>-0.000734</td>\n",
              "      <td>-0.000747</td>\n",
              "      <td>-0.000747</td>\n",
              "      <td>-0.000747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.003561</td>\n",
              "      <td>0.002604</td>\n",
              "      <td>-0.003723</td>\n",
              "      <td>-0.003732</td>\n",
              "      <td>-0.003732</td>\n",
              "      <td>-0.003732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.016696</td>\n",
              "      <td>-0.006639</td>\n",
              "      <td>-0.017758</td>\n",
              "      <td>-0.018240</td>\n",
              "      <td>-0.018247</td>\n",
              "      <td>-0.018247</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3         4         5\n",
              "0 -0.302449 -0.194403  0.314719  0.317461  0.317539  0.317538\n",
              "1  0.049117 -0.001843 -0.055560 -0.055613 -0.055634 -0.055636\n",
              "2  0.000722  0.000503 -0.000734 -0.000747 -0.000747 -0.000747\n",
              "3  0.003561  0.002604 -0.003723 -0.003732 -0.003732 -0.003732\n",
              "4  0.016696 -0.006639 -0.017758 -0.018240 -0.018247 -0.018247"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I-s1vOoA9oo",
        "colab_type": "text"
      },
      "source": [
        "For the bias term it is similar (see table0407).\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned} \n",
        "\\frac{\\partial L}{\\partial b^{(2)}} &= \\frac{1}{M} (\\hat Y - Y) \\cdot \\frac{\\partial H^{(3)}}{\\partial V^{(2)}} \\cdot \\frac{\\partial V^{(2)}}{\\partial H^{(2)}} \\cdot \\frac{\\partial H^{(2)}}{\\partial b^{(2)}} \\\\ &= \\frac{1}{M} (\\hat Y - Y) \\cdot W^{(3)} \\cdot (1 - V^{(2)} \\circ V^{(2)}) \\cdot \\vec 1\n",
        "\\end{aligned}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAISt8bt_ea9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "3f8532d9-60df-4c76-e440-e37f2d08677c"
      },
      "source": [
        "dH_2_db_2 = np.ones(len(Y))\n",
        "dL_db_2 = (1 / len(Y)) * dL_dH_2 @ dH_2_db_2.T\n",
        "print(dL_db_2.shape)\n",
        "dL_db_2"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.317539\n",
              "1   -0.055634\n",
              "2   -0.000747\n",
              "3   -0.003732\n",
              "4   -0.018247\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKP5kLsGA9lk",
        "colab_type": "text"
      },
      "source": [
        "I'll leave deriving the next layer as an exercise. It should be straightforward because layer 1 is so similar to layer 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDbe84QG_jeM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "b6591c21-a3df-466b-8492-c87250bea3fc"
      },
      "source": [
        "dH_2_dV_1 = W_2\n",
        "dV_1_dH_1 = 1 - np.power(Vs[0], 2)\n",
        "dL_dH_1 = np.multiply((dL_dH_2.T @ dH_2_dV_1).T, dV_1_dH_1)\n",
        "dH_1_dW_1 = X.values.T\n",
        "dL_dW_1 = (1 / len(Y)) * dL_dH_1 @ dH_1_dW_1.T\n",
        "print(dL_dW_1.shape)\n",
        "dL_dW_1"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.783060e-01</td>\n",
              "      <td>-1.253225e-01</td>\n",
              "      <td>-5.240050e-02</td>\n",
              "      <td>-7.952154e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.773021e-01</td>\n",
              "      <td>3.260914e-01</td>\n",
              "      <td>1.394070e-01</td>\n",
              "      <td>2.328259e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.808615e-02</td>\n",
              "      <td>3.469462e-02</td>\n",
              "      <td>-4.649400e-02</td>\n",
              "      <td>-2.300012e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-7.880986e-04</td>\n",
              "      <td>-5.902413e-04</td>\n",
              "      <td>-3.475747e-05</td>\n",
              "      <td>8.403521e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-4.729628e-16</td>\n",
              "      <td>-2.866947e-16</td>\n",
              "      <td>-1.341379e-16</td>\n",
              "      <td>-2.326840e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-4.116040e-06</td>\n",
              "      <td>-2.487064e-06</td>\n",
              "      <td>7.311565e-08</td>\n",
              "      <td>4.091940e-07</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              0             1             2             3\n",
              "0 -1.783060e-01 -1.253225e-01 -5.240050e-02 -7.952154e-03\n",
              "1  4.773021e-01  3.260914e-01  1.394070e-01  2.328259e-02\n",
              "2  1.808615e-02  3.469462e-02 -4.649400e-02 -2.300012e-02\n",
              "3 -7.880986e-04 -5.902413e-04 -3.475747e-05  8.403521e-05\n",
              "4 -4.729628e-16 -2.866947e-16 -1.341379e-16 -2.326840e-17\n",
              "5 -4.116040e-06 -2.487064e-06  7.311565e-08  4.091940e-07"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV6aHAbh_liU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "edb51207-3e47-42eb-bb76-e513a981d3a0"
      },
      "source": [
        "dH_1_db_1 = np.ones(len(Y))\n",
        "dL_db_1 = (1 / len(Y)) * dL_dH_1 @ dH_1_db_1.T\n",
        "print(dL_db_1.shape)\n",
        "dL_db_1"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0   -3.627637e-02\n",
              "1    9.832581e-02\n",
              "2    7.392729e-03\n",
              "3   -1.758950e-04\n",
              "4   -1.066024e-16\n",
              "5   -1.025423e-06\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C52dQPtoA9iM",
        "colab_type": "text"
      },
      "source": [
        "Now that we have calculated the gradients for our first iteration, let's build a function for doing these calculations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vComYvny_ots",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = [[W_1, W_2, W_3], [b_1, b_2, b_3]]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R0M5_jMA9ei",
        "colab_type": "text"
      },
      "source": [
        "We need a function for calculating our gradients. This method will need the following: the inputs $X$, the labels $Y$, the predicted probabilities $\\hat Y$, the parameters $W^{(i)}$ and $b^{(i)}$, and the intermediate values $V^{(i)}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3CClhRO_62N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradients(X, Y, Y_hat, params, Vs):\n",
        "    Ws, bs = params\n",
        "    assert len(Ws) == len(bs)\n",
        "    dL_dHs = [None] * len (layers)\n",
        "    dL_dWs = [None] * len (layers)\n",
        "    dL_dbs = [None] * len (layers)\n",
        "    dL_dHs[2] = Y.T - Y_hat\n",
        "    for layer in np.arange(len(layers), 0, -1) - 1:\n",
        "        dL_dH = dL_dHs[layer]\n",
        "        dH_dW = Vs[layer - 1] if layer > 0 else X.T\n",
        "        dL_dW = (1 / len(Y)) * dL_dH @ dH_dW.T\n",
        "        dH_db = np.ones(len(Y))\n",
        "        dL_db = (1 / len(Y)) * dL_dH @ dH_db\n",
        "        dL_dWs[layer] = dL_dW\n",
        "        dL_dbs[layer] = dL_db.reshape(len(dL_db), 1)\n",
        "        if layer > 0:\n",
        "            dH_dV = Ws[layer]\n",
        "            # just supporting tanh\n",
        "            dV_dH_next = 1 - np.power(Vs[layer - 1], 2)\n",
        "            dL_dHs[layer - 1] = \\\n",
        "                np.multiply((dL_dH.T @ dH_dV).T, dV_dH_next)\n",
        "        \n",
        "    return dL_dWs, dL_dbs"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb7S3JJ8A9at",
        "colab_type": "text"
      },
      "source": [
        "We need a method that will evaluate the model then calculate the loss and gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97ToOOQG__I2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update(X, Y, params, learning_rate=0.1):\n",
        "    Ws, bs = params\n",
        "    Y_hat, Hs, Vs = forward(X, layers)\n",
        "    loss = log_loss(Y.T, Y_hat)\n",
        "    dWs, dbs = gradients(X, Y, Y_hat, params, Vs)\n",
        "    for i in range(len(Ws)):\n",
        "        Ws[i] += learning_rate * dWs[i]\n",
        "        bs[i] += learning_rate * dbs[i]\n",
        "    return Ws, bs, loss"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Feb6K2UwA9OV",
        "colab_type": "text"
      },
      "source": [
        "Finally, we will have a method for training the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxDa-CqaAB-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(X, Y, params, learning_rate=0.1, epochs=6000):\n",
        "    X = X.values\n",
        "    Y = Y.values\n",
        "    Ws = [W for W in params[0]]\n",
        "    bs = [b for b in params[1]]\n",
        "    for i in range(epochs):\n",
        "        Ws, bs, loss = update(X, Y, [Ws, bs], learning_rate)\n",
        "        if i % (epochs // 10) == 0:\n",
        "            print('epoch', i, 'loss', loss)\n",
        "    print('epoch', i, 'loss', loss)\n",
        "    return Ws, bs"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmSfLczRA5xK",
        "colab_type": "text"
      },
      "source": [
        "Let's train our network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyr3SmuSAHAm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "809127a2-2152-4847-f366-464635f38d26"
      },
      "source": [
        "Ws, bs = train(X, Y, params)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 loss 1.4781844247149367\n",
            "epoch 600 loss 0.4708947529111574\n",
            "epoch 1200 loss 0.3031961283699454\n",
            "epoch 1800 loss 0.08750030286914373\n",
            "epoch 2400 loss 0.05997825653697613\n",
            "epoch 3000 loss 0.05260970329803143\n",
            "epoch 3600 loss 0.04865561597901619\n",
            "epoch 4200 loss 0.04570335069990855\n",
            "epoch 4800 loss 0.04256926907593557\n",
            "epoch 5400 loss 0.03858140769946972\n",
            "epoch 5999 loss 0.03768397896156344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPowvEo_AIDM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "f8769b1e-565b-4f4e-e699-b37d1a69c25d"
      },
      "source": [
        "Y_hat, _, _ = forward(X, layers)\n",
        "Y_hat = pd.DataFrame(Y_hat.T, columns=[c + '_prob' for c in Y.columns])\n",
        "Y_hat['pred'] = np.argmax(Y_hat.values, axis=1)\n",
        "Y_hat['pred'] = Y_hat['pred'].apply(Y.columns.__getitem__)\n",
        "Y_hat['truth'] = Y.idxmax(axis=1)\n",
        "Y_hat.head()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Iris-setosa_prob</th>\n",
              "      <th>Iris-versicolor_prob</th>\n",
              "      <th>Iris-virginica_prob</th>\n",
              "      <th>pred</th>\n",
              "      <th>truth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.998169</td>\n",
              "      <td>0.001830</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>Iris-setosa</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.998152</td>\n",
              "      <td>0.001846</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>Iris-setosa</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.998167</td>\n",
              "      <td>0.001831</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>Iris-setosa</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.998155</td>\n",
              "      <td>0.001844</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>Iris-setosa</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.998170</td>\n",
              "      <td>0.001829</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>Iris-setosa</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Iris-setosa_prob  Iris-versicolor_prob  ...         pred        truth\n",
              "0          0.998169              0.001830  ...  Iris-setosa  Iris-setosa\n",
              "1          0.998152              0.001846  ...  Iris-setosa  Iris-setosa\n",
              "2          0.998167              0.001831  ...  Iris-setosa  Iris-setosa\n",
              "3          0.998155              0.001844  ...  Iris-setosa  Iris-setosa\n",
              "4          0.998170              0.001829  ...  Iris-setosa  Iris-setosa\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-A7OZzmALFd",
        "colab_type": "text"
      },
      "source": [
        "Let's see the proportion we got right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTPOGi8TAKNW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60bbaa74-5903-4e98-df3c-0e0fa18b39cc"
      },
      "source": [
        "(Y_hat['pred'] == Y_hat['truth']).mean()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9933333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TCbBg-YAOtf",
        "colab_type": "text"
      },
      "source": [
        "This is good, but we have likely overfit. When we try actual training models, we will need to create train, validation, and test data sets. The train data set will be for learning our parameters (e.g., weights), validation for learning our hyperparameters (e.g., number and sizes of layers), and finally the test data set for understanding how our model will perform on unseen data.\n",
        "\n",
        "Let's look at the errors our model makes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gnxEaYAAN8F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "9ddf0227-3506-4e59-c455-2c1a1218ba13"
      },
      "source": [
        "Y_hat[Y_hat['pred'] != Y_hat['truth']]\\\n",
        "  .groupby(['pred', 'truth']).size().to_frame()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred</th>\n",
              "      <th>truth</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Iris-virginica</th>\n",
              "      <th>Iris-versicolor</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                0\n",
              "pred           truth             \n",
              "Iris-virginica Iris-versicolor  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3adrAiaA1pA",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the two classes that it confused along with the error example. Here are the features for the error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X-xxCjXAZ_8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "f7bce1b1-bf1e-43f1-b0f2-0e66bcf87863"
      },
      "source": [
        "X[Y_hat['pred'] != Y_hat['truth']]"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>6.0</td>\n",
              "      <td>2.7</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sepal_length  sepal_width  petal_length  petal_width\n",
              "83           6.0          2.7           5.1          1.6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLgR8-ffCiim",
        "colab_type": "text"
      },
      "source": [
        "Here are the mean class features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptjPPLqQCPJ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "24f279f6-a56c-441d-ffaa-a07e16b127bf"
      },
      "source": [
        "df.groupby('class').mean()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Iris-setosa</th>\n",
              "      <td>5.006</td>\n",
              "      <td>3.418</td>\n",
              "      <td>1.464</td>\n",
              "      <td>0.244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Iris-versicolor</th>\n",
              "      <td>5.936</td>\n",
              "      <td>2.770</td>\n",
              "      <td>4.260</td>\n",
              "      <td>1.326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Iris-virginica</th>\n",
              "      <td>6.588</td>\n",
              "      <td>2.974</td>\n",
              "      <td>5.552</td>\n",
              "      <td>2.026</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 sepal_length  sepal_width  petal_length  petal_width\n",
              "class                                                                \n",
              "Iris-setosa             5.006        3.418         1.464        0.244\n",
              "Iris-versicolor         5.936        2.770         4.260        1.326\n",
              "Iris-virginica          6.588        2.974         5.552        2.026"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxLlDBOUCmEK",
        "colab_type": "text"
      },
      "source": [
        "Here are the standard deviations of the class features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG0LzgcACPvE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "aa44feb5-0311-457b-956c-475ae4889a50"
      },
      "source": [
        "df.groupby('class').std()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Iris-setosa</th>\n",
              "      <td>0.352490</td>\n",
              "      <td>0.381024</td>\n",
              "      <td>0.173511</td>\n",
              "      <td>0.107210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Iris-versicolor</th>\n",
              "      <td>0.516171</td>\n",
              "      <td>0.313798</td>\n",
              "      <td>0.469911</td>\n",
              "      <td>0.197753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Iris-virginica</th>\n",
              "      <td>0.635880</td>\n",
              "      <td>0.322497</td>\n",
              "      <td>0.551895</td>\n",
              "      <td>0.274650</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 sepal_length  sepal_width  petal_length  petal_width\n",
              "class                                                                \n",
              "Iris-setosa          0.352490     0.381024      0.173511     0.107210\n",
              "Iris-versicolor      0.516171     0.313798      0.469911     0.197753\n",
              "Iris-virginica       0.635880     0.322497      0.551895     0.274650"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4qPRmgaAQXT",
        "colab_type": "text"
      },
      "source": [
        "It looks like the flower in question had larger petals than most other Iris versicolor.\n",
        "\n",
        "Training the model is done in batches. These batches are generally small sets of your training data. There are tradeoffs to the size of the batches: if you pick a smaller batch size, you require less computation. However, you are using less data to perform an update, which may be noisy. If you pick a larger batch size, you get a more reliable update, but this requires more computation and can lead to overfitting. The overfitting is possible because you are using more of your data to calculate the updates.\n",
        "\n",
        "Once we have these gradients, we can use them to update our parameters and so perform gradient descent. This is a very simplified introduction to a rich and complicated topic. I encourage you to do additional learning on the topic. As we go on, I will cover deep learning topics to the depth necessary to understand how the techniques are implemented. A thorough explanation of deep learning topics is outside the scope of this book.\n",
        "\n",
        "Now let's look at some developments on top of neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeKidApZDREG",
        "colab_type": "text"
      },
      "source": [
        "## Convolutional Neural Networks\n",
        "\n",
        "In 1959 David H. Hubel and Torsten Wiesel conducted experiments on cats that showed the existence of specialized neurons that detected edges, position, and motion.  This inspired Kunihiko Fukushima to create the \"cognitron\" in 1975 and later the \"neocognitron\" in 1980. This neural network, and others based on it, included early notions of pooling layers and filters. In 1989, the modern convolutional neural network, or CNN, with weights learned fully by backpropagation, was created by  Yann LeCun.\n",
        "\n",
        "Generally, CNNs are explained with images as an example, but it's just as easy to apply these techniques to one-dimensional data.\n",
        "\n",
        "### Filters\n",
        "\n",
        "Filters are layers that take a continuous subset of the previous layer (e.g., a block of a matrix) and feed it into a neuron in the next layer. This technique is inspired by the idea of a receptive field in the human eye, where different neurons are responsible for different regions and shapes in vision.\n",
        "\n",
        "Imagine you have a  matrix coming into a layer. We can use a filter of size  to feed into 9 neurons. We do this by doing an element-wise multiplication between a subsection of the input matrix and the filter and then summing the products. In this example we use elements (1,1) to (4,4) with the filter for the first element of the output vector. We then multiply the elements (1,2) to (4,5) with the filter for the second element. We can also change the stride, which is the number of columns/rows for which we move the filter for each output neuron. If we have our  matrix with  filter and a stride of 2, we can feed into 4 neurons. With padding, we can add extra rows and columns of 0s to our input matrix, so that the values at the edge get the same treatment as the interior values. Otherwise, elements on the edge are used less than inner elements.\n",
        "\n",
        "### Pooling\n",
        "\n",
        "Pooling works similarly to filters—except, instead of using weights that must be learned, simple aggregate is used. Max pooling, taking the max of the continuous subset, is the most commonly used. Though, one can use average pooling or other aggregates.\n",
        "\n",
        "This is often useful for reducing the size of the input data without adding new parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha8pd64EDRbr",
        "colab_type": "text"
      },
      "source": [
        "## Recurrent Neural Networks\n",
        "\n",
        "In the initial research into modeling biological neural networks, it has been assumed that memory and learning have some dependence on time. However, none of the techniques covered so far take that into account.\n",
        "\n",
        "In a multilayer perceptron, we get one example and produce one output. The forward propagation step for one example is not affected by another example. In a recurrent neural network, or RNN, we need to be aware of the previous, and sometimes later, examples. For example, if I am trying to translate a word, it is important that I know its context.\n",
        "\n",
        "Now, the most common type of RNN uses long short-term memory, or LSTM. To understand LSTMs, let's talk about some older techniques.\n",
        "\n",
        "### Backpropagation Through Time\n",
        "\n",
        "The primary training algorithm for RNNs is backpropagation through time, or BPTT. This works by unfolding the network. Let's say we have a sequence of k items. Conceptually, unfolding works by copying the recurrent part of the network k times. Practically, it works by calculating the partial derivate of each intermediate output with respect to the parameters of the recurrent part of the network.\n",
        "\n",
        "We will go through BPTT in depth in #sequence_modeling_with_keras when we cover sequence modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvf_AvrpDRnF",
        "colab_type": "text"
      },
      "source": [
        "### Elman Nets\n",
        "\n",
        "Also known as simple RNNs, or SRNNs, an Elman network reuses the output of the previous layer.  Jeffrey Elman invented the Elman network in 1990. The idea is relatively straightforward. We want the output of the previous example to represent the context. We combine that output with current input, using different weights.\n",
        "\n",
        "\\begin{equation} \n",
        "\\begin{aligned} \n",
        "V^{(0)} &= 0 \\\\ \n",
        "1 &\\le t \\le T \\\\ \n",
        "V^{(t)} &= f_{input}(W_{input} \\cdot X^{(t)} + U_{input} \\cdot V^{(t-1)} + b_{input}) \\\\ \n",
        "Y^{(t)} &= f_{output}(W_{output} \\cdot V^{(t)} + b_{output}) \n",
        "\\end{aligned}\n",
        "\\end{equation} \n",
        "\n",
        "As you can see, the context is represented by . This will provide information from all previous elements in the sequence to some degree. This means that the longer the sequence, the more terms there are in the gradient for the parameters. This can make the parameters change chaotically. To reduce this concern, we could use a much smaller learning rate at the cost of increased training time.   We still have the chance of a training run resulting in exploding or vanishing gradients. Exploding/vanishing gradients are when the gradients for a parameter increase rapidly or go to 0. This problem can occur in any sufficiently deep network, but RNNs are particularly susceptible.\n",
        "\n",
        "\\begin{equation} \n",
        "\\frac{\\partial L}{\\partial W_{input}^{(i)}} = \\frac{\\partial L}{\\partial \\hat Y} \\cdot ... \\cdot \\frac{\\partial L}{\\partial V^{(i,T)}} \\cdot \\prod_{t=2}^{T}{\\frac{\\partial V^{(i,t)}}{\\partial V^{(i,t-1)}}} \\cdot \\frac{\\partial V^{(i,1)}}{\\partial W_{input}^{(i)}}\n",
        "\\end{equation} \n",
        "\n",
        "For long sequences, this could make our gradient go very high or very low quickly.\n",
        "\n",
        "### LSTMs\n",
        "\n",
        "The LSTM was invented by Sepp Hochreiter and Jürgen Schmidhuber in 1997 to address the exploding/vanishing gradients problem. The idea is that we can learn how long to hold on to information by giving our recurrent units state. We can store an output produced from an element of the sequence and use this to modify the output. This state can also be connected with a notion of forgetting, so we can allow some gradients to vanish when appropriate. Here are the components of the LSTM:\n",
        "\n",
        "\\begin{aligned} \n",
        "v_0 &= 0 \\\\ \n",
        "c_0 &= 0 \\\\ \n",
        "1 &\\le t \\le T \\\\ \n",
        "f_t &= \\sigma(W_f \\cdot \\vec x_t + U_f \\cdot v^{(t-1)} + b_f) \\\\ \n",
        "i_t &= \\sigma(W_i \\cdot \\vec x_t + U_i \\cdot v^{(t-1)} + b_i) \\\\ \n",
        "o_t &= \\sigma(W_o \\cdot \\vec x_t + U_o \\cdot v^{(t-1)} + b_o) \\\\ \n",
        "\\tilde c_t &= \\text{tanh}(W_c \\cdot \\vec x_t + U_c \\cdot v^{(t-1)} + b_c) \\\\ c_t &= f_t \\circ c_{t-1} + i_t \\circ \\tilde c_t \\\\ \n",
        "v_t &= o_t \\circ \\text{tanh}(c_t) \n",
        "\\end{aligned}\n",
        "\n",
        "There is a lot to unpack here. We will go into more depth, including covering variants, when we get to #sequence_modeling_with_keras, in which we present a motivating example.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFB8TdCFDRQl",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "Being able to reason mathematically about what is happening in a neural network is important. Derive the updates for the first layer in the network mentioned in this chapter.\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial L}{\\partial W^{(1)}} = \\frac{1}{M} \\frac{\\partial L}{\\partial \\hat Y} \\cdot \\frac{\\partial \\hat Y}{\\partial W^{(1)}}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eXP2khxAQBe",
        "colab_type": "text"
      },
      "source": [
        "A common exercise when studying deep learning is to implement a classifier on the Modified National Institute of Standards and Technology (MNST) data set. This classifier takes in an image of a handwritten digit and predicts which digit it represents.\n",
        "\n",
        "There are thousands of such tutorials available, so I will not retread that overtrodden ground. I recommend doing the [TensorFlow tutorial](https://www.tensorflow.org/tutorials/keras/classification)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vUeSaFjDQXV",
        "colab_type": "text"
      },
      "source": [
        "## Resources\n",
        "\n",
        "* Andrew Ng's Deep Learning Specialization: this course is a great way to become familiar with deep learning concepts.\n",
        "* TensorFlow tutorials: TensorFlow has a number of great resources. Their tutorials are a way to get familiar with deep learning and the TensorFlow API.\n",
        "* Deep Learning, by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press): this is a free online book that goes over the theory of deep learning.\n",
        "* Natural Language Processing with PyTorch, by Delip Rao and Brian McMahan (O'Reilly)\n",
        "  * This book covers NLP with PyTorch, which is another popular deep learning library. This book won't cover PyTorch, but if you want to have a good understanding of the field, learning about PyTorch is a good idea.\n",
        "* Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow, by Aurélien Géron (O'Reilly)\n",
        "  * This book covers many machine learning techniques in addition to deep learning.  "
      ]
    }
  ]
}