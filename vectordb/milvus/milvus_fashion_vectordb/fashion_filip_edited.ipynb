{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, tensor\n",
    "from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import Resize\n",
    "import torchvision.transforms as T\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision.transforms.functional import crop\n",
    "import time\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Working with Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the Images into the Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looping through all the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "for image in glob.glob(\"./photos/**/*.jpg\", recursive=True):\n",
    "    image_paths.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from milvus import default_server\n",
    "from pymilvus import utility, connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    __  _________ _   ____  ______\n",
      "   /  |/  /  _/ /| | / / / / / __/\n",
      "  / /|_/ // // /_| |/ / /_/ /\\ \\\n",
      " /_/  /_/___/____/___/\\____/___/ {Lite}\n",
      "\n",
      " Welcome to use Milvus!\n",
      "\n",
      " Version:   v2.2.10-lite\n",
      " Process:   50053\n",
      " Started:   2023-06-22 17:38:11\n",
      " Config:    /Users/filiphaltmayer/.milvus.io/milvus-server/2.2.10/configs/milvus.yaml\n",
      " Logs:      /Users/filiphaltmayer/.milvus.io/milvus-server/2.2.10/logs\n",
      "\n",
      " Ctrl+C to exit ...\n"
     ]
    }
   ],
   "source": [
    "default_server.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "connections.connect(host=\"127.0.0.1\", port=default_server.listen_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION = 2048\n",
    "BATCH_SIZE = 128\n",
    "COLLECTION_NAME = \"fashion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this before importing th resnet50 model if you run into an SSL certificate URLError\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/filiphaltmayer/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "/Users/filiphaltmayer/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "/Users/filiphaltmayer/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/Classification/ConvNets/image_classification/models/efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "2023-06-22 17:38:20.336051: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/filiphaltmayer/miniconda3/envs/general/lib/python3.9/site-packages/transformers/models/segformer/feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/Users/filiphaltmayer/miniconda3/envs/general/lib/python3.9/site-packages/transformers/models/segformer/image_processing_segformer.py:99: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Load the embedding model with the last layer removed\n",
    "embeddings_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\n",
    "embeddings_model = torch.nn.Sequential(*(list(embeddings_model.children())[:-1]))\n",
    "embeddings_model.eval()\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\n",
    "segmentation_model = SegformerForSemanticSegmentation.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import FieldSchema, CollectionSchema, Collection, DataType\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name='filepath', dtype=DataType.VARCHAR, max_length=200),\n",
    "    FieldSchema(name=\"name\", dtype=DataType.VARCHAR, max_length=200),\n",
    "    FieldSchema(name=\"seg_id\", dtype=DataType.INT64),\n",
    "    FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields=fields, enable_dynamic_field=True)\n",
    "collection = Collection(name=COLLECTION_NAME, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = {\n",
    "    \"index_type\": \"IVF_FLAT\",\n",
    "    \"metric_type\": \"L2\",\n",
    "    \"params\": {\"nlist\": 128},\n",
    "}\n",
    "collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentation(image):\n",
    "    inputs = extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = segmentation_model(**inputs)\n",
    "    logits = outputs.logits.cpu()\n",
    "\n",
    "    upsampled_logits = nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    pred_seg = upsampled_logits.argmax(dim=1)[0]\n",
    "    return pred_seg \n",
    "\n",
    "# returns two lists masks (tensor) and obj_ids (int)\n",
    "# \"mattmdjaga/segformer_b2_clothes\" from hugging face\n",
    "def get_masks(segmentation):\n",
    "    obj_ids = torch.unique(segmentation)\n",
    "    obj_ids = obj_ids[1:]\n",
    "    masks = segmentation == obj_ids[:, None, None]\n",
    "    return masks, obj_ids\n",
    "\n",
    "def crop_images(masks, obj_ids, img):\n",
    "    boxes = masks_to_boxes(masks)\n",
    "    crop_boxes = []\n",
    "    for box in boxes:\n",
    "        crop_box = tensor([box[0], box[1], box[2]-box[0], box[3]-box[1]])\n",
    "        crop_boxes.append(crop_box)\n",
    "    \n",
    "    preprocess = T.Compose([\n",
    "        T.Resize(size=(256, 256)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    preprocess_bounded = T.Compose([T.ToTensor()])\n",
    "    \n",
    "    cropped_images = []\n",
    "    seg_ids = []\n",
    "    for i in range(len(crop_boxes)):\n",
    "        crop_box = crop_boxes[i]\n",
    "        cropped = crop(img, crop_box[1].item(), crop_box[0].item(), crop_box[3].item(), crop_box[2].item())\n",
    "        cropped_images.append(preprocess(cropped))\n",
    "        seg_ids.append(obj_ids[i].item())\n",
    "    with torch.no_grad():\n",
    "        embeddings = embeddings_model(torch.stack(cropped_images)).squeeze().tolist()\n",
    "    return embeddings, boxes.tolist(), seg_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for path in image_paths[:3]:\n",
    "    image = Image.open(path)\n",
    "    path_split = path.split(\"/\")\n",
    "    name = \" \".join(path_split[2].split(\"_\"))\n",
    "    segmentation = get_segmentation(image)\n",
    "    masks, ids = get_masks(segmentation)\n",
    "    embeddings, crop_corners, seg_ids = crop_images(masks, ids, image)\n",
    "    inserts = [{\"embedding\": embeddings[x], \n",
    "                \"seg_id\": seg_ids[x],\n",
    "                \"name\": name,\n",
    "                \"filepath\": path,\n",
    "                \"crop_corner\": crop_corners[x]} for x in range(len(embeddings))]\n",
    "    collection.insert(inserts)\n",
    "                \n",
    "collection.flush()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the Vector DB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n",
      "11\n",
      "0.032022953033447266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t1/4dsdtlv12r7cpxx9hkz33b840000gn/T/ipykernel_50038/2984603024.py:11: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  return plt.cm.get_cmap(name, n)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from PIL import ImageDraw\n",
    "from collections import Counter\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "%matplotlib auto\n",
    "\n",
    "def get_cmap(n, name='hsv'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)\n",
    "# cmap = get_cmap(len(data))\n",
    "# for i, (X, Y) in enumerate(data):\n",
    "#    scatter(X, Y, c=cmap(i))\n",
    "\n",
    "search_paths = [\"./photos/Taylor_Swift/Taylor_Swift_3.jpg\"]\n",
    "\n",
    "for path in search_paths:\n",
    "    image = Image.open(path)\n",
    "    path_split = path.split(\"/\")\n",
    "    name = \" \".join(path_split[2].split(\"_\"))\n",
    "    segmentation = get_segmentation(image)\n",
    "    masks, ids = get_masks(segmentation)\n",
    "    embeddings, crop_corners, _ = crop_images(masks, ids, image)\n",
    "    start = time.time()\n",
    "    print(len(embeddings))\n",
    "    res = collection.search(embeddings, \n",
    "       anns_field='embedding', \n",
    "       param={\"metric_type\": \"L2\",\n",
    "              \"params\": {\"nprobe\": 10}, \"offset\": 1}, \n",
    "       limit=5, \n",
    "       output_fields=['filepath', 'crop_corner'])\n",
    "    finish = time.time()\n",
    "\n",
    "    print(finish - start)\n",
    "\n",
    "    filepaths = []\n",
    "    for hits in res:\n",
    "        seen = set()\n",
    "        for i, hit in enumerate(hits):\n",
    "            if hit.entity.get(\"filepath\") not in seen:\n",
    "                seen.add(hit.entity.get(\"filepath\"))\n",
    "                filepaths.extend([hit.entity.get(\"filepath\") for _ in range(len(hits) - i)])\n",
    "    \n",
    "    counts = Counter(filepaths)\n",
    "    most_common = [path for path, _ in counts.most_common(2)]\n",
    "    \n",
    "    matches = {}\n",
    "    for i, hits in enumerate(res):\n",
    "        matches[i] = {\"search\": crop_corners[i], \"res\": {}}\n",
    "        tracker = set(most_common)\n",
    "        for hit in hits:\n",
    "            if hit.entity.get(\"filepath\") in tracker:\n",
    "                matches[i][\"res\"][hit.entity.get(\"filepath\")] = hit.entity.get(\"crop_corner\")\n",
    "                tracker.remove( hit.entity.get(\"filepath\"))\n",
    "                if len(tracker) == 0:\n",
    "                    continue\n",
    "    \n",
    "    # pprint(matches)\n",
    "    cmap = get_cmap(len(crop_corners))\n",
    "    # plt.axis('off')\n",
    "    #subplot(r,c) provide the no. of rows and columns\n",
    "    f, axarr = plt.subplots(1,3) \n",
    "    axarr[0].imshow(image)\n",
    "    axarr[0].set_title(\"lol\")\n",
    "    axarr[0].axis('off')\n",
    "    for i, (x0, y0, x1, y1) in enumerate(crop_corners):\n",
    "        rect = patches.Rectangle((x0, y0), x1-x0, y1-y0, linewidth=1, edgecolor=cmap(i), facecolor='none')\n",
    "        axarr[0].add_patch(rect)\n",
    "    for i, x in enumerate(most_common):\n",
    "        # use the created array to output your multiple images. In this case I have stacked 4 images vertically\n",
    "        image = Image.open(x)\n",
    "        axarr[i+1].imshow(image)\n",
    "        axarr[i+1].set_title(\"lol\")\n",
    "        axarr[i+1].axis('off')\n",
    "        for key, value in matches.items():\n",
    "            if x in value[\"res\"]:\n",
    "                x0, y0, x1, y1 = value[\"res\"][x]\n",
    "                rect = patches.Rectangle((x0, y0), x1-x0, y1-y0, linewidth=1, edgecolor=cmap(key), facecolor='none')\n",
    "                axarr[i+1].add_patch(rect)\n",
    "        # for key, val in matches.items():\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[\"id: 442362938519978091, distance: 388.6242370605469, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [478.0, 118.0, 643.0, 302.0]}\", \"id: 442362938519978100, distance: 453.6371765136719, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_6.jpg', 'crop_corner': [156.0, 222.0, 474.0, 531.0]}\", \"id: 442362938519978115, distance: 456.8822021484375, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_4.jpg', 'crop_corner': [706.0, 183.0, 851.0, 499.0]}\", \"id: 442362938519978101, distance: 469.0942687988281, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_6.jpg', 'crop_corner': [213.0, 438.0, 456.0, 993.0]}\", \"id: 442362938519978104, distance: 509.27178955078125, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_6.jpg', 'crop_corner': [295.0, 113.0, 408.0, 461.0]}\"]\n",
      "1\n",
      "[\"id: 442362938519978089, distance: 474.35711669921875, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [592.0, 1431.0, 698.0, 1766.0]}\", \"id: 442362938519978087, distance: 529.7330322265625, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [263.0, 683.0, 698.0, 1661.0]}\", \"id: 442362938519978116, distance: 536.1331176757812, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_4.jpg', 'crop_corner': [491.0, 1044.0, 601.0, 1234.0]}\", \"id: 442362938519978091, distance: 544.4028930664062, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [478.0, 118.0, 643.0, 302.0]}\", \"id: 442362938519978112, distance: 546.9963989257812, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_4.jpg', 'crop_corner': [367.0, 921.0, 994.0, 2076.0]}\"]\n",
      "2\n",
      "[\"id: 442362938519978102, distance: 426.22625732421875, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_6.jpg', 'crop_corner': [405.0, 953.0, 530.0, 1031.0]}\", \"id: 442362938519978089, distance: 439.0660400390625, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [592.0, 1431.0, 698.0, 1766.0]}\", \"id: 442362938519978116, distance: 442.7605895996094, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_4.jpg', 'crop_corner': [491.0, 1044.0, 601.0, 1234.0]}\", \"id: 442362938519978095, distance: 444.3660583496094, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [160.0, 893.0, 243.0, 965.0]}\", \"id: 442362938519978110, distance: 448.0102844238281, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_4.jpg', 'crop_corner': [706.0, 239.0, 857.0, 297.0]}\"]\n",
      "3\n",
      "[\"id: 442362938519978114, distance: 429.14349365234375, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_4.jpg', 'crop_corner': [320.0, 1712.0, 1076.0, 2196.0]}\", \"id: 442362938519978091, distance: 457.8908386230469, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [478.0, 118.0, 643.0, 302.0]}\", \"id: 442362938519978090, distance: 468.00347900390625, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [338.0, 1448.0, 780.0, 1777.0]}\", \"id: 442362938519978102, distance: 478.4916687011719, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_6.jpg', 'crop_corner': [405.0, 953.0, 530.0, 1031.0]}\", \"id: 442362938519978101, distance: 497.5997009277344, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_6.jpg', 'crop_corner': [213.0, 438.0, 456.0, 993.0]}\"]\n",
      "4\n",
      "[\"id: 442362938519978114, distance: 435.18017578125, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_4.jpg', 'crop_corner': [320.0, 1712.0, 1076.0, 2196.0]}\", \"id: 442362938519978091, distance: 484.09210205078125, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [478.0, 118.0, 643.0, 302.0]}\", \"id: 442362938519978090, distance: 501.2704162597656, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [338.0, 1448.0, 780.0, 1777.0]}\", \"id: 442362938519978102, distance: 509.51324462890625, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_6.jpg', 'crop_corner': [405.0, 953.0, 530.0, 1031.0]}\", \"id: 442362938519978110, distance: 519.1204833984375, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_4.jpg', 'crop_corner': [706.0, 239.0, 857.0, 297.0]}\"]\n",
      "5\n",
      "[\"id: 442362938519978098, distance: 287.6953430175781, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_6.jpg', 'crop_corner': [273.0, 84.0, 446.0, 241.0]}\", \"id: 442362938519978115, distance: 357.02789306640625, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_4.jpg', 'crop_corner': [706.0, 183.0, 851.0, 499.0]}\", \"id: 442362938519978113, distance: 400.9638977050781, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_4.jpg', 'crop_corner': [843.0, 2054.0, 1087.0, 2198.0]}\", \"id: 442362938519978110, distance: 421.3062744140625, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_4.jpg', 'crop_corner': [706.0, 239.0, 857.0, 297.0]}\", \"id: 442362938519978102, distance: 435.32122802734375, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_6.jpg', 'crop_corner': [405.0, 953.0, 530.0, 1031.0]}\"]\n",
      "6\n",
      "[\"id: 442362938519978101, distance: 409.3504943847656, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_6.jpg', 'crop_corner': [213.0, 438.0, 456.0, 993.0]}\", \"id: 442362938519978090, distance: 429.71075439453125, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [338.0, 1448.0, 780.0, 1777.0]}\", \"id: 442362938519978089, distance: 457.4825134277344, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [592.0, 1431.0, 698.0, 1766.0]}\", \"id: 442362938519978116, distance: 481.6555480957031, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_4.jpg', 'crop_corner': [491.0, 1044.0, 601.0, 1234.0]}\", \"id: 442362938519978102, distance: 483.0933837890625, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_6.jpg', 'crop_corner': [405.0, 953.0, 530.0, 1031.0]}\"]\n",
      "7\n",
      "[\"id: 442362938519978101, distance: 416.5279541015625, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_6.jpg', 'crop_corner': [213.0, 438.0, 456.0, 993.0]}\", \"id: 442362938519978090, distance: 431.02532958984375, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [338.0, 1448.0, 780.0, 1777.0]}\", \"id: 442362938519978098, distance: 489.8555908203125, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_6.jpg', 'crop_corner': [273.0, 84.0, 446.0, 241.0]}\", \"id: 442362938519978089, distance: 489.8616943359375, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [592.0, 1431.0, 698.0, 1766.0]}\", \"id: 442362938519978116, distance: 494.35906982421875, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_4.jpg', 'crop_corner': [491.0, 1044.0, 601.0, 1234.0]}\"]\n",
      "8\n",
      "[\"id: 442362938519978116, distance: 300.5565490722656, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_4.jpg', 'crop_corner': [491.0, 1044.0, 601.0, 1234.0]}\", \"id: 442362938519978105, distance: 410.33056640625, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_6.jpg', 'crop_corner': [451.0, 311.0, 518.0, 384.0]}\", \"id: 442362938519978113, distance: 419.8349609375, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_4.jpg', 'crop_corner': [843.0, 2054.0, 1087.0, 2198.0]}\", \"id: 442362938519978091, distance: 428.367919921875, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [478.0, 118.0, 643.0, 302.0]}\", \"id: 442362938519978110, distance: 442.7868957519531, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_4.jpg', 'crop_corner': [706.0, 239.0, 857.0, 297.0]}\"]\n",
      "9\n",
      "[\"id: 442362938519978089, distance: 470.90380859375, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [592.0, 1431.0, 698.0, 1766.0]}\", \"id: 442362938519978091, distance: 477.3206787109375, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [478.0, 118.0, 643.0, 302.0]}\", \"id: 442362938519978098, distance: 481.96539306640625, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_6.jpg', 'crop_corner': [273.0, 84.0, 446.0, 241.0]}\", \"id: 442362938519978090, distance: 482.7386474609375, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [338.0, 1448.0, 780.0, 1777.0]}\", \"id: 442362938519978100, distance: 488.8482971191406, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_6.jpg', 'crop_corner': [156.0, 222.0, 474.0, 531.0]}\"]\n",
      "10\n",
      "[\"id: 442362938519978090, distance: 513.9728393554688, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [338.0, 1448.0, 780.0, 1777.0]}\", \"id: 442362938519978088, distance: 521.6709594726562, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [535.0, 666.0, 617.0, 721.0]}\", \"id: 442362938519978117, distance: 525.5494384765625, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_4.jpg', 'crop_corner': [472.0, 476.0, 658.0, 1165.0]}\", \"id: 442362938519978087, distance: 531.03564453125, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_7.jpg', 'crop_corner': [263.0, 683.0, 698.0, 1661.0]}\", \"id: 442362938519978113, distance: 532.2638549804688, entity: {'filepath': './photos/Kendall_Jenner/Kendall_Jenner_4.jpg', 'crop_corner': [843.0, 2054.0, 1087.0, 2198.0]}\"]\n"
     ]
    }
   ],
   "source": [
    "for index, result in enumerate(res):\n",
    "    print(index)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[39m.\u001b[39mimshow(Image\u001b[39m.\u001b[39mopen(data_batch[\u001b[39m2\u001b[39m][\u001b[39m0\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_batch' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(Image.open(data_batch[2][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Image.open(res[0][0].entity.filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if utility.has_collection(COLLECTION_NAME):\n",
    "    utility.drop_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_server.stop()\n",
    "default_server.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
