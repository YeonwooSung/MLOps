{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Gonna just use the Taylor Swift Images in this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ZILLIZ_URI = os.getenv(\"ZILLIZ_URI\")\n",
    "ZILLIZ_TOKEN = os.getenv(\"ZILLIZ_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import utility, connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections.connect(uri=ZILLIZ_URI, token=ZILLIZ_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, tensor\n",
    "from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import Resize\n",
    "import torchvision.transforms as T\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision.transforms.functional import crop\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/yujiantang/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "/Users/yujiantang/Documents/workspace/fashionai/fai/lib/python3.10/site-packages/transformers/models/segformer/feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/Users/yujiantang/Documents/workspace/fashionai/fai/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:101: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# run this before importing the resnet50 model if you run into an SSL certificate URLError\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Load the embedding model with the last layer removed\n",
    "embeddings_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\n",
    "embeddings_model = torch.nn.Sequential(*(list(embeddings_model.children())[:-1]))\n",
    "embeddings_model.eval()\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\n",
    "segmentation_model = SegformerForSemanticSegmentation.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "for image in glob.glob(\"./photos/Taylor_Swift/*.jpg\", recursive=True):\n",
    "    image_paths.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSION = 2048\n",
    "BATCH_SIZE = 128\n",
    "COLLECTION_NAME = \"TSwizzleFashionComparison\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import FieldSchema, CollectionSchema, Collection, DataType\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name='filepath', dtype=DataType.VARCHAR, max_length=200),\n",
    "    FieldSchema(name=\"name\", dtype=DataType.VARCHAR, max_length=200),\n",
    "    FieldSchema(name=\"seg_id\", dtype=DataType.INT64),\n",
    "    FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields=fields, enable_dynamic_field=True)\n",
    "collection = Collection(name=COLLECTION_NAME, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = {\n",
    "    \"index_type\": \"IVF_FLAT\",\n",
    "    \"metric_type\": \"L2\",\n",
    "    \"params\": {\"nlist\": 5},\n",
    "}\n",
    "collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label= {\n",
    "    0: \"Background\",\n",
    "    1: \"Hat\",\n",
    "    2: \"Hair\",\n",
    "    3: \"Sunglasses\",\n",
    "    4: \"Upper-clothes\",\n",
    "    5: \"Skirt\",\n",
    "    6: \"Pants\",\n",
    "    7: \"Dress\",\n",
    "    8: \"Belt\",\n",
    "    9: \"Left-shoe\",\n",
    "    10: \"Right-shoe\",\n",
    "    11: \"Face\",\n",
    "    12: \"Left-leg\",\n",
    "    13: \"Right-leg\",\n",
    "    14: \"Left-arm\",\n",
    "    15: \"Right-arm\",\n",
    "    16: \"Bag\",\n",
    "    17: \"Scarf\"\n",
    "  }\n",
    "# want to extract keys: 1, 3, 4, 5, 6, 7, 8, 9, 10, 16, 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted = [1, 3, 4, 5, 6, 7, 8, 9, 10, 16, 17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentation(image):\n",
    "    inputs = extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = segmentation_model(**inputs)\n",
    "    logits = outputs.logits.cpu()\n",
    "\n",
    "    upsampled_logits = nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    pred_seg = upsampled_logits.argmax(dim=1)[0]\n",
    "    return pred_seg \n",
    "\n",
    "# returns two lists masks (tensor) and obj_ids (int)\n",
    "# \"mattmdjaga/segformer_b2_clothes\" from hugging face\n",
    "def get_masks(segmentation):\n",
    "    obj_ids = torch.unique(segmentation)\n",
    "    obj_ids = obj_ids[1:]\n",
    "    wanted_ids = [x.item() for x in obj_ids if x in wanted]\n",
    "    # print(obj_ids)\n",
    "    # print(wanted_ids)\n",
    "    wanted_ids = torch.Tensor(wanted_ids)\n",
    "    # print(wanted_ids)\n",
    "    masks = segmentation == wanted_ids[:, None, None]\n",
    "    return masks, obj_ids\n",
    "\n",
    "def crop_images(masks, obj_ids, img):\n",
    "    boxes = masks_to_boxes(masks)\n",
    "    crop_boxes = []\n",
    "    for box in boxes:\n",
    "        crop_box = tensor([box[0], box[1], box[2]-box[0], box[3]-box[1]])\n",
    "        crop_boxes.append(crop_box)\n",
    "    \n",
    "    preprocess = T.Compose([\n",
    "        T.Resize(size=(256, 256)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    cropped_images = []\n",
    "    seg_ids = []\n",
    "    for i in range(len(crop_boxes)):\n",
    "        crop_box = crop_boxes[i]\n",
    "        cropped = crop(img, crop_box[1].item(), crop_box[0].item(), crop_box[3].item(), crop_box[2].item())\n",
    "        cropped_images.append(preprocess(cropped))\n",
    "        seg_ids.append(obj_ids[i].item())\n",
    "    with torch.no_grad():\n",
    "        embeddings = embeddings_model(torch.stack(cropped_images)).squeeze().tolist()\n",
    "    return embeddings, boxes.tolist(), seg_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb Cell 19\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     masks, ids \u001b[39m=\u001b[39m get_masks(segmentation)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     embeddings, crop_corners, seg_ids \u001b[39m=\u001b[39m crop_images(masks, ids, image)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     inserts \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m: embeddings[x],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mseg_id\u001b[39m\u001b[39m\"\u001b[39m: seg_ids[x],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: name,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfilepath\u001b[39m\u001b[39m\"\u001b[39m: path,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mcrop_corner\u001b[39m\u001b[39m\"\u001b[39m: crop_corners[x]} \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(embeddings))]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     collection\u001b[39m.\u001b[39minsert(inserts)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m collection\u001b[39m.\u001b[39mflush()\n",
      "\u001b[1;32m/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb Cell 19\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     masks, ids \u001b[39m=\u001b[39m get_masks(segmentation)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     embeddings, crop_corners, seg_ids \u001b[39m=\u001b[39m crop_images(masks, ids, image)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     inserts \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m: embeddings[x],\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mseg_id\u001b[39m\u001b[39m\"\u001b[39m: seg_ids[x],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: name,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfilepath\u001b[39m\u001b[39m\"\u001b[39m: path,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mcrop_corner\u001b[39m\u001b[39m\"\u001b[39m: crop_corners[x]} \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(embeddings))]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     collection\u001b[39m.\u001b[39minsert(inserts)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yujiantang/Documents/workspace/fashionai/setup_gradio_fashion.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m collection\u001b[39m.\u001b[39mflush()\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for path in image_paths:\n",
    "    image = Image.open(path)\n",
    "    path_split = path.split(\"/\")\n",
    "    name = \" \".join(path_split[2].split(\"_\"))\n",
    "    segmentation = get_segmentation(image)\n",
    "    masks, ids = get_masks(segmentation)\n",
    "    embeddings, crop_corners, seg_ids = crop_images(masks, ids, image)\n",
    "    inserts = [{\"embedding\": embeddings[x],\n",
    "                \"seg_id\": seg_ids[x],\n",
    "                \"name\": name,\n",
    "                \"filepath\": path,\n",
    "                \"crop_corner\": crop_corners[x]} for x in range(len(embeddings))]\n",
    "    collection.insert(inserts)\n",
    "                \n",
    "collection.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if utility.has_collection(COLLECTION_NAME):\n",
    "    utility.drop_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
