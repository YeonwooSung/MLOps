# MLOps

## Table of Contents

- [AI as a Service](./ai-as-a-service/): simple AI-based service with serverless (AWS Lambda)

- [AWS](./AWS/)
    - [Lambda](./AWS/Lambda/)
        * [invoke sagemaker with aws lambda](./AWS/Lambda/python/invoke_sagemaker.py)

    - [SageMaker](./AWS/SageMaker/)
        * [SageMaker HuggingFace toolkits](./AWS/SageMaker/sagemaker-huggingface-toolkit/)
        * [SageMaker serverless inference](./AWS/SageMaker/serverless-inference/)

- [Data Management](./data_management/)
    - [DVC](./data_management/dvc/)

- [Kubernetes](./kubernetes/)
    - [Basic examples](./kubernetes/basic/)
    - [Troubleshootings](./kubernetes/troubleshooting/)
        * [minikube troubleshooting](./kubernetes/troubleshooting/minikube.md)

- [LLM](./LLM/)
    - [large language models](./LLM/large_laguage_models/)
        * [document oriented agent with openai api](./LLM/large_laguage_models/document_oriented_agent/)
        * [function callings - make LLM to use tools](./LLM/large_laguage_models/function_calling/)

    - [peft](./LLM/PEFT/)

    - [prompt engineering](./LLM/prompt-engineering/)
        * [leaked-system-prompts: leaked system prompts of actual LLM-based services](./LLM/prompt-engineering/leaked-system-prompts/)
        * [source codes and notebooks for learning prompt engineering with openai](./LLM/prompt-engineering/src/)

- [ML serving](./ml-serving/)
    - [bento-ml](./ml-serving/bento-ml/)
    - [build ML pipelines with Nvidia Triton Server](./ml-serving/Build-ML-pipelines-for-Computer-Vision-NLP-and-GNN-using-Nvidia-Triton-Server/)
    - [serving ML models with fastapi](./ml-serving/custom-serving/fastapi/)
    - [serving ML models with flask](./ml-serving/custom-serving/flask/)
    - [ray](./ml-serving/ray/)
        * [Serve GPT-2 pipeline with Ray-serve and FastAPI](./ml-serving/ray/gpt2-with-ray-serve-and-fastapi/)
        * [Serve GPT-J-6B model with Ray-AIR](./ml-serving/ray/ray-air-with-gpt-j-6b/)
