{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9c47fd-5631-4f71-999c-39141186ac33",
   "metadata": {},
   "source": [
    "# ray_deploy.ipynb\n",
    "\n",
    "Optimized model serving implementation from the [benchmark notebook](./benchmark.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70512e88-058d-4644-a86a-71cd8334f57c",
   "metadata": {},
   "source": [
    "## Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8c73a81-6f3d-40a4-a0c7-20a955d4aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization and import code goes in this cell.\n",
    "\n",
    "# Imports: Python core, then third-party, then local.\n",
    "# Try to keep each block in alphabetical order, or the linter may get angry.\n",
    "\n",
    "import asyncio\n",
    "import requests\n",
    "import starlette\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "import scipy.special\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "import torch\n",
    "import transformers\n",
    "import zerocopy\n",
    "\n",
    "import concurrent\n",
    "\n",
    "# Fix silly warning messages about parallel tokenizers\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "\n",
    "\n",
    "# Reduce the volume of warning messages from `transformers`\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "def reboot_ray():\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        return ray.init(num_gpus=1)\n",
    "    else:\n",
    "        return ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfec84ca-c245-44ec-b5e3-4d68aa6ea093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants go here\n",
    "INTENT_MODEL_NAME = 'mrm8488/t5-base-finetuned-e2m-intent'\n",
    "SENTIMENT_MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "QA_MODEL_NAME = 'deepset/roberta-base-squad2'\n",
    "GENERATE_MODEL_NAME = 'gpt2'\n",
    "\n",
    "\n",
    "INTENT_INPUT = {\n",
    "    'context':\n",
    "        (\"I came here to eat chips and beat you up, \"\n",
    "         \"and I'm all out of chips.\")\n",
    "}\n",
    "\n",
    "SENTIMENT_INPUT = {\n",
    "    'context': \"We're not happy unless you're not happy.\"\n",
    "}\n",
    "\n",
    "QA_INPUT = {\n",
    "    'question': 'What is 1 + 1?',\n",
    "    'context': \n",
    "        \"\"\"Addition (usually signified by the plus symbol +) is one of the four basic operations of \n",
    "        arithmetic, the other three being subtraction, multiplication and division. The addition of two \n",
    "        whole numbers results in the total amount or sum of those values combined. The example in the\n",
    "        adjacent image shows a combination of three apples and two apples, making a total of five apples. \n",
    "        This observation is equivalent to the mathematical expression \"3 + 2 = 5\" (that is, \"3 plus 2 \n",
    "        is equal to 5\").\n",
    "        \"\"\"\n",
    "}\n",
    "\n",
    "GENERATE_INPUT = {\n",
    "    'prompt_text': 'All your base are'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756da010-486f-49b7-8023-74af80661c11",
   "metadata": {},
   "source": [
    "## Example model code\n",
    "\n",
    "This is the single-node code on which the Serve deployments below are based.  Some of this code is duplicated in `benchmark.ipynb` and should be kept in sync."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f636547-d45b-4acd-ab7d-4268f2957416",
   "metadata": {},
   "source": [
    "### Intent model\n",
    "\n",
    "For our intent detection models, we'll use the model [`mrm8488/t5-base-finetuned-e2m-intent`](https://huggingface.co/mrm8488/t5-base-finetuned-e2m-intent).\n",
    "\n",
    "The intent model comes as three parts: A *tokenizer* that converts raw text into a sequence numeric token IDs, a core *model* that transforms these token sequences, and *preprocessing and postprocessing code* to choreograph the usage of the first two parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9bad291-e541-45f8-9a7a-d6c7b020f76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to eat'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "intent_tokenizer = transformers.AutoTokenizer.from_pretrained('t5-base')\n",
    "intent_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    INTENT_MODEL_NAME)\n",
    "\n",
    "# Preprocessing\n",
    "input_text = f'{INTENT_INPUT[\"context\"]} </s>'\n",
    "features = intent_tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "# Inference\n",
    "output = intent_model.generate(**features)\n",
    "\n",
    "# Postprocessing\n",
    "result_string = intent_tokenizer.decode(output[0])\n",
    "result_string = result_string.replace('<pad>', '')\n",
    "result_string = result_string[len(' '):-len('</s>')]\n",
    "\n",
    "result_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46f6c1-720a-49cb-8078-400e286e7dbf",
   "metadata": {},
   "source": [
    "### Sentiment model\n",
    "\n",
    "For our sentiment models, we'll use model [`cardiffnlp/twitter-roberta-base-sentiment`](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment).\n",
    "\n",
    "Like the intent model, the sentiment model is packaged as a tokenizer, a core model, and instructions for pre- and post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e9c38c0-1210-431e-8024-9b47f77373aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 0.5419477820396423,\n",
       " 'neutral': 0.38251084089279175,\n",
       " 'negative': 0.07554134726524353}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model loading\n",
    "sentiment_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    SENTIMENT_MODEL_NAME)\n",
    "sentiment_model = (transformers.AutoModelForSequenceClassification\n",
    "                .from_pretrained(SENTIMENT_MODEL_NAME))\n",
    "\n",
    "# Preprocessing\n",
    "encoded_input = sentiment_tokenizer(SENTIMENT_INPUT['context'], \n",
    "                                 return_tensors='pt')   \n",
    "\n",
    "# Inference\n",
    "output = sentiment_model(**encoded_input)\n",
    "\n",
    "# Postprocessing\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = scipy.special.softmax(scores)\n",
    "scores = [float(s) for s in scores]\n",
    "scores = {k: v for k, v in zip(['positive', 'neutral', 'negative'], scores)}\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c59f03b-dd87-4d87-845d-4123ccdad396",
   "metadata": {},
   "source": [
    "### Question Answering Model\n",
    "\n",
    "For our question answering models, we'll use the model [`deepset/roberta-base-squad2`](https://huggingface.co/deepset/roberta-base-squad2).\n",
    "\n",
    "Unlike the intent and sentiment models, the question answering model comes prepackaged as a `question-answering` pipeline via the `tokenizers` library's [Pipelines API](https://huggingface.co/docs/transformers/main_classes/pipelines). \n",
    "\n",
    "So we can load and run all parts of the model, including pre- and post-processing code, by creating an instance of the pipeline class. The pipeline object has methods `preprocess()`, `forward()`, and `postprocess()` to perform preprocessing, inference, and postprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96486779-6bc5-4a3f-8dd2-6faeeb117ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 4.278938831703272e-06, 'start': 483, 'end': 484, 'answer': '5'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the model\n",
    "qa_pipeline = transformers.pipeline('question-answering',\n",
    "                                    model=QA_MODEL_NAME)\n",
    "# Preprocessing (returns a Python generator)\n",
    "qa_pre = qa_pipeline.preprocess(qa_pipeline.create_sample(**QA_INPUT))\n",
    "\n",
    "# Inference\n",
    "qa_output = (qa_pipeline.forward(example) for example in qa_pre)\n",
    "\n",
    "# Postprocessing\n",
    "qa_result = qa_pipeline.postprocess(qa_output)\n",
    "\n",
    "qa_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c376f12-e679-4f37-8fe7-8ec336c009f8",
   "metadata": {},
   "source": [
    "There is also a convenience method `__call__()` that runs all three phases of processing in sequence.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42b72d2a-8d20-48a6-9094-7189d2b76be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 4.278938831703272e-06, 'start': 483, 'end': 484, 'answer': '5'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code also appears in `benchmark.ipynb`\n",
    "\n",
    "# Loading the model and associated resources\n",
    "qa_pipeline = transformers.pipeline('question-answering',\n",
    "                                    model=QA_MODEL_NAME)\n",
    "# Preprocessing, inference, and postprocessing all happen in\n",
    "# the Python object's the __call__() method.\n",
    "qa_result = qa_pipeline(**QA_INPUT)\n",
    "\n",
    "qa_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcdc62b-d777-4b97-b144-dac251a68865",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Natural Language Generation Model\n",
    "\n",
    "For natural language generation, we'll use the [`gpt2`](https://huggingface.co/gpt2) language model. Like the question answering model, this natural language generation model comes wrapped in a `tokenizers` pipeline class. The class's `__call__()` method performs all the steps necessary to run end-to-end inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73389972-e841-45c5-a061-44a7696a4652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'All your base are just to get you going. If you have any problems you can use this guide to try and start playing with our new cards. There are a lot of great options you can use.\\n\\nFor the players that will run into'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "generate_pipeline = transformers.pipeline(\n",
    "    'text-generation', model=GENERATE_MODEL_NAME)\n",
    "pad_token_id = generate_pipeline.tokenizer.eos_token_id\n",
    "\n",
    "# Preprocessing\n",
    "generate_pre = generate_pipeline.preprocess(**GENERATE_INPUT)\n",
    "\n",
    "# Inference\n",
    "generate_output = generate_pipeline.forward(generate_pre,\n",
    "                                            pad_token_id=pad_token_id)\n",
    "\n",
    "# Postprocessing\n",
    "generate_result = generate_pipeline.postprocess(generate_output)\n",
    "generate_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aa4967-b9a1-4843-820d-98cc3f775523",
   "metadata": {},
   "source": [
    "## Start Ray Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3111006c-4a4a-4301-9e15-cc761ddb33ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 16:23:27,706\tINFO services.py:1412 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=51116)\u001b[0m 2022-04-14 16:23:32,867\tINFO checkpoint_path.py:16 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=51116)\u001b[0m 2022-04-14 16:23:32,974\tINFO http_state.py:98 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:TOnaCn:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "2022-04-14 16:23:33,483\tINFO api.py:521 -- Started Serve instance in namespace '5ba3a27e-f16a-4829-9268-ad13be21fc2e'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.serve.api.Client at 0x7fe790835a90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=51122)\u001b[0m INFO:     Started server process [51122]\n"
     ]
    }
   ],
   "source": [
    "serve.shutdown()\n",
    "reboot_ray()\n",
    "serve.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0983420f-8ac2-496d-9fb6-5632370a5387",
   "metadata": {},
   "source": [
    "## Optimized Model Deployments\n",
    "\n",
    "Some of these classes appear in slightly modified format in `benchmark.ipynb`. Make sure to keep the code in sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "356441ab-f4f1-4b6f-8e6c-fbd3d3eeae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class also appears in `benchmark.ipynb`\n",
    "@serve.deployment\n",
    "class Intent:\n",
    "    def __init__(self):\n",
    "        self._tokenizer = transformers.AutoTokenizer.from_pretrained('t5-base')\n",
    "\n",
    "        # Extract weights and load them onto the Plasma object store\n",
    "        self._model_ref = ray.put(zerocopy.extract_tensors(\n",
    "            transformers.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    INTENT_MODEL_NAME)))\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        json_request = await request.json()\n",
    "\n",
    "        # Preprocessing\n",
    "        input_text = f'{json_request[\"context\"]} </s>'\n",
    "        features = self._tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "        # Model inference runs asynchronously in a Ray task\n",
    "        output = await zerocopy.call_model.remote(\n",
    "            self._model_ref, [], features, 'generate')\n",
    "\n",
    "        # Postprocessing\n",
    "        result_string = self._tokenizer.decode(output[0])\n",
    "        result_string = result_string[len('<pad> '):-len('</s>')]\n",
    "        return {\n",
    "            'intent': result_string\n",
    "        }\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class Sentiment:\n",
    "    def __init__(self):\n",
    "        self._tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            SENTIMENT_MODEL_NAME)\n",
    "\n",
    "        model = (transformers.AutoModelForSequenceClassification\n",
    "                 .from_pretrained(SENTIMENT_MODEL_NAME))\n",
    "        self._model_ref = ray.put(zerocopy.extract_tensors(model))\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        json_request = await request.json()\n",
    "\n",
    "        # Preprocessing\n",
    "        encoded_input = self._tokenizer(json_request['context'], \n",
    "                                         return_tensors='pt')   \n",
    "\n",
    "        # Inference\n",
    "        output = await zerocopy.call_model.remote(\n",
    "            self._model_ref, [], encoded_input)\n",
    "\n",
    "        # Postprocessing\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = scipy.special.softmax(scores)\n",
    "        scores = [float(s) for s in scores]\n",
    "        scores = {k: v for k, v in zip(['positive', 'neutral', 'negative'], scores)}\n",
    "        return scores\n",
    "\n",
    "\n",
    "# This class also appears in `benchmark.ipynb`\n",
    "@serve.deployment\n",
    "class QA:\n",
    "    def __init__(self):\n",
    "        # Load the pipeline and move the model's weights onto the\n",
    "        # Plasma object store.\n",
    "        self._pipeline = zerocopy.rewrite_pipeline(\n",
    "            transformers.pipeline('question-answering', \n",
    "                                  model=QA_MODEL_NAME))\n",
    "        self._threadpool = concurrent.futures.ThreadPoolExecutor()\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        json_request = await request.json()\n",
    "\n",
    "        # The original `transformers` code is not async-aware, so we\n",
    "        # call it from `run_in_executor()`\n",
    "        result = await asyncio.get_running_loop().run_in_executor(\n",
    "             self._threadpool, lambda: self._pipeline(**json_request))\n",
    "        return result\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class Generate:\n",
    "    def __init__(self):\n",
    "        self._pipeline = zerocopy.rewrite_pipeline(\n",
    "            transformers.pipeline('text-generation',\n",
    "                                  model=GENERATE_MODEL_NAME),\n",
    "            ('__call__', 'generate'))\n",
    "        self._pad_token_id = self._pipeline.tokenizer.eos_token_id\n",
    "        self._threadpool = concurrent.futures.ThreadPoolExecutor()\n",
    "\n",
    "    async def __call__(self, request: starlette.requests.Request):\n",
    "        json_request = await request.json()\n",
    "\n",
    "        result = await asyncio.get_running_loop().run_in_executor(\n",
    "            self._threadpool, \n",
    "            lambda: self._pipeline(\n",
    "                json_request['prompt_text'], \n",
    "                pad_token_id=self._pad_token_id))\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fa3569-8f71-4158-a1ce-a47d1c0da78a",
   "metadata": {},
   "source": [
    "Now we can deploy all of these pipelines as Serve endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae657532-5495-43d4-9bba-ae102cba0634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 16:23:58,332\tINFO api.py:262 -- Updating deployment 'intent_en'. component=serve deployment=intent_en\n",
      "2022-04-14 16:23:58,343\tINFO api.py:262 -- Updating deployment 'sentiment_en'. component=serve deployment=sentiment_en\n",
      "2022-04-14 16:23:58,353\tINFO api.py:262 -- Updating deployment 'qa_en'. component=serve deployment=qa_en\n",
      "2022-04-14 16:23:58,364\tINFO api.py:262 -- Updating deployment 'generate_en'. component=serve deployment=generate_en\n",
      "2022-04-14 16:23:58,377\tINFO api.py:262 -- Updating deployment 'intent_es'. component=serve deployment=intent_es\n",
      "2022-04-14 16:23:58,391\tINFO api.py:262 -- Updating deployment 'sentiment_es'. component=serve deployment=sentiment_es\n",
      "2022-04-14 16:23:58,404\tINFO api.py:262 -- Updating deployment 'qa_es'. component=serve deployment=qa_es\n",
      "2022-04-14 16:23:58,417\tINFO api.py:262 -- Updating deployment 'generate_es'. component=serve deployment=generate_es\n",
      "2022-04-14 16:23:58,431\tINFO api.py:262 -- Updating deployment 'intent_zh'. component=serve deployment=intent_zh\n",
      "\u001b[2m\u001b[36m(ServeController pid=51116)\u001b[0m 2022-04-14 16:23:58,433\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'intent_en'. component=serve deployment=intent_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=51116)\u001b[0m 2022-04-14 16:23:58,475\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'sentiment_en'. component=serve deployment=sentiment_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=51116)\u001b[0m 2022-04-14 16:23:58,539\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'qa_en'. component=serve deployment=qa_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=51116)\u001b[0m 2022-04-14 16:23:58,565\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'generate_en'. component=serve deployment=generate_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=51116)\u001b[0m 2022-04-14 16:23:58,587\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'intent_es'. component=serve deployment=intent_es\n",
      "\u001b[2m\u001b[36m(ServeController pid=51116)\u001b[0m 2022-04-14 16:23:58,609\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'sentiment_es'. component=serve deployment=sentiment_es\n",
      "\u001b[2m\u001b[36m(ServeController pid=51116)\u001b[0m 2022-04-14 16:23:58,637\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'qa_es'. component=serve deployment=qa_es\n",
      "\u001b[2m\u001b[36m(ServeController pid=51116)\u001b[0m 2022-04-14 16:23:58,664\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'generate_es'. component=serve deployment=generate_es\n",
      "\u001b[2m\u001b[36m(ServeController pid=51116)\u001b[0m 2022-04-14 16:23:58,727\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'intent_zh'. component=serve deployment=intent_zh\n",
      "2022-04-14 16:23:58,776\tINFO api.py:262 -- Updating deployment 'sentiment_zh'. component=serve deployment=sentiment_zh\n",
      "2022-04-14 16:23:58,800\tINFO api.py:262 -- Updating deployment 'qa_zh'. component=serve deployment=qa_zh\n",
      "2022-04-14 16:23:58,828\tINFO api.py:262 -- Updating deployment 'generate_zh'. component=serve deployment=generate_zh\n",
      "\u001b[2m\u001b[36m(ServeController pid=51116)\u001b[0m 2022-04-14 16:23:58,868\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'sentiment_zh'. component=serve deployment=sentiment_zh\n",
      "\u001b[2m\u001b[36m(ServeController pid=51116)\u001b[0m 2022-04-14 16:23:58,899\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'qa_zh'. component=serve deployment=qa_zh\n",
      "\u001b[2m\u001b[36m(ServeController pid=51116)\u001b[0m 2022-04-14 16:23:58,932\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'generate_zh'. component=serve deployment=generate_zh\n",
      "\u001b[2m\u001b[36m(intent_en pid=51113)\u001b[0m The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "\u001b[2m\u001b[36m(intent_es pid=51120)\u001b[0m The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "\u001b[2m\u001b[36m(intent_zh pid=51108)\u001b[0m The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "# Define endpoints.\n",
    "# Everything gets deployed under the prefix /predictions/ to make\n",
    "# the deployment as similar as possible to the TorchServe baseline.\n",
    "LANGUAGES = ['en', 'es', 'zh']\n",
    "\n",
    "\n",
    "for lang in LANGUAGES:\n",
    "    (Intent.options(name=f'intent_{lang}',\n",
    "                   route_prefix=f'/predictions/intent_{lang}',\n",
    "                   ray_actor_options={\"num_cpus\": 0.1})\n",
    "     .deploy(_blocking=False))\n",
    "    (Sentiment.options(name=f'sentiment_{lang}',\n",
    "                   route_prefix=f'/predictions/sentiment_{lang}',\n",
    "                   ray_actor_options={\"num_cpus\": 0.1})\n",
    "     .deploy(_blocking=False))\n",
    "    (QA.options(name=f'qa_{lang}',\n",
    "                   route_prefix=f'/predictions/qa_{lang}',\n",
    "                   ray_actor_options={\"num_cpus\": 0.1})\n",
    "     .deploy(_blocking=False))\n",
    "    (Generate.options(name=f'generate_{lang}',\n",
    "                   route_prefix=f'/predictions/generate_{lang}',\n",
    "                   ray_actor_options={\"num_cpus\": 0.1})\n",
    "     .deploy(_blocking=False))\n",
    "\n",
    "# Wait a moment so log output doesn't go to the next cell's output\n",
    "time.sleep(5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44da4cee-44a2-423a-9aa4-d5ed1c5dd8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7950.210511 MB       61, (7950.210511 MB)  0, (0.0 MB)   0, (0.0 MB)    0, (0.0 MB)          53, (0.0 MB) \n",
      "127.0.0.1     | 51116    | Worker  |           | 1.5e-05 MB | LOCAL_REFERENCE | b4567f7b86f1c9b04089143b9fcd9bcfa6aa4e880100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 1.5e-05 MB | LOCAL_REFERENCE | 0cb686442cb43d5ecb863c842117b9ea56b331370100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 1.5e-05 MB | LOCAL_REFERENCE | 5072e9fc92a6447effd95f38533e692f8796b72b0100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 1.5e-05 MB | LOCAL_REFERENCE | d714b645ac9c0d738e0f785a690e9d7149e007d60100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 1.5e-05 MB | LOCAL_REFERENCE | dd797876ac844e6cec693c0afb09f36ab69518530100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 1.5e-05 MB | LOCAL_REFERENCE | d53def7e0cdfbb7ceeeccac7d4023d02b21863c30100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 1.5e-05 MB | LOCAL_REFERENCE | 16f770dbddefee94dbc9313ac49521c8741ff04e0100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 1.5e-05 MB | LOCAL_REFERENCE | 0dd10f8a1fd1a1671f480c93ba18e35136121ee60100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 1.5e-05 MB | LOCAL_REFERENCE | 442bdb152e236da1adc15c5c0617f64b080feff00100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 1.5e-05 MB | LOCAL_REFERENCE | 0897720c35efdaa055c84712d1dc142f48c765530100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 1.5e-05 MB | LOCAL_REFERENCE | c3d6e51ec39b88c7160a6428f05a1d6f1a04bf6d0100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 1.5e-05 MB | LOCAL_REFERENCE | 165d6c3f4d631fe6a91801fb44e469d6867cee2f0100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 0.000471 MB | LOCAL_REFERENCE | 7693d3820768ba018e0f785a690e9d7149e007d60100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 0.000471 MB | LOCAL_REFERENCE | 5f2178ca2cd853c3a91801fb44e469d6867cee2f0100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 0.000471 MB | LOCAL_REFERENCE | 8aa143a5a17e8df9ec693c0afb09f36ab69518530100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 0.000471 MB | LOCAL_REFERENCE | 0ef590906626002955c84712d1dc142f48c765530100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 0.000473 MB | LOCAL_REFERENCE | be485eabab1d7255eeeccac7d4023d02b21863c30100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 0.000473 MB | LOCAL_REFERENCE | 283b5af397b1effacb863c842117b9ea56b331370100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 0.000473 MB | LOCAL_REFERENCE | 262c93c2c998dd361f480c93ba18e35136121ee60100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 0.000473 MB | LOCAL_REFERENCE | 9cd3a330b64d82beadc15c5c0617f64b080feff00100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 0.000473 MB | LOCAL_REFERENCE | 3c82fff14f280076dbc9313ac49521c8741ff04e0100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 0.000473 MB | LOCAL_REFERENCE | 1dd65e66828b562e4089143b9fcd9bcfa6aa4e880100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 0.000473 MB | LOCAL_REFERENCE | 5c0c06b8d916f1c2ffd95f38533e692f8796b72b0100000001000000\n",
      "127.0.0.1     | 51116    | Worker  |           | 0.000473 MB | LOCAL_REFERENCE | 9e1e06801a61eab9160a6428f05a1d6f1a04bf6d0100000001000000\n",
      "127.0.0.1     | 51118    | Worker  |           | 496.274462 MB | LOCAL_REFERENCE | 006f7d45c018d6754385d0e52b5eeba5858ffb400100000001000000\n",
      "127.0.0.1     | 51117    | Worker  |           | 496.274462 MB | LOCAL_REFERENCE | 0023ab1ef11a6f790b38151c590b1a985f32c1a40100000001000000\n",
      "127.0.0.1     | 51112    | Worker  |           | 496.274462 MB | LOCAL_REFERENCE | 005dd9a1b100e5ceae26dde31018e40eb0e985670100000001000000\n",
      "127.0.0.1     | 51119    | Worker  |           | 498.640411 MB | LOCAL_REFERENCE | 002f0321dd4dbd029963848382af24fed2a1f0cb0100000001000000\n",
      "127.0.0.1     | 51114    | Worker  |           | 498.640411 MB | LOCAL_REFERENCE | 0068374d51ce6ab1dc5cbf77007d565d24fc599a0100000001000000\n",
      "127.0.0.1     | 51109    | Worker  |           | 498.640411 MB | LOCAL_REFERENCE | 00334dba1b575d7a9bb7057c0f79a831587babfc0100000001000000\n",
      "127.0.0.1     | 51111    | Worker  |           | 664.763098 MB | LOCAL_REFERENCE | 00e8aa4f74d575572745a1e2397a88531383532e0100000001000000\n",
      "127.0.0.1     | 51115    | Worker  |           | 664.763098 MB | LOCAL_REFERENCE | 005917687e8f2a9bc05e4bc971486f45791e4f2f0100000001000000\n",
      "127.0.0.1     | 51121    | Worker  |           | 664.763098 MB | LOCAL_REFERENCE | 006d607cfebc96249ba9ae16371ef67ad0c613030100000001000000\n",
      "127.0.0.1     | 51120    | Worker  |           | 990.39025 MB | LOCAL_REFERENCE | 00912000d9d33992ebc7332134e2a6b8f09abbb30100000001000000\n",
      "127.0.0.1     | 51113    | Worker  |           | 990.39025 MB | LOCAL_REFERENCE | 008560c9ad45e06b537af4cf84a28d4483df29250100000001000000\n",
      "127.0.0.1     | 51108    | Worker  |           | 990.39025 MB | LOCAL_REFERENCE | 0060d76c711e8b5826bd76ce63772f8aaf06d3560100000001000000\n"
     ]
    }
   ],
   "source": [
    "# Dump object sizes from Plasma. Used to populate the table of model sizes in the main notebook.\n",
    "!ray memory --units MB | grep MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baac194f-9c7e-4d7c-8228-4e55b31738f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent result: {'intent': 'to eat'}\n",
      "Sentiment result: {'positive': 0.5419476628303528, 'neutral': 0.38251087069511414, 'negative': 0.07554134726524353}\n",
      "Question answering result: {'score': 4.278897904441692e-06, 'start': 483, 'end': 484, 'answer': '5'}\n",
      "Natural language generation result: [{'generated_text': \"All your base are in a position to be able to compete to be the best in the world. We take your feedback very seriously. We're going to be working to make sure that we're doing everything we can to make a better game for everyone\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-08 11:42:42,682\tWARNING worker.py:1257 -- Warning: More than 5000 tasks are pending submission to actor fbce1094d701fcfadbb0ea8a01000000. To reduce memory usage, wait for these tasks to finish before sending more.\n",
      "\u001b[2m\u001b[36m(pid=66875)\u001b[0m [2022-03-08 11:49:17,044 E 66875 66920] core_worker_process.cc:348: The global worker has already been shutdown. This happens when the language frontend accesses the Ray's worker after it is shutdown. The process will exit\n",
      "2022-03-08 11:51:57,926\tWARNING worker.py:1257 -- Warning: More than 10000 tasks are pending submission to actor fbce1094d701fcfadbb0ea8a01000000. To reduce memory usage, wait for these tasks to finish before sending more.\n",
      "\u001b[2m\u001b[36m(pid=70721)\u001b[0m [2022-03-08 11:52:00,501 E 70721 70771] core_worker_process.cc:348: The global worker has already been shutdown. This happens when the language frontend accesses the Ray's worker after it is shutdown. The process will exit\n",
      "\u001b[2m\u001b[36m(pid=71167)\u001b[0m [2022-03-08 11:52:22,160 E 71167 71218] core_worker_process.cc:348: The global worker has already been shutdown. This happens when the language frontend accesses the Ray's worker after it is shutdown. The process will exit\n",
      "\u001b[2m\u001b[36m(pid=71286)\u001b[0m [2022-03-08 11:52:27,603 E 71286 71403] core_worker_process.cc:348: The global worker has already been shutdown. This happens when the language frontend accesses the Ray's worker after it is shutdown. The process will exit\n",
      "2022-03-08 12:05:17,889\tWARNING worker.py:1257 -- Warning: More than 20000 tasks are pending submission to actor fbce1094d701fcfadbb0ea8a01000000. To reduce memory usage, wait for these tasks to finish before sending more.\n"
     ]
    }
   ],
   "source": [
    "# Verify that everything deployed properly.\n",
    "intent_result = requests.put(\n",
    "    'http://127.0.0.1:8000/predictions/intent_en', \n",
    "    json.dumps(INTENT_INPUT)).json()\n",
    "print(f'Intent result: {intent_result}')\n",
    "\n",
    "sentiment_result = requests.put(\n",
    "    'http://127.0.0.1:8000/predictions/sentiment_en', \n",
    "    json.dumps(SENTIMENT_INPUT)).json()\n",
    "print(f'Sentiment result: {sentiment_result}')\n",
    "\n",
    "qa_result = requests.put(\n",
    "    'http://127.0.0.1:8000/predictions/qa_en', \n",
    "    json.dumps(QA_INPUT)).json()\n",
    "print(f'Question answering result: {qa_result}')\n",
    "\n",
    "generate_result = requests.put(\n",
    "    'http://127.0.0.1:8000/predictions/generate_en', \n",
    "    json.dumps(GENERATE_INPUT)).json()\n",
    "print(f'Natural language generation result: {generate_result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fb1cf4-e53f-463f-a7f8-deadb3947186",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "\n",
    "Once the benchmark is complete, shut down this notebook's Ray cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42017025-37d7-4dd1-b436-ae4fea31bc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=19731)\u001b[0m 2022-03-08 12:47:52,714\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'intent_en'. component=serve deployment=intent_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=19731)\u001b[0m 2022-03-08 12:47:52,719\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'sentiment_en'. component=serve deployment=sentiment_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=19731)\u001b[0m 2022-03-08 12:47:52,734\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'qa_en'. component=serve deployment=qa_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=19731)\u001b[0m 2022-03-08 12:47:52,737\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'generate_en'. component=serve deployment=generate_en\n",
      "\u001b[2m\u001b[36m(ServeController pid=19731)\u001b[0m 2022-03-08 12:47:52,740\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'intent_es'. component=serve deployment=intent_es\n",
      "\u001b[2m\u001b[36m(ServeController pid=19731)\u001b[0m 2022-03-08 12:47:52,743\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'sentiment_es'. component=serve deployment=sentiment_es\n",
      "\u001b[2m\u001b[36m(ServeController pid=19731)\u001b[0m 2022-03-08 12:47:52,747\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'qa_es'. component=serve deployment=qa_es\n",
      "\u001b[2m\u001b[36m(ServeController pid=19731)\u001b[0m 2022-03-08 12:47:52,756\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'generate_es'. component=serve deployment=generate_es\n",
      "\u001b[2m\u001b[36m(ServeController pid=19731)\u001b[0m 2022-03-08 12:47:52,758\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'intent_zh'. component=serve deployment=intent_zh\n",
      "\u001b[2m\u001b[36m(ServeController pid=19731)\u001b[0m 2022-03-08 12:47:52,761\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'sentiment_zh'. component=serve deployment=sentiment_zh\n",
      "\u001b[2m\u001b[36m(ServeController pid=19731)\u001b[0m 2022-03-08 12:47:52,765\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'qa_zh'. component=serve deployment=qa_zh\n",
      "\u001b[2m\u001b[36m(ServeController pid=19731)\u001b[0m 2022-03-08 12:47:52,768\tINFO deployment_state.py:940 -- Removing 1 replicas from deployment 'generate_zh'. component=serve deployment=generate_zh\n"
     ]
    }
   ],
   "source": [
    "serve.shutdown()\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ddf0f-c6a0-475e-b549-b16d29dce24b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
