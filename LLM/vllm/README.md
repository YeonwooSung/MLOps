# vLLM for serving LLMs

## Sample codes

- [CPU offload](./samples/cpu_offload.py)
- [LoRA Quantization](./samples/lora_with_quantization.py)
- [Multi-LoRA](./samples/multilora_inference.py)
- Offline inference
    - [Audio-Language Inference](./samples/offline_inference_audio_language.py)
    - [Tensor Parallel Inference](./samples/offline_inference_distributed.py)
    - [LLM2Vec Embeddings](./samples/offline_inference_embedding.py)
    - [Run Pixtral](./samples/offline_inference_pixtral.py)
    - [Run with Profiler](./samples/offline_inference_with_profiler.py)
    - [Speculation Decoding](./samples/offline_inference_speculator.py)
    - [Vision-Language multi-image Inference](./samples/offline_inference_vision_language_multi_image.py)
